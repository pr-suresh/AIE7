{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e-IqJAMkwnCF"
      },
      "source": [
        "# Advanced Retrieval with LangChain\n",
        "\n",
        "In the following notebook, we'll explore various methods of advanced retrieval using LangChain!\n",
        "\n",
        "We'll touch on:\n",
        "\n",
        "- Naive Retrieval\n",
        "- Best-Matching 25 (BM25)\n",
        "- Multi-Query Retrieval\n",
        "- Parent-Document Retrieval\n",
        "- Contextual Compression (a.k.a. Rerank)\n",
        "- Ensemble Retrieval\n",
        "- Semantic chunking\n",
        "\n",
        "We'll also discuss how these methods impact performance on our set of documents with a simple RAG chain.\n",
        "\n",
        "There will be two breakout rooms:\n",
        "\n",
        "- 🤝 Breakout Room Part #1\n",
        "  - Task 1: Getting Dependencies!\n",
        "  - Task 2: Data Collection and Preparation\n",
        "  - Task 3: Setting Up QDrant!\n",
        "  - Task 4-10: Retrieval Strategies\n",
        "- 🤝 Breakout Room Part #2\n",
        "  - Activity: Evaluate with Ragas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4rKP3hgHivpe"
      },
      "source": [
        "# 🤝 Breakout Room Part #1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3xes8oT-xHN7"
      },
      "source": [
        "## Task 1: Getting Dependencies!\n",
        "\n",
        "We're going to need a few specific LangChain community packages, like OpenAI (for our [LLM](https://platform.openai.com/docs/models) and [Embedding Model](https://platform.openai.com/docs/guides/embeddings)) and Cohere (for our [Reranker](https://cohere.com/rerank))."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z7OHJXzfyJyA"
      },
      "source": [
        "We'll also provide our OpenAI key, as well as our Cohere API key."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7LttlDQUYgSI",
        "outputId": "9dca95ab-4d02-4adf-ec3f-cb831326dc54"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import getpass\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter your OpenAI API Key:\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3iUahNiJyQbv",
        "outputId": "78bf06ef-2ee8-46c3-f73d-27958b4dd79b"
      },
      "outputs": [],
      "source": [
        "os.environ[\"COHERE_API_KEY\"] = getpass.getpass(\"Cohere API Key:\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "from uuid import uuid4\n",
        "\n",
        "unique_id = uuid4().hex[0:8]\n",
        "\n",
        "os.environ[\"LANGSMITH_TRACING\"] = \"true\"\n",
        "os.environ[\"LANGSMITH_PROJECT\"] = f\"LangSmith - {unique_id}\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "os.environ[\"LANGCHAIN_API_KEY\"] = getpass.getpass(\"Enter your LangChain API key: \")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mw304iAFyRtl"
      },
      "source": [
        "## Task 2: Data Collection and Preparation\n",
        "\n",
        "We'll be using our Loan Data once again - this time the strutured data available through the CSV!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A92NC2QZzCsi"
      },
      "source": [
        "### Data Preparation\n",
        "\n",
        "We want to make sure all our documents have the relevant metadata for the various retrieval strategies we're going to be applying today."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "GshBjVRJZ6p8"
      },
      "outputs": [],
      "source": [
        "from langchain_community.document_loaders.csv_loader import CSVLoader\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "loader = CSVLoader(\n",
        "    file_path=f\"./data/complaints.csv\",\n",
        "    metadata_columns=[\n",
        "      \"Date received\", \n",
        "      \"Product\", \n",
        "      \"Sub-product\", \n",
        "      \"Issue\", \n",
        "      \"Sub-issue\", \n",
        "      \"Consumer complaint narrative\", \n",
        "      \"Company public response\", \n",
        "      \"Company\", \n",
        "      \"State\", \n",
        "      \"ZIP code\", \n",
        "      \"Tags\", \n",
        "      \"Consumer consent provided?\", \n",
        "      \"Submitted via\", \n",
        "      \"Date sent to company\", \n",
        "      \"Company response to consumer\", \n",
        "      \"Timely response?\", \n",
        "      \"Consumer disputed?\", \n",
        "      \"Complaint ID\"\n",
        "    ]\n",
        ")\n",
        "\n",
        "loan_complaint_data = loader.load()\n",
        "\n",
        "for doc in loan_complaint_data:\n",
        "    doc.page_content = doc.metadata[\"Consumer complaint narrative\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9gQphb6y0C0S"
      },
      "source": [
        "Let's look at an example document to see if everything worked as expected!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PkUkCf7DaMiq",
        "outputId": "e90bd5da-1d87-423b-838a-cb6efc16b199"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Document(metadata={'source': './data/complaints.csv', 'row': 0, 'Date received': '03/27/25', 'Product': 'Student loan', 'Sub-product': 'Federal student loan servicing', 'Issue': 'Dealing with your lender or servicer', 'Sub-issue': 'Trouble with how payments are being handled', 'Consumer complaint narrative': \"The federal student loan COVID-19 forbearance program ended in XX/XX/XXXX. However, payments were not re-amortized on my federal student loans currently serviced by Nelnet until very recently. The new payment amount that is effective starting with the XX/XX/XXXX payment will nearly double my payment from {$180.00} per month to {$360.00} per month. I'm fortunate that my current financial position allows me to be able to handle the increased payment amount, but I am sure there are likely many borrowers who are not in the same position. The re-amortization should have occurred once the forbearance ended to reduce the impact to borrowers.\", 'Company public response': 'None', 'Company': 'Nelnet, Inc.', 'State': 'IL', 'ZIP code': '60030', 'Tags': 'None', 'Consumer consent provided?': 'Consent provided', 'Submitted via': 'Web', 'Date sent to company': '03/27/25', 'Company response to consumer': 'Closed with explanation', 'Timely response?': 'Yes', 'Consumer disputed?': 'N/A', 'Complaint ID': '12686613'}, page_content=\"The federal student loan COVID-19 forbearance program ended in XX/XX/XXXX. However, payments were not re-amortized on my federal student loans currently serviced by Nelnet until very recently. The new payment amount that is effective starting with the XX/XX/XXXX payment will nearly double my payment from {$180.00} per month to {$360.00} per month. I'm fortunate that my current financial position allows me to be able to handle the increased payment amount, but I am sure there are likely many borrowers who are not in the same position. The re-amortization should have occurred once the forbearance ended to reduce the impact to borrowers.\")"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "loan_complaint_data[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lWaQpdHl0Gzc"
      },
      "source": [
        "## Task 3: Setting up QDrant!\n",
        "\n",
        "Now that we have our documents, let's create a QDrant VectorStore with the collection name \"LoanComplaints\".\n",
        "\n",
        "We'll leverage OpenAI's [`text-embedding-3-small`](https://openai.com/blog/new-embedding-models-and-api-updates) because it's a very powerful (and low-cost) embedding model.\n",
        "\n",
        "> NOTE: We'll be creating additional vectorstores where necessary, but this pattern is still extremely useful."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "NT8ihRJbYmMT"
      },
      "outputs": [],
      "source": [
        "from langchain_community.vectorstores import Qdrant\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "\n",
        "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
        "\n",
        "vectorstore = Qdrant.from_documents(\n",
        "    loan_complaint_data,\n",
        "    embeddings,\n",
        "    location=\":memory:\",\n",
        "    collection_name=\"LoanComplaints\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-x2SS4Rh0hiN"
      },
      "source": [
        "## Task 4: Naive RAG Chain\n",
        "\n",
        "Since we're focusing on the \"R\" in RAG today - we'll create our Retriever first."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NEH7X5Ai08FH"
      },
      "source": [
        "### R - Retrieval\n",
        "\n",
        "This naive retriever will simply look at each review as a document, and use cosine-similarity to fetch the 10 most relevant documents.\n",
        "\n",
        "> NOTE: We're choosing `10` as our `k` here to provide enough documents for our reranking process later"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "GFDPrNBtb72o"
      },
      "outputs": [],
      "source": [
        "naive_retriever = vectorstore.as_retriever(search_kwargs={\"k\" : 10})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MbBhyQjz06dx"
      },
      "source": [
        "### A - Augmented\n",
        "\n",
        "We're going to go with a standard prompt for our simple RAG chain today! Nothing fancy here, we want this to mostly be about the Retrieval process."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "7uSz-Dbqcoki"
      },
      "outputs": [],
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "RAG_TEMPLATE = \"\"\"\\\n",
        "You are a helpful and kind assistant. Use the context provided below to answer the question.\n",
        "\n",
        "If you do not know the answer, or are unsure, say you don't know.\n",
        "\n",
        "Query:\n",
        "{question}\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "\"\"\"\n",
        "\n",
        "rag_prompt = ChatPromptTemplate.from_template(RAG_TEMPLATE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BlRzpb231GGJ"
      },
      "source": [
        "### G - Generation\n",
        "\n",
        "We're going to leverage `gpt-4.1-nano` as our LLM today, as - again - we want this to largely be about the Retrieval process."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "c-1t9H60dJLg"
      },
      "outputs": [],
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "chat_model = ChatOpenAI(model=\"gpt-4.1-nano\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mg3QRGzA1M2x"
      },
      "source": [
        "### LCEL RAG Chain\n",
        "\n",
        "We're going to use LCEL to construct our chain.\n",
        "\n",
        "> NOTE: This chain will be exactly the same across the various examples with the exception of our Retriever!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "0bvstS7mdOW3"
      },
      "outputs": [],
      "source": [
        "from langchain_core.runnables import RunnablePassthrough\n",
        "from operator import itemgetter\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "naive_retrieval_chain = (\n",
        "    # INVOKE CHAIN WITH: {\"question\" : \"<<SOME USER QUESTION>>\"}\n",
        "    # \"question\" : populated by getting the value of the \"question\" key\n",
        "    # \"context\"  : populated by getting the value of the \"question\" key and chaining it into the base_retriever\n",
        "    {\"context\": itemgetter(\"question\") | naive_retriever, \"question\": itemgetter(\"question\")}\n",
        "    # \"context\"  : is assigned to a RunnablePassthrough object (will not be called or considered in the next step)\n",
        "    #              by getting the value of the \"context\" key from the previous step\n",
        "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
        "    # \"response\" : the \"context\" and \"question\" values are used to format our prompt object and then piped\n",
        "    #              into the LLM and stored in a key called \"response\"\n",
        "    # \"context\"  : populated by getting the value of the \"context\" key from the previous step\n",
        "    | {\"response\": rag_prompt | chat_model, \"context\": itemgetter(\"context\")}\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "izKujhNb1ZG8"
      },
      "source": [
        "Let's see how this simple chain does on a few different prompts.\n",
        "\n",
        "> NOTE: You might think that we've cherry picked prompts that showcase the individual skill of each of the retrieval strategies - you'd be correct!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "LI-5ueEddku9",
        "outputId": "7f3cec18-5f4e-41bb-cf71-51ba0be5388e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\"Based on the provided complaints, one of the most common issues with loans appears to be mismanagement and errors by loan servicers. Specifically, many complaints relate to errors in loan balances, misapplied payments, incorrect or inconsistent reporting of loan status on credit reports, and issues with repayment plans or loan transfers without proper notification. \\n\\nIn addition, other frequent issues include dealing with servicer misconduct, such as providing bad information, unlawful collection actions, and difficulties in applying payments correctly. These problems can significantly impact borrowers' credit scores, financial stability, and ability to manage their loans effectively.\\n\\nTherefore, the most common issue with loans, as reflected in these complaints, is **mismanagement and errors by loan servicers, including incorrect balances, misapplied payments, and lack of transparency or communication**.\""
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "naive_retrieval_chain.invoke({\"question\" : \"What is the most common issue with loans?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "43zdcdUydtXh",
        "outputId": "db874e67-f568-4ed1-b863-b7c17b387052"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Yes, based on the provided complaints, several complaints indicate that they were not handled in a timely manner. Specifically, complaints with Complaint IDs 12709087 and 13062402 were marked as \"Not timely,\" and others mention delays or lack of response over extended periods, such as over a year or several months. Therefore, multiple complaints did not get handled in a timely manner.'"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "naive_retrieval_chain.invoke({\"question\" : \"Did any complaints not get handled in a timely manner?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "lpG6rlvvvKFq",
        "outputId": "a1b330b0-628e-41be-d829-9c1d55e781f5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\"People failed to pay back their loans for several reasons, including:\\n\\n1. **Accumulation of Interest and Unmanageable Balances:** Many borrowers found that interest continued to accrue even when they deferred or put their loans into forbearance. This caused their balances to grow over time, making it difficult to pay off the loans and leading to larger total debt.\\n\\n2. **Limited and Unfavorable Repayment Options:** Borrowers were often only offered options like forbearance or deferment, which did not reduce the overall debt but allowed interest to accumulate. Lowering monthly payments was sometimes unaffordable, and increasing payments to accelerate payoff was not feasible for many.\\n\\n3. **Lack of Clear Communication or Transparency:** Some borrowers were not properly notified about transfers between loan servicers, initiation of repayment, or changes in their payment status. This lack of information led to missed payments, delinquencies, or defaults.\\n\\n4. **Financial Hardships and Economic Conditions:** Borrowers faced financial hardships, stagnant wages, and economic downturns, which made it impossible to keep up with payments, especially when combined with rising interest and lack of income-based repayment options.\\n\\n5. **Mismanagement and Poor Servicing Practices:** Complaints included issues like loans being transferred without proper notification, inability to apply payments to principal, or being subject to predatory repayment schemes, which further hindered repayment efforts.\\n\\n6. **Unclear or Incorrect Information and Debt Disputes:** Some borrowers discovered discrepancies in their account balances and interest calculations, and lacked access to documentation or breakdowns explaining their debt, leading to confusion and difficulty in managing repayment.\\n\\nIn summary, the combination of rising interest, insufficient communication, limited flexible repayment options, economic hardship, and servicing issues contributed to many people's inability to pay back their loans.\""
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "naive_retrieval_chain.invoke({\"question\" : \"Why did people fail to pay back their loans?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jsbfQmbr1leg"
      },
      "source": [
        "Overall, this is not bad! Let's see if we can make it better!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ft1vt8HPR16w"
      },
      "source": [
        "## Task 5: Best-Matching 25 (BM25) Retriever\n",
        "\n",
        "Taking a step back in time - [BM25](https://www.nowpublishers.com/article/Details/INR-019) is based on [Bag-Of-Words](https://en.wikipedia.org/wiki/Bag-of-words_model) which is a sparse representation of text.\n",
        "\n",
        "In essence, it's a way to compare how similar two pieces of text are based on the words they both contain.\n",
        "\n",
        "This retriever is very straightforward to set-up! Let's see it happen down below!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "qdF4wuj5R-cG"
      },
      "outputs": [],
      "source": [
        "from langchain_community.retrievers import BM25Retriever\n",
        "\n",
        "bm25_retriever = BM25Retriever.from_documents(loan_complaint_data, )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KIjJlBQ8drKH"
      },
      "source": [
        "We'll construct the same chain - only changing the retriever."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "WR15EQG7SLuw"
      },
      "outputs": [],
      "source": [
        "bm25_retrieval_chain = (\n",
        "    {\"context\": itemgetter(\"question\") | bm25_retriever, \"question\": itemgetter(\"question\")}\n",
        "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
        "    | {\"response\": rag_prompt | chat_model, \"context\": itemgetter(\"context\")}\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Gi-yXCDdvJk"
      },
      "source": [
        "Let's look at the responses!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "oY9qzmm3SOrF",
        "outputId": "4d4f450f-5978-460f-f242-b32407868353"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Based on the provided context, the most common issue with loans appears to be problems related to dealing with lenders or servicers. Specifically, these issues include disputes over fees charged, difficulties applying payments correctly (such as only being able to allocate funds in a specific way that may be unfavorable to the borrower), receiving inaccurate or insufficient information about loan balances and terms, and unresolved complaints involving miscommunication or alleged dishonesty by the servicers. \\n\\nTherefore, the most common issue with loans, as indicated by these complaints, is challenges in dealing with lenders or servicers, often involving disputes over fees, payment application, or information accuracy.'"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "bm25_retrieval_chain.invoke({\"question\" : \"What is the most common issue with loans?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "igfinyneSQkh",
        "outputId": "9752d4a9-dd16-45b1-f63f-a76e93a05eb3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Based on the provided information, all the complaints in the context received timely responses from the companies. The entries for each complaint include a \"Timely response?\" status marked as \"Yes.\" Therefore, there is no evidence in the provided data to suggest that any complaints were not handled in a timely manner.'"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "bm25_retrieval_chain.invoke({\"question\" : \"Did any complaints not get handled in a timely manner?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "w0H7pV_USSMQ",
        "outputId": "bdead654-3109-4143-9a30-e1d6ca8dc534"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'People failed to pay back their loans for various reasons, including problems with loan servicing and payment plans, poor communication from loan servicers, and issues with the transfer or management of their loans. Specific examples from the complaints indicate that some borrowers were misled into incorrect payment plans, experienced unauthorized transfers of their loans to different companies without proper notification, or did not receive responses to their requests for forbearance or deferment. Additionally, some borrowers faced technical issues such as reversed payments, inaccurate billing, or lack of assistance when attempting to resolve repayment problems. Overall, these issues contributed to borrowers being unable to repay their loans as scheduled.'"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "bm25_retrieval_chain.invoke({\"question\" : \"Why did people fail to pay back their loans?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zvg5xHaUdxCl"
      },
      "source": [
        "It's not clear that this is better or worse, if only we had a way to test this (SPOILERS: We do, the second half of the notebook will cover this)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### ❓ Question #1:\n",
        "\n",
        "Give an example query where BM25 is better than embeddings and justify your answer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\"The specific issues with auto-debit payments at Nelnet include:\\n\\n1. Auto-debit setup confirmations are sometimes followed by record deletions, leading borrowers to believe their auto-pay enrollment is confirmed when it is actually not active.\\n2. Payments that are supposed to process via auto-debit may not be executed or processed correctly, resulting in missed payments.\\n3. Auto-debit accounts can be placed on forbearance or have their payment amounts changed without the borrower's consent or prior notification.\\n4. Borrowers report difficulty in accessing their payment history or statements for previous payments, especially after transfers from other servicers.\\n5. Automatic payments can be incorrectly marked as missed or late, leading to credit reporting errors and damage.\\n6. Auto-debit payments may be returned or rejected by the bank without clear notification from Nelnet, and borrowers often are not informed promptly.\\n7. Changes to autopay amounts or status, such as being placed into forbearance or reduced payments, occur without prior communication.\\n8. Auto-pay can be canceled or stopped without the borrower’s knowledge due to technical or administrative errors, leading to unnotified missed payments and credit impact.\\n9. Communication failures, including lack of notifications about payment issues, delays, or account status changes, cause confusion and credit reporting problems.\\n\\nOverall, borrowers have experienced significant difficulties with the handling, communication, and reliability of auto-debit payments through Nelnet.\""
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "naive_retrieval_chain.invoke({\"question\" : \"What are the specific issues with auto-debit payments at Nelnet?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\"The specific issues with auto-debit payments at Nelnet, based on the complaints, include:\\n\\n1. Payments not processing despite receiving confirmation of auto-debit setup.\\n2. The company's records of auto-debit enrollment being deleted or not retained, leading to repeated failures.\\n3. Conflicting information between confirmation letters and website records, causing confusion about enrollment status.\\n4. Suspected manipulation or skimming of payment amounts, potentially reducing payment discounts unfairly.\\n5. Overall, borrowers experience difficulties in ensuring automatic payments are correctly set up and maintained consistently.\\n\\nIf you need further assistance or details, please let me know!\""
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "bm25_retrieval_chain.invoke({\"question\" : \"What are the specific issues with auto-debit payments at Nelnet?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'The complaints involving FERPA violations and data breaches primarily concern the unauthorized disclosure or misuse of personal and educational records protected under FERPA. Specifically, these complaints include:\\n\\n- Complaints about personal and financial data being compromised, violating FERPA, such as reports from Nelnet, Inc., EdFinancial Services, MOHELA, Maximus Federal Services, Inc., and others, citing data breaches, improper use, or unauthorized access to student education records.\\n- Concerns raised about potential violations of the Family Educational Rights and Privacy Act (FERPA), alleging that confidentiality of student records was breached without permission or appropriate safeguards.\\n\\nIn summary, the issues involve data breaches and improper handling of student or personal information that violate FERPA regulations.'"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "naive_retrieval_chain.invoke({\"question\" : \"What complaints involve FERPA violations and data breaches?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Yes, there are complaints involving FERPA violations and data breaches. Specifically, several complaints detail unauthorized access, disclosure, or mishandling of educational records, personally identifiable information (PII), and financial data in violation of FERPA. These complaints often mention breaches of student records, exposure of sensitive information without consent, and attempts to access or share data improperly. Additionally, some complaints explicitly describe data breaches involving student loan records and private information, which are also consistent with FERPA violations and cybersecurity issues.'"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "bm25_retrieval_chain.invoke({\"question\" : \"What complaints involve FERPA violations and data breaches?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### ✅ Answer:\n",
        "\n",
        "\"What are the specific issues with auto-debit payments at Nelnet?\"\n",
        "\n",
        "BM 25 seems to perform better in anwering this question because it does exact term matching in finding the documents containing those the terms - Nelnet, payments, auto-debit. Company-specific complaints that require exact name matching.\n",
        "\n",
        "Embedding Limitation for this query:\n",
        "Embeddings might retrive documents about payment issues in general and not specifically about Nelnet or auto-debit.\n",
        "\n",
        "This type of query represents a scenario where precision over recall is more important, and BM25's bag-of-words approach with exact term matching provides better results than the semantic understanding of embeddings."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q-dcbFn2vpZF"
      },
      "source": [
        "## Task 6: Contextual Compression (Using Reranking)\n",
        "\n",
        "Contextual Compression is a fairly straightforward idea: We want to \"compress\" our retrieved context into just the most useful bits.\n",
        "\n",
        "There are a few ways we can achieve this - but we're going to look at a specific example called reranking.\n",
        "\n",
        "The basic idea here is this:\n",
        "\n",
        "- We retrieve lots of documents that are very likely related to our query vector\n",
        "- We \"compress\" those documents into a smaller set of *more* related documents using a reranking algorithm.\n",
        "\n",
        "We'll be leveraging Cohere's Rerank model for our reranker today!\n",
        "\n",
        "All we need to do is the following:\n",
        "\n",
        "- Create a basic retriever\n",
        "- Create a compressor (reranker, in this case)\n",
        "\n",
        "That's it!\n",
        "\n",
        "Let's see it in the code below!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "psHvO2K1v_ZQ"
      },
      "outputs": [],
      "source": [
        "from langchain.retrievers.contextual_compression import ContextualCompressionRetriever\n",
        "from langchain_cohere import CohereRerank\n",
        "\n",
        "compressor = CohereRerank(model=\"rerank-v3.5\")\n",
        "compression_retriever = ContextualCompressionRetriever(\n",
        "    base_compressor=compressor, base_retriever=naive_retriever\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_TA9RB2x-j7P"
      },
      "source": [
        "Let's create our chain again, and see how this does!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "1BXqmxvHwX6T"
      },
      "outputs": [],
      "source": [
        "contextual_compression_retrieval_chain = (\n",
        "    {\"context\": itemgetter(\"question\") | compression_retriever, \"question\": itemgetter(\"question\")}\n",
        "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
        "    | {\"response\": rag_prompt | chat_model, \"context\": itemgetter(\"context\")}\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "V3iGpokswcBb",
        "outputId": "f15d2aa1-5e8b-417d-f623-eb835d072e59"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Based on the provided context, the most common issue with loans appears to be problems related to dealing with lenders or servicers, particularly errors in loan balances, misapplied payments, incorrect or incomplete information, and mishandling of loan data. Many complaints involve discrepancies in loan amounts, lack of clear communication or documentation, and issues with unapproved transfers or unauthorized disclosures.'"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "contextual_compression_retrieval_chain.invoke({\"question\" : \"What is the most common issue with loans?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "7u_k0i4OweUd",
        "outputId": "be5fccc8-2352-4189-c524-bbeaa28cf799"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Based on the provided information, yes, there are complaints that did not get handled in a timely manner. For example:\\n\\n- One complaint regarding student loans (row 503) has been open for nearly 18 months without resolution, with the consumer still awaiting a response and resolution.\\n- Several other complaints also mention ongoing issues and delays, such as unresolved payment application issues and problems with account updates.\\n\\nHowever, the complaint from Maximus Federal Services received a response labeled \"Closed with explanation\" and was responded to in a timely manner, so not all complaints experienced delays.\\n\\nOverall, at least some complaints remained unresolved for extended periods, indicating they were not handled in a timely manner.'"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "contextual_compression_retrieval_chain.invoke({\"question\" : \"Did any complaints not get handled in a timely manner?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "zn1EqaGqweXN",
        "outputId": "42bc5972-4164-46eb-f49d-4272f39bb89b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'People failed to pay back their loans primarily due to a lack of clear communication and understanding of their loan obligations, as well as the accumulation of interest over time. Many borrowers were not informed about their need to repay loans or how interest would grow, especially when they were unaware of ownership transfers or lacked access to their account details. Additionally, options such as forbearance and deferment, while available, often led to interest continuing to accrue, increasing the total amount owed rather than decreasing it. The complexity of loan management, combined with limited support and information from loan servicers, contributed to borrowers struggling to make payments or inadvertently falling behind.'"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "contextual_compression_retrieval_chain.invoke({\"question\" : \"Why did people fail to pay back their loans?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OEbT0g2S-mZ4"
      },
      "source": [
        "We'll need to rely on something like Ragas to help us get a better sense of how this is performing overall - but it \"feels\" better!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qqbghrBEQNn5"
      },
      "source": [
        "## Task 7: Multi-Query Retriever\n",
        "\n",
        "Typically in RAG we have a single query - the one provided by the user.\n",
        "\n",
        "What if we had....more than one query!\n",
        "\n",
        "In essence, a Multi-Query Retriever works by:\n",
        "\n",
        "1. Taking the original user query and creating `n` number of new user queries using an LLM.\n",
        "2. Retrieving documents for each query.\n",
        "3. Using all unique retrieved documents as context\n",
        "\n",
        "So, how is it to set-up? Not bad! Let's see it down below!\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "pfM26ReXQjzU"
      },
      "outputs": [],
      "source": [
        "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
        "\n",
        "multi_query_retriever = MultiQueryRetriever.from_llm(\n",
        "    retriever=naive_retriever, llm=chat_model\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "1vRc129jQ5WW"
      },
      "outputs": [],
      "source": [
        "multi_query_retrieval_chain = (\n",
        "    {\"context\": itemgetter(\"question\") | multi_query_retriever, \"question\": itemgetter(\"question\")}\n",
        "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
        "    | {\"response\": rag_prompt | chat_model, \"context\": itemgetter(\"context\")}\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "CGgNuOb3Q3M9",
        "outputId": "c5273ecf-da35-40b8-fbdb-0f8beab425f7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\"The most common issue with loans, based on the provided complaints and context, appears to be problems related to the handling and servicing of student loans. This includes:\\n\\n- Dealing with lenders or servicers who mishandle payments, misapply payments, or apply them incorrectly (e.g., applying to interest instead of principal).\\n- Errors in loan balances and interest calculations.\\n- Lack of transparent communication or failure to provide necessary documentation like original promissory notes.\\n- Unjustified increases in interest rates or balances, often due to forbearances or transfers between servicers.\\n- Problems with loan repayment plans, such as being steered into unsuitable options or facing difficulty in applying additional payments to principal.\\n- Erroneous or unauthorized loans appearing on credit reports.\\n- Servicers' failure to verify or maintain proper legal documentation, such as signed master promissory notes.\\n\\nOverall, the most prevalent issue is the mishandling and mismanagement of loan servicing, leading to errors, misinformation, and adverse impacts on borrowers' credit and finances.\""
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "multi_query_retrieval_chain.invoke({\"question\" : \"What is the most common issue with loans?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "aAlSthxrRDBC",
        "outputId": "230ff807-23ae-4d25-8d11-cfdbed0b77cb"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Based on the provided information, yes, some complaints did not get handled in a timely manner. Specifically, one complaint received a response marked as \"No\" for timely response, indicating it was late. Additionally, there are multiple instances where complainants reported waiting over long periods (hours) without resolution, or their issues remained unresolved for over a year despite multiple follow-ups. Therefore, it can be concluded that certain complaints were not addressed promptly.'"
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "multi_query_retrieval_chain.invoke({\"question\" : \"Did any complaints not get handled in a timely manner?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "id": "Uv1mpCK8REs4",
        "outputId": "00fbc22a-ed9b-4613-9695-0b179e3f8369"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'People failed to pay back their loans primarily due to a combination of factors highlighted in these complaints:\\n\\n1. **Accumulation of Interest and Unmanageable Balances**: Many borrowers reported that interest continued to accrue during forbearance or deferment periods, sometimes capitalizing (adding to principal), which increased the total amount owed and made repayment more difficult.\\n\\n2. **Lack of Clear and Accurate Information from Servicers**: Several complaints cite servicers steering borrowers into forbearance or consolidations without informing them of better options like income-driven repayment plans or rehabilitation, leading to increased balances and loss of forgiveness eligibility.\\n\\n3. **Financial Hardship and Economic Conditions**: Borrowers faced hardships such as unemployment, low income, or unexpected expenses, making it physically or financially impossible to increase monthly payments without sacrificing essentials.\\n\\n4. **Mismanagement and Lack of Transparency**: Reports include issues like incorrect account statuses, misapplied payments, errors in balances, or transfer of loans without proper notification, all of which hindered borrowers’ ability to manage and repay their loans effectively.\\n\\n5. **Procedural and Servicing Practices**: Tactics such as “forbearance steering,” coercive consolidation, and inadequate guidance about repayment options contributed to the inability to qualify for loan forgiveness or to reduce debt burdens.\\n\\n6. **Inadequate Support and Communication**: Many borrowers experienced unhelpful customer service, received little to no notice about loan status changes, or were misled about options, leading to default or increased difficulty in repayment.\\n\\nIn summary, the failure to pay back loans was driven by the combination of rising balances due to interest accrual, lack of proper information and support, economic hardship, and systemic issues within student loan servicing practices.'"
            ]
          },
          "execution_count": 33,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "multi_query_retrieval_chain.invoke({\"question\" : \"Why did people fail to pay back their loans?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### ❓ Question #2:\n",
        "\n",
        "Explain how generating multiple reformulations of a user query can improve recall."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### ✅ Answer:\n",
        "\n",
        "1. Query expansion and Synonym coverage\n",
        "Each reformulation uses different terminology that might appear in different documents, capturing documents that the original query might miss.\n",
        "\n",
        "2. Multiple Perspective Capture\n",
        "Multi query captures different perspectives like technical issues, customer service, legal aspects.\n",
        "\n",
        "3. Reduced Query Bias\n",
        "A single query might be too specific or too general, multiple reformulations provide a range of specificity levels\n",
        "\n",
        "Improved Recall: The multi-query approach significantly increases the number of potentially relevant documents retrieved."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EDEawBf_d_3G"
      },
      "source": [
        "## Task 8: Parent Document Retriever\n",
        "\n",
        "A \"small-to-big\" strategy - the Parent Document Retriever works based on a simple strategy:\n",
        "\n",
        "1. Each un-split \"document\" will be designated as a \"parent document\" (You could use larger chunks of document as well, but our data format allows us to consider the overall document as the parent chunk)\n",
        "2. Store those \"parent documents\" in a memory store (not a VectorStore)\n",
        "3. We will chunk each of those documents into smaller documents, and associate them with their respective parents, and store those in a VectorStore. We'll call those \"child chunks\".\n",
        "4. When we query our Retriever, we will do a similarity search comparing our query vector to the \"child chunks\".\n",
        "5. Instead of returning the \"child chunks\", we'll return their associated \"parent chunks\".\n",
        "\n",
        "Okay, maybe that was a few steps - but the basic idea is this:\n",
        "\n",
        "- Search for small documents\n",
        "- Return big documents\n",
        "\n",
        "The intuition is that we're likely to find the most relevant information by limiting the amount of semantic information that is encoded in each embedding vector - but we're likely to miss relevant surrounding context if we only use that information.\n",
        "\n",
        "Let's start by creating our \"parent documents\" and defining a `RecursiveCharacterTextSplitter`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "qJ53JJuMd_ZH"
      },
      "outputs": [],
      "source": [
        "from langchain.retrievers import ParentDocumentRetriever\n",
        "from langchain.storage import InMemoryStore\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from qdrant_client import QdrantClient, models\n",
        "\n",
        "parent_docs = loan_complaint_data\n",
        "child_splitter = RecursiveCharacterTextSplitter(chunk_size=750)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oOpXfVUH3gL3"
      },
      "source": [
        "We'll need to set up a new QDrant vectorstore - and we'll use another useful pattern to do so!\n",
        "\n",
        "> NOTE: We are manually defining our embedding dimension, you'll need to change this if you're using a different embedding model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rzFc-_9HlGQ-",
        "outputId": "223662dd-c36f-42f7-d1b0-b086e571484e"
      },
      "outputs": [],
      "source": [
        "from langchain_qdrant import QdrantVectorStore\n",
        "\n",
        "client = QdrantClient(location=\":memory:\")\n",
        "\n",
        "client.create_collection(\n",
        "    collection_name=\"full_documents\",\n",
        "    vectors_config=models.VectorParams(size=1536, distance=models.Distance.COSINE)\n",
        ")\n",
        "\n",
        "parent_document_vectorstore = QdrantVectorStore(\n",
        "    collection_name=\"full_documents\", embedding=OpenAIEmbeddings(model=\"text-embedding-3-small\"), client=client\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sf_g95FA3s6w"
      },
      "source": [
        "Now we can create our `InMemoryStore` that will hold our \"parent documents\" - and build our retriever!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "BpWVjPf4fLUp"
      },
      "outputs": [],
      "source": [
        "store = InMemoryStore()\n",
        "\n",
        "parent_document_retriever = ParentDocumentRetriever(\n",
        "    vectorstore = parent_document_vectorstore,\n",
        "    docstore=store,\n",
        "    child_splitter=child_splitter,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KoYmSWfE32Zo"
      },
      "source": [
        "By default, this is empty as we haven't added any documents - let's add some now!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "iQ2ZzfKigMZc"
      },
      "outputs": [],
      "source": [
        "parent_document_retriever.add_documents(parent_docs, ids=None)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bI7Tip1335rE"
      },
      "source": [
        "We'll create the same chain we did before - but substitute our new `parent_document_retriever`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "Qq_adt2KlSqp"
      },
      "outputs": [],
      "source": [
        "parent_document_retrieval_chain = (\n",
        "    {\"context\": itemgetter(\"question\") | parent_document_retriever, \"question\": itemgetter(\"question\")}\n",
        "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
        "    | {\"response\": rag_prompt | chat_model, \"context\": itemgetter(\"context\")}\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jNolUVQb4Apt"
      },
      "source": [
        "Let's give it a whirl!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "TXB5i89Zly5W",
        "outputId": "94c240be-7c5b-4c58-9eee-56d93285a054"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'The most common issue with loans, based on the complaints provided, appears to be related to errors and misconduct in federal student loan servicing. Specific recurring problems include incorrect information on credit reports, misapplication of payments, wrongful denials of payment plans, discrepancies in loan balances and interest rates, and issues with collection and verification of debts.'"
            ]
          },
          "execution_count": 39,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "parent_document_retrieval_chain.invoke({\"question\" : \"What is the most common issue with loans?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "V5F1T-wNl3cg",
        "outputId": "9b81e72e-5db7-4b8a-b25b-400ea0df5335"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Based on the provided information, it appears that several complaints were not handled in a timely manner. Specifically, the complaints related to the student loan issues with MOHELA (Complaint IDs 12709087 and 12935889) indicate that the responses were \"No\" in the \"Timely response?\" field, meaning they were not handled promptly. Additionally, the complaint about the dispute settlement with Nelnet (Complaint ID 13205525) was responded to within the expected timeframe (\"Yes\" in \"Timely response?\"). \\n\\nTherefore, yes, some complaints—particularly those regarding MOHELA—did not get handled in a timely manner.'"
            ]
          },
          "execution_count": 40,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "parent_document_retrieval_chain.invoke({\"question\" : \"Did any complaints not get handled in a timely manner?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "ZqARszGzvGcG",
        "outputId": "8867f83c-db13-4db4-d57f-9bd51d32cd8a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'People failed to pay back their loans for various reasons, including:\\n\\n1. Lack of proper communication or notification from loan servicers about payment obligations, as indicated by complaints about not being notified when payments were due or about changes in loan ownership.\\n2. Financial hardship or severe economic difficulties that made it impossible to make timely payments, such as unemployment or inability to find employment in their field.\\n3. Misrepresentation or lack of transparency from educational institutions and loan providers regarding the long-term financial consequences, job prospects after graduation, and the sustainability of the school’s operations.\\n4. Relying on deferment and forbearance options that increased interest and debt over time.\\n5. Disputes over the legitimacy or ownership of the debt, including issues related to the legal verification of loans and deceptive practices by collection agencies.\\n6. Personal health issues or other personal circumstances that hindered ability to make payments.\\n\\nIn summary, failure to repay loans often resulted from a combination of financial hardship, lack of clear communication, and sometimes the mismanagement or misrepresentation by educational and loan servicing entities.'"
            ]
          },
          "execution_count": 41,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "parent_document_retrieval_chain.invoke({\"question\" : \"Why did people fail to pay back their loans?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B41cj42s4DPM"
      },
      "source": [
        "Overall, the performance *seems* largely the same. We can leverage a tool like [Ragas]() to more effectively answer the question about the performance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VUrIBKl_TwS9"
      },
      "source": [
        "## Task 9: Ensemble Retriever\n",
        "\n",
        "In brief, an Ensemble Retriever simply takes 2, or more, retrievers and combines their retrieved documents based on a rank-fusion algorithm.\n",
        "\n",
        "In this case - we're using the [Reciprocal Rank Fusion](https://plg.uwaterloo.ca/~gvcormac/cormacksigir09-rrf.pdf) algorithm.\n",
        "\n",
        "Setting it up is as easy as providing a list of our desired retrievers - and the weights for each retriever."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "8j7jpZsKTxic"
      },
      "outputs": [],
      "source": [
        "from langchain.retrievers import EnsembleRetriever\n",
        "\n",
        "retriever_list = [bm25_retriever, naive_retriever, parent_document_retriever, compression_retriever, multi_query_retriever]\n",
        "equal_weighting = [1/len(retriever_list)] * len(retriever_list)\n",
        "\n",
        "ensemble_retriever = EnsembleRetriever(\n",
        "    retrievers=retriever_list, weights=equal_weighting\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kpo9Psl5hhJ-"
      },
      "source": [
        "We'll pack *all* of these retrievers together in an ensemble."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "KZ__EZwpUKkd"
      },
      "outputs": [],
      "source": [
        "ensemble_retrieval_chain = (\n",
        "    {\"context\": itemgetter(\"question\") | ensemble_retriever, \"question\": itemgetter(\"question\")}\n",
        "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
        "    | {\"response\": rag_prompt | chat_model, \"context\": itemgetter(\"context\")}\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SSsvHpRMj24L"
      },
      "source": [
        "Let's look at our results!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "0lMvqL88UQI-",
        "outputId": "d86dd5f7-0a13-4836-c0ce-cc4c431fd889"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'The most common issue with loans, based on the provided data, appears to be dealing with the loan servicer or lender, including errors in loan balances, misapplied payments, wrongful denials of payment plans, and problems with how payments are being handled. Several complaints highlight issues such as receiving bad information about loans, inability to properly apply payments to principal, inaccurate reporting of delinquency, and mishandling of loan transfers or consolidations. \\n\\nIn summary, a predominant and recurring problem is the mismanagement and poor communication from loan servicers, which leads to misapplied payments, incorrect account information, and difficulties in resolving repayment issues.'"
            ]
          },
          "execution_count": 44,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "ensemble_retrieval_chain.invoke({\"question\" : \"What is the most common issue with loans?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "MNFWLYECURI1",
        "outputId": "b17973b5-66a9-4481-97d5-880b5754b5c5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Based on the provided complaints, yes, there are several instances indicating complaints not handled in a timely manner. For example:\\n\\n- One complaint (#12935889) about Mohela was marked as \"Timely response?\": No.\\n- Another (#12744910) regarding inaccuracies in reporting and an ongoing dispute was \"Timely response?\": Yes, but the complaint was about inaccurate reporting and delays in correction, suggesting the issue persisted over time.\\n- Multiple complaints (#12739706, #13062402, #13126709, #13127090, and others) mention delays, extended wait times, or responses that were not addressed promptly, with some even explicitly stating they did not receive responses within expected timeframes.\\n- There are cases where the response was \"Closed with explanation\" but the delays or unresolved issues strongly imply they were not handled promptly or adequately.\\n\\nOverall, the evidence suggests that at least some complaints were not handled in a timely manner, as indicated directly by the response statuses and detailed descriptions of delays.'"
            ]
          },
          "execution_count": 45,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "ensemble_retrieval_chain.invoke({\"question\" : \"Did any complaints not get handled in a timely manner?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "id": "A7qbHfWgUR4c",
        "outputId": "f7373144-59ef-4fc7-b75d-ca00e7df881e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'People failed to pay back their loans for several reasons, often related to mismanagement, misinformation, and systemic issues. Based on the provided complaints, common reasons include:\\n\\n1. **Lack of Notification and Communication:** Many borrowers were not properly notified about loan transfers, due dates, or repayment start dates, leading to unintentional delinquency and missed payments.\\n\\n2. **Misleading or Incomplete Information:** Borrowers reported receiving incorrect or misleading information about their loan balances, repayment obligations, or eligibility for programs like income-driven repayment or forgiveness, which caused confusion and unintended default.\\n\\n3. **System Errors and Technical Difficulties:** Issues such as online portal lockouts, incorrect account statuses, and errors in reporting contributed to borrowers not making payments or being marked delinquent improperly.\\n\\n4. **Inadequate Support and Assistance:** Borrowers often found customer service unhelpful, unresponsive, or dismissive, preventing them from arranging manageable payment plans or resolving discrepancies.\\n\\n5. **Financial Hardship and Economic Factors:** Some borrowers experienced severe financial difficulties, unemployment, or health issues that made repayment impossible despite intentions to pay.\\n\\n6. **Systemic and Administrative Failures:** Transitions between servicers, erroneous reporting to credit bureaus, and failure to follow regulations led to credit damage and further complications in repayment.\\n\\nIn summary, failures to pay back loans often stemmed from a combination of administrative mismanagement, inadequate communication, lack of clarity about repayment options, and financial hardships.'"
            ]
          },
          "execution_count": 46,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "ensemble_retrieval_chain.invoke({\"question\" : \"Why did people fail to pay back their loans?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MopbkNJAXVaN"
      },
      "source": [
        "## Task 10: Semantic Chunking\n",
        "\n",
        "While this is not a retrieval method - it *is* an effective way of increasing retrieval performance on corpora that have clean semantic breaks in them.\n",
        "\n",
        "Essentially, Semantic Chunking is implemented by:\n",
        "\n",
        "1. Embedding all sentences in the corpus.\n",
        "2. Combining or splitting sequences of sentences based on their semantic similarity based on a number of [possible thresholding methods](https://python.langchain.com/docs/how_to/semantic-chunker/):\n",
        "  - `percentile`\n",
        "  - `standard_deviation`\n",
        "  - `interquartile`\n",
        "  - `gradient`\n",
        "3. Each sequence of related sentences is kept as a document!\n",
        "\n",
        "Let's see how to implement this!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U9ciZbFEldv_"
      },
      "source": [
        "We'll use the `percentile` thresholding method for this example which will:\n",
        "\n",
        "Calculate all distances between sentences, and then break apart sequences of setences that exceed a given percentile among all distances."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "66EIEWiEYl5y"
      },
      "outputs": [],
      "source": [
        "from langchain_experimental.text_splitter import SemanticChunker\n",
        "\n",
        "semantic_chunker = SemanticChunker(\n",
        "    embeddings,\n",
        "    breakpoint_threshold_type=\"percentile\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YqoKmz12mhRW"
      },
      "source": [
        "Now we can split our documents."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "ROcV7o68ZIq7"
      },
      "outputs": [],
      "source": [
        "semantic_documents = semantic_chunker.split_documents(loan_complaint_data[:20])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L8-LNC-Xmjex"
      },
      "source": [
        "Let's create a new vector store."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "h3sl9QjyZhIe"
      },
      "outputs": [],
      "source": [
        "semantic_vectorstore = Qdrant.from_documents(\n",
        "    semantic_documents,\n",
        "    embeddings,\n",
        "    location=\":memory:\",\n",
        "    collection_name=\"Loan_Complaint_Data_Semantic_Chunks\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eh_r_-LHmmKn"
      },
      "source": [
        "We'll use naive retrieval for this example."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "odVyDUHwZftc"
      },
      "outputs": [],
      "source": [
        "semantic_retriever = semantic_vectorstore.as_retriever(search_kwargs={\"k\" : 10})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mkeiv_ojmp6G"
      },
      "source": [
        "Finally we can create our classic chain!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "xWE_0J0mZveG"
      },
      "outputs": [],
      "source": [
        "semantic_retrieval_chain = (\n",
        "    {\"context\": itemgetter(\"question\") | semantic_retriever, \"question\": itemgetter(\"question\")}\n",
        "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
        "    | {\"response\": rag_prompt | chat_model, \"context\": itemgetter(\"context\")}\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R5pfjLQ3ms9_"
      },
      "source": [
        "And view the results!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "0lN2j-e4Z0SD",
        "outputId": "ef483e21-7200-4dfc-b8bf-aed4f23587b2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Based on the provided complaints, the most common issues with loans appear to be related to difficulties in communication and account management, such as:\\n\\n- Struggling to repay loans due to errors or issues with payment plans.\\n- Problems with loan reporting, including incorrect or improper reporting of account status or default.\\n- Difficulties in obtaining clear information about loan balances, loan servicer changes, or payment amounts.\\n- Issues with loan servicing companies failing to respond appropriately or failing to verify or process applications.\\n- Unauthorized or illegal reporting and collection practices, including violations of privacy laws.\\n\\nWhile these are specific to student loans in the context provided, a recurring theme is that many complaints involve mismanagement, lack of transparency, or errors in the handling of loans and related information. \\n\\nTherefore, a common underlying issue with loans, especially highlighted here, is **mismanagement or errors in servicing, reporting, or communication that cause confusion, financial hardship, or violations of legal protections.**'"
            ]
          },
          "execution_count": 56,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "semantic_retrieval_chain.invoke({\"question\" : \"What is the most common issue with loans?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "xdqfBH1SZ3f9",
        "outputId": "ed62b2d1-7586-46cc-aaf4-c54192a56155"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Based on the provided complaints, it appears that many complaints were responded to in a timely manner, with responses marked as \\'Yes\\' under the \\'Timely response?\\' field. Notably, several complaints state \"Closed with explanation,\" indicating that they were addressed within the required time frame. \\n\\nHowever, there is at least one complaint regarding a lack of response or handling—specifically, the complaint about Nelnet (row 17). The consumer\\'s narrative details multiple issues with lack of responses and conduct that suggests their complaint was not handled promptly or satisfactorily.\\n\\nIn summary:\\n\\n- Multiple complaints confirm responses were handled in a timely manner.\\n- One complaint (about Nelnet\\'s failure to respond to Certified Mail and ongoing misconduct) indicates that the complaint was not properly handled or responded to, suggesting that some complaints did not get handled in a timely manner.\\n\\nTherefore, yes, some complaints did not get handled in a timely manner, specifically the complaint involving Nelnet\\'s lack of response to legal and misconduct issues.'"
            ]
          },
          "execution_count": 57,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "semantic_retrieval_chain.invoke({\"question\" : \"Did any complaints not get handled in a timely manner?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "rAcAObZnZ4o6",
        "outputId": "3f1cade3-41e4-4e42-ef71-048dd18e5e3a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\"People failed to pay back their loans for various reasons, including issues such as difficulties dealing with their loan servicers, miscommunications or inadequate information about their loan status, problems with payment processing, and disputes over the legitimacy or accuracy of their loan details. Some specific reasons noted in the complaints include receiving bad information about loan statuses, delays or errors in re-amortizing payments after forbearance ended, and inaccurate reports of default or delinquency. Additionally, instances of alleged mismanagement, lack of transparency, or improper handling of personal data have also contributed to borrowers' difficulties in repayment.\""
            ]
          },
          "execution_count": 58,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "semantic_retrieval_chain.invoke({\"question\" : \"Why did people fail to pay back their loans?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### ❓ Question #3:\n",
        "\n",
        "If sentences are short and highly repetitive (e.g., FAQs), how might semantic chunking behave, and how would you adjust the algorithm?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### ✅ Answer:\n",
        "\n",
        "1. Over chunking\n",
        "Semantic chunker would create small redundant chunks losing the broader context about say payment issues\n",
        "\n",
        "2. Similarity threshold issues\n",
        "Sentence embeddings would be very similar and could result in either too many tiny chunks or chunks that are too large\n",
        "\n",
        "\n",
        "3. Context Fragmentation\n",
        "Semantic chunking might break the logical groupings of related questions grouped together\n",
        "\n",
        "Adjust the algorithm\n",
        "\n",
        "1. Increase chunk sizes for repetitive content\n",
        "2. Use different thresholding methods (standard_deviation instead of percentile)\n",
        "3. Implement pre-processing to identify and group repetitive content\n",
        "4. Use hybrid approaches combining rule-based and semantic chunking\n",
        "5. Apply topic-based grouping before semantic chunking\n",
        "6. Adjust retrieval parameters (higher k values) for better coverage\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xk2n3-pnVWDJ"
      },
      "source": [
        "# 🤝 Breakout Room Part #2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2SkJLYwMVZkj"
      },
      "source": [
        "#### 🏗️ Activity #1\n",
        "\n",
        "Your task is to evaluate the various Retriever methods against eachother.\n",
        "\n",
        "You are expected to:\n",
        "\n",
        "1. Create a \"golden dataset\"\n",
        " - Use Synthetic Data Generation (powered by Ragas, or otherwise) to create this dataset\n",
        "2. Evaluate each retriever with *retriever specific* Ragas metrics\n",
        " - Semantic Chunking is not considered a retriever method and will not be required for marks, but you may find it useful to do a \"semantic chunking on\" vs. \"semantic chunking off\" comparision between them\n",
        "3. Compile these in a list and write a small paragraph about which is best for this particular data and why.\n",
        "\n",
        "Your analysis should factor in:\n",
        "  - Cost\n",
        "  - Latency\n",
        "  - Performance\n",
        "\n",
        "> NOTE: This is **NOT** required to be completed in class. Please spend time in your breakout rooms creating a plan before moving on to writing code."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TWAr16a5XMub"
      },
      "source": [
        "##### HINTS:\n",
        "\n",
        "- LangSmith provides detailed information about latency and cost."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "from ragas.llms import LangchainLLMWrapper\n",
        "from ragas.embeddings import LangchainEmbeddingsWrapper\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "generator_llm = LangchainLLMWrapper(ChatOpenAI(model=\"gpt-4.1\"))\n",
        "generator_embeddings = LangchainEmbeddingsWrapper(OpenAIEmbeddings())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "825"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(loan_complaint_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "tgDICngKXLGK"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ea61664a17d44d43af63a526dc8246bf",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Applying SummaryExtractor:   0%|          | 0/14 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0cedbf33377d496bafba14ee79889748",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Applying CustomNodeFilter:   0%|          | 0/20 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Node 8cc167c3-d38a-4746-873b-82cd32ef30d6 does not have a summary. Skipping filtering.\n",
            "Node 86437f7d-064e-4588-8e50-f8039f4c7ff7 does not have a summary. Skipping filtering.\n",
            "Node 4ebb5247-7470-4da2-97f1-4e54236e1b02 does not have a summary. Skipping filtering.\n",
            "Node 42fbb830-2d2f-463d-a452-ede6b69b90c6 does not have a summary. Skipping filtering.\n",
            "Node 3d240637-1cee-4681-aee6-910642f6463b does not have a summary. Skipping filtering.\n",
            "Node bc1e6cd3-4f00-43ee-9ee9-993a1263e75b does not have a summary. Skipping filtering.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4aab4e955090469894b12278c1b9acd4",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Applying [EmbeddingExtractor, ThemesExtractor, NERExtractor]:   0%|          | 0/54 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "875e40c3f0334ec5a5ca5ed076ef2d27",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Applying OverlapScoreBuilder:   0%|          | 0/1 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "73cd6a2f282c49e79713238fe21e3ae4",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating personas:   0%|          | 0/3 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4aee0d724b614636ad76a89a14f08520",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating Scenarios:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b05462e8b3824a50870ebc0ecc157bad",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating Samples:   0%|          | 0/10 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Task exception was never retrieved\n",
            "future: <Task finished name='Task-59' coro=<as_completed.<locals>.sema_coro() done, defined at /Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/ragas/executor.py:46> exception=RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4.1 in organization org-0rHLjwHvNzPhGkB8I6dgKadJ on tokens per min (TPM): Limit 30000, Used 29365, Requested 1197. Please try again in 1.124s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")>\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/homebrew/Cellar/python@3.13/3.13.2/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/tasks.py\", line 304, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/ragas/executor.py\", line 48, in sema_coro\n",
            "    return await coro\n",
            "           ^^^^^^^^^^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/ragas/testset/transforms/base.py\", line 178, in apply_extract\n",
            "    property_name, property_value = await self.extract(node)\n",
            "                                    ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/ragas/testset/transforms/extractors/llm_based.py\", line 180, in extract\n",
            "    result = await self.prompt.generate(self.llm, data=StringIO(text=chunks[0]))\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/ragas/prompt/pydantic_prompt.py\", line 126, in generate\n",
            "    output_single = await self.generate_multiple(\n",
            "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "    ...<7 lines>...\n",
            "    )\n",
            "    ^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/ragas/prompt/pydantic_prompt.py\", line 187, in generate_multiple\n",
            "    resp = await llm.generate(\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "    ...<5 lines>...\n",
            "    )\n",
            "    ^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/ragas/llms/base.py\", line 108, in generate\n",
            "    result = await agenerate_text_with_retry(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "    ...<5 lines>...\n",
            "    )\n",
            "    ^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/tenacity/asyncio/__init__.py\", line 189, in async_wrapped\n",
            "    return await copy(fn, *args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/tenacity/asyncio/__init__.py\", line 111, in __call__\n",
            "    do = await self.iter(retry_state=retry_state)\n",
            "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n",
            "    result = await action(retry_state)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/tenacity/_utils.py\", line 99, in inner\n",
            "    return call(*args, **kwargs)\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/tenacity/__init__.py\", line 420, in exc_check\n",
            "    raise retry_exc.reraise()\n",
            "          ~~~~~~~~~~~~~~~~~^^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/tenacity/__init__.py\", line 187, in reraise\n",
            "    raise self.last_attempt.result()\n",
            "          ~~~~~~~~~~~~~~~~~~~~~~~~^^\n",
            "  File \"/opt/homebrew/Cellar/python@3.13/3.13.2/Frameworks/Python.framework/Versions/3.13/lib/python3.13/concurrent/futures/_base.py\", line 449, in result\n",
            "    return self.__get_result()\n",
            "           ~~~~~~~~~~~~~~~~~^^\n",
            "  File \"/opt/homebrew/Cellar/python@3.13/3.13.2/Frameworks/Python.framework/Versions/3.13/lib/python3.13/concurrent/futures/_base.py\", line 401, in __get_result\n",
            "    raise self._exception\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/tenacity/asyncio/__init__.py\", line 114, in __call__\n",
            "    result = await fn(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/ragas/llms/base.py\", line 251, in agenerate_text\n",
            "    result = await self.langchain_llm.agenerate_prompt(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "    ...<3 lines>...\n",
            "    )\n",
            "    ^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langchain_core/language_models/chat_models.py\", line 991, in agenerate_prompt\n",
            "    return await self.agenerate(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "        prompt_messages, stop=stop, callbacks=callbacks, **kwargs\n",
            "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "    )\n",
            "    ^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langchain_core/language_models/chat_models.py\", line 949, in agenerate\n",
            "    raise exceptions[0]\n",
            "  File \"/opt/homebrew/Cellar/python@3.13/3.13.2/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/tasks.py\", line 304, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langchain_core/language_models/chat_models.py\", line 1117, in _agenerate_with_cache\n",
            "    result = await self._agenerate(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^\n",
            "        messages, stop=stop, run_manager=run_manager, **kwargs\n",
            "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "    )\n",
            "    ^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langchain_openai/chat_models/base.py\", line 1367, in _agenerate\n",
            "    response = await self.async_client.create(**payload)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py\", line 2454, in create\n",
            "    return await self._post(\n",
            "           ^^^^^^^^^^^^^^^^^\n",
            "    ...<45 lines>...\n",
            "    )\n",
            "    ^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/openai/_base_client.py\", line 1791, in post\n",
            "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/openai/_base_client.py\", line 1591, in request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4.1 in organization org-0rHLjwHvNzPhGkB8I6dgKadJ on tokens per min (TPM): Limit 30000, Used 29365, Requested 1197. Please try again in 1.124s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Task exception was never retrieved\n",
            "future: <Task finished name='Task-63' coro=<as_completed.<locals>.sema_coro() done, defined at /Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/ragas/executor.py:46> exception=RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4.1 in organization org-0rHLjwHvNzPhGkB8I6dgKadJ on tokens per min (TPM): Limit 30000, Used 29579, Requested 1242. Please try again in 1.642s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")>\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/homebrew/Cellar/python@3.13/3.13.2/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/tasks.py\", line 304, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/ragas/executor.py\", line 48, in sema_coro\n",
            "    return await coro\n",
            "           ^^^^^^^^^^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/ragas/testset/transforms/base.py\", line 178, in apply_extract\n",
            "    property_name, property_value = await self.extract(node)\n",
            "                                    ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/ragas/testset/transforms/extractors/llm_based.py\", line 180, in extract\n",
            "    result = await self.prompt.generate(self.llm, data=StringIO(text=chunks[0]))\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/ragas/prompt/pydantic_prompt.py\", line 126, in generate\n",
            "    output_single = await self.generate_multiple(\n",
            "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "    ...<7 lines>...\n",
            "    )\n",
            "    ^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/ragas/prompt/pydantic_prompt.py\", line 187, in generate_multiple\n",
            "    resp = await llm.generate(\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "    ...<5 lines>...\n",
            "    )\n",
            "    ^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/ragas/llms/base.py\", line 108, in generate\n",
            "    result = await agenerate_text_with_retry(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "    ...<5 lines>...\n",
            "    )\n",
            "    ^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/tenacity/asyncio/__init__.py\", line 189, in async_wrapped\n",
            "    return await copy(fn, *args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/tenacity/asyncio/__init__.py\", line 111, in __call__\n",
            "    do = await self.iter(retry_state=retry_state)\n",
            "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n",
            "    result = await action(retry_state)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/tenacity/_utils.py\", line 99, in inner\n",
            "    return call(*args, **kwargs)\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/tenacity/__init__.py\", line 420, in exc_check\n",
            "    raise retry_exc.reraise()\n",
            "          ~~~~~~~~~~~~~~~~~^^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/tenacity/__init__.py\", line 187, in reraise\n",
            "    raise self.last_attempt.result()\n",
            "          ~~~~~~~~~~~~~~~~~~~~~~~~^^\n",
            "  File \"/opt/homebrew/Cellar/python@3.13/3.13.2/Frameworks/Python.framework/Versions/3.13/lib/python3.13/concurrent/futures/_base.py\", line 449, in result\n",
            "    return self.__get_result()\n",
            "           ~~~~~~~~~~~~~~~~~^^\n",
            "  File \"/opt/homebrew/Cellar/python@3.13/3.13.2/Frameworks/Python.framework/Versions/3.13/lib/python3.13/concurrent/futures/_base.py\", line 401, in __get_result\n",
            "    raise self._exception\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/tenacity/asyncio/__init__.py\", line 114, in __call__\n",
            "    result = await fn(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/ragas/llms/base.py\", line 251, in agenerate_text\n",
            "    result = await self.langchain_llm.agenerate_prompt(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "    ...<3 lines>...\n",
            "    )\n",
            "    ^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langchain_core/language_models/chat_models.py\", line 991, in agenerate_prompt\n",
            "    return await self.agenerate(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "        prompt_messages, stop=stop, callbacks=callbacks, **kwargs\n",
            "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "    )\n",
            "    ^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langchain_core/language_models/chat_models.py\", line 949, in agenerate\n",
            "    raise exceptions[0]\n",
            "  File \"/opt/homebrew/Cellar/python@3.13/3.13.2/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/tasks.py\", line 304, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langchain_core/language_models/chat_models.py\", line 1117, in _agenerate_with_cache\n",
            "    result = await self._agenerate(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^\n",
            "        messages, stop=stop, run_manager=run_manager, **kwargs\n",
            "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "    )\n",
            "    ^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langchain_openai/chat_models/base.py\", line 1367, in _agenerate\n",
            "    response = await self.async_client.create(**payload)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py\", line 2454, in create\n",
            "    return await self._post(\n",
            "           ^^^^^^^^^^^^^^^^^\n",
            "    ...<45 lines>...\n",
            "    )\n",
            "    ^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/openai/_base_client.py\", line 1791, in post\n",
            "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/openai/_base_client.py\", line 1591, in request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4.1 in organization org-0rHLjwHvNzPhGkB8I6dgKadJ on tokens per min (TPM): Limit 30000, Used 29579, Requested 1242. Please try again in 1.642s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Task exception was never retrieved\n",
            "future: <Task finished name='Task-60' coro=<as_completed.<locals>.sema_coro() done, defined at /Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/ragas/executor.py:46> exception=RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4.1 in organization org-0rHLjwHvNzPhGkB8I6dgKadJ on tokens per min (TPM): Limit 30000, Used 30000, Requested 1029. Please try again in 2.058s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")>\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/homebrew/Cellar/python@3.13/3.13.2/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/tasks.py\", line 304, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/ragas/executor.py\", line 48, in sema_coro\n",
            "    return await coro\n",
            "           ^^^^^^^^^^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/ragas/testset/transforms/base.py\", line 178, in apply_extract\n",
            "    property_name, property_value = await self.extract(node)\n",
            "                                    ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/ragas/testset/transforms/extractors/llm_based.py\", line 180, in extract\n",
            "    result = await self.prompt.generate(self.llm, data=StringIO(text=chunks[0]))\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/ragas/prompt/pydantic_prompt.py\", line 126, in generate\n",
            "    output_single = await self.generate_multiple(\n",
            "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "    ...<7 lines>...\n",
            "    )\n",
            "    ^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/ragas/prompt/pydantic_prompt.py\", line 187, in generate_multiple\n",
            "    resp = await llm.generate(\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "    ...<5 lines>...\n",
            "    )\n",
            "    ^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/ragas/llms/base.py\", line 108, in generate\n",
            "    result = await agenerate_text_with_retry(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "    ...<5 lines>...\n",
            "    )\n",
            "    ^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/tenacity/asyncio/__init__.py\", line 189, in async_wrapped\n",
            "    return await copy(fn, *args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/tenacity/asyncio/__init__.py\", line 111, in __call__\n",
            "    do = await self.iter(retry_state=retry_state)\n",
            "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n",
            "    result = await action(retry_state)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/tenacity/_utils.py\", line 99, in inner\n",
            "    return call(*args, **kwargs)\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/tenacity/__init__.py\", line 420, in exc_check\n",
            "    raise retry_exc.reraise()\n",
            "          ~~~~~~~~~~~~~~~~~^^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/tenacity/__init__.py\", line 187, in reraise\n",
            "    raise self.last_attempt.result()\n",
            "          ~~~~~~~~~~~~~~~~~~~~~~~~^^\n",
            "  File \"/opt/homebrew/Cellar/python@3.13/3.13.2/Frameworks/Python.framework/Versions/3.13/lib/python3.13/concurrent/futures/_base.py\", line 449, in result\n",
            "    return self.__get_result()\n",
            "           ~~~~~~~~~~~~~~~~~^^\n",
            "  File \"/opt/homebrew/Cellar/python@3.13/3.13.2/Frameworks/Python.framework/Versions/3.13/lib/python3.13/concurrent/futures/_base.py\", line 401, in __get_result\n",
            "    raise self._exception\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/tenacity/asyncio/__init__.py\", line 114, in __call__\n",
            "    result = await fn(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/ragas/llms/base.py\", line 251, in agenerate_text\n",
            "    result = await self.langchain_llm.agenerate_prompt(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "    ...<3 lines>...\n",
            "    )\n",
            "    ^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langchain_core/language_models/chat_models.py\", line 991, in agenerate_prompt\n",
            "    return await self.agenerate(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "        prompt_messages, stop=stop, callbacks=callbacks, **kwargs\n",
            "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "    )\n",
            "    ^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langchain_core/language_models/chat_models.py\", line 949, in agenerate\n",
            "    raise exceptions[0]\n",
            "  File \"/opt/homebrew/Cellar/python@3.13/3.13.2/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/tasks.py\", line 304, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langchain_core/language_models/chat_models.py\", line 1117, in _agenerate_with_cache\n",
            "    result = await self._agenerate(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^\n",
            "        messages, stop=stop, run_manager=run_manager, **kwargs\n",
            "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "    )\n",
            "    ^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langchain_openai/chat_models/base.py\", line 1367, in _agenerate\n",
            "    response = await self.async_client.create(**payload)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py\", line 2454, in create\n",
            "    return await self._post(\n",
            "           ^^^^^^^^^^^^^^^^^\n",
            "    ...<45 lines>...\n",
            "    )\n",
            "    ^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/openai/_base_client.py\", line 1791, in post\n",
            "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/openai/_base_client.py\", line 1591, in request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4.1 in organization org-0rHLjwHvNzPhGkB8I6dgKadJ on tokens per min (TPM): Limit 30000, Used 30000, Requested 1029. Please try again in 2.058s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Task exception was never retrieved\n",
            "future: <Task finished name='Task-146' coro=<as_completed.<locals>.sema_coro() done, defined at /Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/ragas/executor.py:46> exception=RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4.1 in organization org-0rHLjwHvNzPhGkB8I6dgKadJ on tokens per min (TPM): Limit 30000, Used 29156, Requested 1367. Please try again in 1.046s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")>\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/homebrew/Cellar/python@3.13/3.13.2/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/tasks.py\", line 304, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/ragas/executor.py\", line 48, in sema_coro\n",
            "    return await coro\n",
            "           ^^^^^^^^^^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/ragas/testset/transforms/base.py\", line 178, in apply_extract\n",
            "    property_name, property_value = await self.extract(node)\n",
            "                                    ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/ragas/testset/transforms/extractors/llm_based.py\", line 180, in extract\n",
            "    result = await self.prompt.generate(self.llm, data=StringIO(text=chunks[0]))\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/ragas/prompt/pydantic_prompt.py\", line 126, in generate\n",
            "    output_single = await self.generate_multiple(\n",
            "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "    ...<7 lines>...\n",
            "    )\n",
            "    ^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/ragas/prompt/pydantic_prompt.py\", line 187, in generate_multiple\n",
            "    resp = await llm.generate(\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "    ...<5 lines>...\n",
            "    )\n",
            "    ^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/ragas/llms/base.py\", line 108, in generate\n",
            "    result = await agenerate_text_with_retry(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "    ...<5 lines>...\n",
            "    )\n",
            "    ^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/tenacity/asyncio/__init__.py\", line 189, in async_wrapped\n",
            "    return await copy(fn, *args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/tenacity/asyncio/__init__.py\", line 111, in __call__\n",
            "    do = await self.iter(retry_state=retry_state)\n",
            "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n",
            "    result = await action(retry_state)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/tenacity/_utils.py\", line 99, in inner\n",
            "    return call(*args, **kwargs)\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/tenacity/__init__.py\", line 420, in exc_check\n",
            "    raise retry_exc.reraise()\n",
            "          ~~~~~~~~~~~~~~~~~^^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/tenacity/__init__.py\", line 187, in reraise\n",
            "    raise self.last_attempt.result()\n",
            "          ~~~~~~~~~~~~~~~~~~~~~~~~^^\n",
            "  File \"/opt/homebrew/Cellar/python@3.13/3.13.2/Frameworks/Python.framework/Versions/3.13/lib/python3.13/concurrent/futures/_base.py\", line 449, in result\n",
            "    return self.__get_result()\n",
            "           ~~~~~~~~~~~~~~~~~^^\n",
            "  File \"/opt/homebrew/Cellar/python@3.13/3.13.2/Frameworks/Python.framework/Versions/3.13/lib/python3.13/concurrent/futures/_base.py\", line 401, in __get_result\n",
            "    raise self._exception\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/tenacity/asyncio/__init__.py\", line 114, in __call__\n",
            "    result = await fn(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/ragas/llms/base.py\", line 251, in agenerate_text\n",
            "    result = await self.langchain_llm.agenerate_prompt(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "    ...<3 lines>...\n",
            "    )\n",
            "    ^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langchain_core/language_models/chat_models.py\", line 991, in agenerate_prompt\n",
            "    return await self.agenerate(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "        prompt_messages, stop=stop, callbacks=callbacks, **kwargs\n",
            "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "    )\n",
            "    ^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langchain_core/language_models/chat_models.py\", line 949, in agenerate\n",
            "    raise exceptions[0]\n",
            "  File \"/opt/homebrew/Cellar/python@3.13/3.13.2/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/tasks.py\", line 304, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langchain_core/language_models/chat_models.py\", line 1117, in _agenerate_with_cache\n",
            "    result = await self._agenerate(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^\n",
            "        messages, stop=stop, run_manager=run_manager, **kwargs\n",
            "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "    )\n",
            "    ^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langchain_openai/chat_models/base.py\", line 1367, in _agenerate\n",
            "    response = await self.async_client.create(**payload)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py\", line 2454, in create\n",
            "    return await self._post(\n",
            "           ^^^^^^^^^^^^^^^^^\n",
            "    ...<45 lines>...\n",
            "    )\n",
            "    ^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/openai/_base_client.py\", line 1791, in post\n",
            "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/openai/_base_client.py\", line 1591, in request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4.1 in organization org-0rHLjwHvNzPhGkB8I6dgKadJ on tokens per min (TPM): Limit 30000, Used 29156, Requested 1367. Please try again in 1.046s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Task exception was never retrieved\n",
            "future: <Task finished name='Task-164' coro=<as_completed.<locals>.sema_coro() done, defined at /Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/ragas/executor.py:46> exception=RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4.1 in organization org-0rHLjwHvNzPhGkB8I6dgKadJ on tokens per min (TPM): Limit 30000, Used 29449, Requested 908. Please try again in 714ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")>\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/homebrew/Cellar/python@3.13/3.13.2/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/tasks.py\", line 304, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/ragas/executor.py\", line 48, in sema_coro\n",
            "    return await coro\n",
            "           ^^^^^^^^^^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/ragas/testset/transforms/base.py\", line 178, in apply_extract\n",
            "    property_name, property_value = await self.extract(node)\n",
            "                                    ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/ragas/testset/transforms/extractors/llm_based.py\", line 180, in extract\n",
            "    result = await self.prompt.generate(self.llm, data=StringIO(text=chunks[0]))\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/ragas/prompt/pydantic_prompt.py\", line 126, in generate\n",
            "    output_single = await self.generate_multiple(\n",
            "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "    ...<7 lines>...\n",
            "    )\n",
            "    ^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/ragas/prompt/pydantic_prompt.py\", line 187, in generate_multiple\n",
            "    resp = await llm.generate(\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "    ...<5 lines>...\n",
            "    )\n",
            "    ^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/ragas/llms/base.py\", line 108, in generate\n",
            "    result = await agenerate_text_with_retry(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "    ...<5 lines>...\n",
            "    )\n",
            "    ^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/tenacity/asyncio/__init__.py\", line 189, in async_wrapped\n",
            "    return await copy(fn, *args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/tenacity/asyncio/__init__.py\", line 111, in __call__\n",
            "    do = await self.iter(retry_state=retry_state)\n",
            "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n",
            "    result = await action(retry_state)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/tenacity/_utils.py\", line 99, in inner\n",
            "    return call(*args, **kwargs)\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/tenacity/__init__.py\", line 420, in exc_check\n",
            "    raise retry_exc.reraise()\n",
            "          ~~~~~~~~~~~~~~~~~^^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/tenacity/__init__.py\", line 187, in reraise\n",
            "    raise self.last_attempt.result()\n",
            "          ~~~~~~~~~~~~~~~~~~~~~~~~^^\n",
            "  File \"/opt/homebrew/Cellar/python@3.13/3.13.2/Frameworks/Python.framework/Versions/3.13/lib/python3.13/concurrent/futures/_base.py\", line 449, in result\n",
            "    return self.__get_result()\n",
            "           ~~~~~~~~~~~~~~~~~^^\n",
            "  File \"/opt/homebrew/Cellar/python@3.13/3.13.2/Frameworks/Python.framework/Versions/3.13/lib/python3.13/concurrent/futures/_base.py\", line 401, in __get_result\n",
            "    raise self._exception\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/tenacity/asyncio/__init__.py\", line 114, in __call__\n",
            "    result = await fn(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/ragas/llms/base.py\", line 251, in agenerate_text\n",
            "    result = await self.langchain_llm.agenerate_prompt(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "    ...<3 lines>...\n",
            "    )\n",
            "    ^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langchain_core/language_models/chat_models.py\", line 991, in agenerate_prompt\n",
            "    return await self.agenerate(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "        prompt_messages, stop=stop, callbacks=callbacks, **kwargs\n",
            "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "    )\n",
            "    ^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langchain_core/language_models/chat_models.py\", line 949, in agenerate\n",
            "    raise exceptions[0]\n",
            "  File \"/opt/homebrew/Cellar/python@3.13/3.13.2/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/tasks.py\", line 304, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langchain_core/language_models/chat_models.py\", line 1117, in _agenerate_with_cache\n",
            "    result = await self._agenerate(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^\n",
            "        messages, stop=stop, run_manager=run_manager, **kwargs\n",
            "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "    )\n",
            "    ^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langchain_openai/chat_models/base.py\", line 1367, in _agenerate\n",
            "    response = await self.async_client.create(**payload)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py\", line 2454, in create\n",
            "    return await self._post(\n",
            "           ^^^^^^^^^^^^^^^^^\n",
            "    ...<45 lines>...\n",
            "    )\n",
            "    ^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/openai/_base_client.py\", line 1791, in post\n",
            "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/openai/_base_client.py\", line 1591, in request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4.1 in organization org-0rHLjwHvNzPhGkB8I6dgKadJ on tokens per min (TPM): Limit 30000, Used 29449, Requested 908. Please try again in 714ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Task exception was never retrieved\n",
            "future: <Task finished name='Task-258' coro=<as_completed.<locals>.sema_coro() done, defined at /Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/ragas/executor.py:46> exception=InternalServerError('<html>\\r\\n<head><title>502 Bad Gateway</title></head>\\r\\n<body>\\r\\n<center><h1>502 Bad Gateway</h1></center>\\r\\n<hr><center>cloudflare</center>\\r\\n</body>\\r\\n</html>')>\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/homebrew/Cellar/python@3.13/3.13.2/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/tasks.py\", line 304, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/ragas/executor.py\", line 48, in sema_coro\n",
            "    return await coro\n",
            "           ^^^^^^^^^^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/ragas/testset/transforms/base.py\", line 178, in apply_extract\n",
            "    property_name, property_value = await self.extract(node)\n",
            "                                    ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/ragas/testset/transforms/extractors/llm_based.py\", line 180, in extract\n",
            "    result = await self.prompt.generate(self.llm, data=StringIO(text=chunks[0]))\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/ragas/prompt/pydantic_prompt.py\", line 126, in generate\n",
            "    output_single = await self.generate_multiple(\n",
            "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "    ...<7 lines>...\n",
            "    )\n",
            "    ^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/ragas/prompt/pydantic_prompt.py\", line 187, in generate_multiple\n",
            "    resp = await llm.generate(\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "    ...<5 lines>...\n",
            "    )\n",
            "    ^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/ragas/llms/base.py\", line 108, in generate\n",
            "    result = await agenerate_text_with_retry(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "    ...<5 lines>...\n",
            "    )\n",
            "    ^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/tenacity/asyncio/__init__.py\", line 189, in async_wrapped\n",
            "    return await copy(fn, *args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/tenacity/asyncio/__init__.py\", line 111, in __call__\n",
            "    do = await self.iter(retry_state=retry_state)\n",
            "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n",
            "    result = await action(retry_state)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/tenacity/_utils.py\", line 99, in inner\n",
            "    return call(*args, **kwargs)\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/tenacity/__init__.py\", line 400, in <lambda>\n",
            "    self._add_action_func(lambda rs: rs.outcome.result())\n",
            "                                     ~~~~~~~~~~~~~~~~~^^\n",
            "  File \"/opt/homebrew/Cellar/python@3.13/3.13.2/Frameworks/Python.framework/Versions/3.13/lib/python3.13/concurrent/futures/_base.py\", line 449, in result\n",
            "    return self.__get_result()\n",
            "           ~~~~~~~~~~~~~~~~~^^\n",
            "  File \"/opt/homebrew/Cellar/python@3.13/3.13.2/Frameworks/Python.framework/Versions/3.13/lib/python3.13/concurrent/futures/_base.py\", line 401, in __get_result\n",
            "    raise self._exception\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/tenacity/asyncio/__init__.py\", line 114, in __call__\n",
            "    result = await fn(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/ragas/llms/base.py\", line 251, in agenerate_text\n",
            "    result = await self.langchain_llm.agenerate_prompt(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "    ...<3 lines>...\n",
            "    )\n",
            "    ^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langchain_core/language_models/chat_models.py\", line 991, in agenerate_prompt\n",
            "    return await self.agenerate(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "        prompt_messages, stop=stop, callbacks=callbacks, **kwargs\n",
            "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "    )\n",
            "    ^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langchain_core/language_models/chat_models.py\", line 949, in agenerate\n",
            "    raise exceptions[0]\n",
            "  File \"/opt/homebrew/Cellar/python@3.13/3.13.2/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/tasks.py\", line 304, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langchain_core/language_models/chat_models.py\", line 1117, in _agenerate_with_cache\n",
            "    result = await self._agenerate(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^\n",
            "        messages, stop=stop, run_manager=run_manager, **kwargs\n",
            "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "    )\n",
            "    ^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langchain_openai/chat_models/base.py\", line 1367, in _agenerate\n",
            "    response = await self.async_client.create(**payload)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py\", line 2454, in create\n",
            "    return await self._post(\n",
            "           ^^^^^^^^^^^^^^^^^\n",
            "    ...<45 lines>...\n",
            "    )\n",
            "    ^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/openai/_base_client.py\", line 1791, in post\n",
            "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/openai/_base_client.py\", line 1591, in request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.InternalServerError: <html>\n",
            "<head><title>502 Bad Gateway</title></head>\n",
            "<body>\n",
            "<center><h1>502 Bad Gateway</h1></center>\n",
            "<hr><center>cloudflare</center>\n",
            "</body>\n",
            "</html>\n",
            "Task exception was never retrieved\n",
            "future: <Task finished name='Task-110' coro=<as_completed.<locals>.sema_coro() done, defined at /Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/ragas/executor.py:46> exception=RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4.1 in organization org-0rHLjwHvNzPhGkB8I6dgKadJ on tokens per min (TPM): Limit 30000, Used 29976, Requested 1244. Please try again in 2.44s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")>\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/homebrew/Cellar/python@3.13/3.13.2/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/tasks.py\", line 304, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/ragas/executor.py\", line 48, in sema_coro\n",
            "    return await coro\n",
            "           ^^^^^^^^^^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/ragas/testset/transforms/base.py\", line 178, in apply_extract\n",
            "    property_name, property_value = await self.extract(node)\n",
            "                                    ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/ragas/testset/transforms/extractors/llm_based.py\", line 180, in extract\n",
            "    result = await self.prompt.generate(self.llm, data=StringIO(text=chunks[0]))\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/ragas/prompt/pydantic_prompt.py\", line 126, in generate\n",
            "    output_single = await self.generate_multiple(\n",
            "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "    ...<7 lines>...\n",
            "    )\n",
            "    ^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/ragas/prompt/pydantic_prompt.py\", line 187, in generate_multiple\n",
            "    resp = await llm.generate(\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "    ...<5 lines>...\n",
            "    )\n",
            "    ^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/ragas/llms/base.py\", line 108, in generate\n",
            "    result = await agenerate_text_with_retry(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "    ...<5 lines>...\n",
            "    )\n",
            "    ^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/tenacity/asyncio/__init__.py\", line 189, in async_wrapped\n",
            "    return await copy(fn, *args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/tenacity/asyncio/__init__.py\", line 111, in __call__\n",
            "    do = await self.iter(retry_state=retry_state)\n",
            "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n",
            "    result = await action(retry_state)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/tenacity/_utils.py\", line 99, in inner\n",
            "    return call(*args, **kwargs)\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/tenacity/__init__.py\", line 420, in exc_check\n",
            "    raise retry_exc.reraise()\n",
            "          ~~~~~~~~~~~~~~~~~^^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/tenacity/__init__.py\", line 187, in reraise\n",
            "    raise self.last_attempt.result()\n",
            "          ~~~~~~~~~~~~~~~~~~~~~~~~^^\n",
            "  File \"/opt/homebrew/Cellar/python@3.13/3.13.2/Frameworks/Python.framework/Versions/3.13/lib/python3.13/concurrent/futures/_base.py\", line 449, in result\n",
            "    return self.__get_result()\n",
            "           ~~~~~~~~~~~~~~~~~^^\n",
            "  File \"/opt/homebrew/Cellar/python@3.13/3.13.2/Frameworks/Python.framework/Versions/3.13/lib/python3.13/concurrent/futures/_base.py\", line 401, in __get_result\n",
            "    raise self._exception\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/tenacity/asyncio/__init__.py\", line 114, in __call__\n",
            "    result = await fn(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/ragas/llms/base.py\", line 251, in agenerate_text\n",
            "    result = await self.langchain_llm.agenerate_prompt(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "    ...<3 lines>...\n",
            "    )\n",
            "    ^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langchain_core/language_models/chat_models.py\", line 991, in agenerate_prompt\n",
            "    return await self.agenerate(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "        prompt_messages, stop=stop, callbacks=callbacks, **kwargs\n",
            "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "    )\n",
            "    ^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langchain_core/language_models/chat_models.py\", line 949, in agenerate\n",
            "    raise exceptions[0]\n",
            "  File \"/opt/homebrew/Cellar/python@3.13/3.13.2/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/tasks.py\", line 304, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langchain_core/language_models/chat_models.py\", line 1117, in _agenerate_with_cache\n",
            "    result = await self._agenerate(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^\n",
            "        messages, stop=stop, run_manager=run_manager, **kwargs\n",
            "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "    )\n",
            "    ^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langchain_openai/chat_models/base.py\", line 1367, in _agenerate\n",
            "    response = await self.async_client.create(**payload)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py\", line 2454, in create\n",
            "    return await self._post(\n",
            "           ^^^^^^^^^^^^^^^^^\n",
            "    ...<45 lines>...\n",
            "    )\n",
            "    ^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/openai/_base_client.py\", line 1791, in post\n",
            "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/openai/_base_client.py\", line 1591, in request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4.1 in organization org-0rHLjwHvNzPhGkB8I6dgKadJ on tokens per min (TPM): Limit 30000, Used 29976, Requested 1244. Please try again in 2.44s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Task exception was never retrieved\n",
            "future: <Task finished name='Task-170' coro=<as_completed.<locals>.sema_coro() done, defined at /Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/ragas/executor.py:46> exception=RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4.1 in organization org-0rHLjwHvNzPhGkB8I6dgKadJ on tokens per min (TPM): Limit 30000, Used 29665, Requested 1234. Please try again in 1.798s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")>\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/homebrew/Cellar/python@3.13/3.13.2/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/tasks.py\", line 304, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/ragas/executor.py\", line 48, in sema_coro\n",
            "    return await coro\n",
            "           ^^^^^^^^^^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/ragas/testset/transforms/base.py\", line 178, in apply_extract\n",
            "    property_name, property_value = await self.extract(node)\n",
            "                                    ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/ragas/testset/transforms/extractors/llm_based.py\", line 180, in extract\n",
            "    result = await self.prompt.generate(self.llm, data=StringIO(text=chunks[0]))\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/ragas/prompt/pydantic_prompt.py\", line 126, in generate\n",
            "    output_single = await self.generate_multiple(\n",
            "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "    ...<7 lines>...\n",
            "    )\n",
            "    ^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/ragas/prompt/pydantic_prompt.py\", line 187, in generate_multiple\n",
            "    resp = await llm.generate(\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "    ...<5 lines>...\n",
            "    )\n",
            "    ^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/ragas/llms/base.py\", line 108, in generate\n",
            "    result = await agenerate_text_with_retry(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "    ...<5 lines>...\n",
            "    )\n",
            "    ^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/tenacity/asyncio/__init__.py\", line 189, in async_wrapped\n",
            "    return await copy(fn, *args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/tenacity/asyncio/__init__.py\", line 111, in __call__\n",
            "    do = await self.iter(retry_state=retry_state)\n",
            "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n",
            "    result = await action(retry_state)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/tenacity/_utils.py\", line 99, in inner\n",
            "    return call(*args, **kwargs)\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/tenacity/__init__.py\", line 420, in exc_check\n",
            "    raise retry_exc.reraise()\n",
            "          ~~~~~~~~~~~~~~~~~^^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/tenacity/__init__.py\", line 187, in reraise\n",
            "    raise self.last_attempt.result()\n",
            "          ~~~~~~~~~~~~~~~~~~~~~~~~^^\n",
            "  File \"/opt/homebrew/Cellar/python@3.13/3.13.2/Frameworks/Python.framework/Versions/3.13/lib/python3.13/concurrent/futures/_base.py\", line 449, in result\n",
            "    return self.__get_result()\n",
            "           ~~~~~~~~~~~~~~~~~^^\n",
            "  File \"/opt/homebrew/Cellar/python@3.13/3.13.2/Frameworks/Python.framework/Versions/3.13/lib/python3.13/concurrent/futures/_base.py\", line 401, in __get_result\n",
            "    raise self._exception\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/tenacity/asyncio/__init__.py\", line 114, in __call__\n",
            "    result = await fn(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/ragas/llms/base.py\", line 251, in agenerate_text\n",
            "    result = await self.langchain_llm.agenerate_prompt(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "    ...<3 lines>...\n",
            "    )\n",
            "    ^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langchain_core/language_models/chat_models.py\", line 991, in agenerate_prompt\n",
            "    return await self.agenerate(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "        prompt_messages, stop=stop, callbacks=callbacks, **kwargs\n",
            "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "    )\n",
            "    ^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langchain_core/language_models/chat_models.py\", line 949, in agenerate\n",
            "    raise exceptions[0]\n",
            "  File \"/opt/homebrew/Cellar/python@3.13/3.13.2/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/tasks.py\", line 304, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langchain_core/language_models/chat_models.py\", line 1117, in _agenerate_with_cache\n",
            "    result = await self._agenerate(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^\n",
            "        messages, stop=stop, run_manager=run_manager, **kwargs\n",
            "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "    )\n",
            "    ^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langchain_openai/chat_models/base.py\", line 1367, in _agenerate\n",
            "    response = await self.async_client.create(**payload)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py\", line 2454, in create\n",
            "    return await self._post(\n",
            "           ^^^^^^^^^^^^^^^^^\n",
            "    ...<45 lines>...\n",
            "    )\n",
            "    ^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/openai/_base_client.py\", line 1791, in post\n",
            "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/openai/_base_client.py\", line 1591, in request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4.1 in organization org-0rHLjwHvNzPhGkB8I6dgKadJ on tokens per min (TPM): Limit 30000, Used 29665, Requested 1234. Please try again in 1.798s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Task exception was never retrieved\n",
            "future: <Task finished name='Task-124' coro=<as_completed.<locals>.sema_coro() done, defined at /Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/ragas/executor.py:46> exception=RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4.1 in organization org-0rHLjwHvNzPhGkB8I6dgKadJ on tokens per min (TPM): Limit 30000, Used 30000, Requested 1034. Please try again in 2.068s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")>\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/homebrew/Cellar/python@3.13/3.13.2/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/tasks.py\", line 304, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/ragas/executor.py\", line 48, in sema_coro\n",
            "    return await coro\n",
            "           ^^^^^^^^^^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/ragas/testset/transforms/base.py\", line 178, in apply_extract\n",
            "    property_name, property_value = await self.extract(node)\n",
            "                                    ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/ragas/testset/transforms/extractors/llm_based.py\", line 180, in extract\n",
            "    result = await self.prompt.generate(self.llm, data=StringIO(text=chunks[0]))\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/ragas/prompt/pydantic_prompt.py\", line 126, in generate\n",
            "    output_single = await self.generate_multiple(\n",
            "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "    ...<7 lines>...\n",
            "    )\n",
            "    ^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/ragas/prompt/pydantic_prompt.py\", line 187, in generate_multiple\n",
            "    resp = await llm.generate(\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "    ...<5 lines>...\n",
            "    )\n",
            "    ^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/ragas/llms/base.py\", line 108, in generate\n",
            "    result = await agenerate_text_with_retry(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "    ...<5 lines>...\n",
            "    )\n",
            "    ^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/tenacity/asyncio/__init__.py\", line 189, in async_wrapped\n",
            "    return await copy(fn, *args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/tenacity/asyncio/__init__.py\", line 111, in __call__\n",
            "    do = await self.iter(retry_state=retry_state)\n",
            "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n",
            "    result = await action(retry_state)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/tenacity/_utils.py\", line 99, in inner\n",
            "    return call(*args, **kwargs)\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/tenacity/__init__.py\", line 420, in exc_check\n",
            "    raise retry_exc.reraise()\n",
            "          ~~~~~~~~~~~~~~~~~^^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/tenacity/__init__.py\", line 187, in reraise\n",
            "    raise self.last_attempt.result()\n",
            "          ~~~~~~~~~~~~~~~~~~~~~~~~^^\n",
            "  File \"/opt/homebrew/Cellar/python@3.13/3.13.2/Frameworks/Python.framework/Versions/3.13/lib/python3.13/concurrent/futures/_base.py\", line 449, in result\n",
            "    return self.__get_result()\n",
            "           ~~~~~~~~~~~~~~~~~^^\n",
            "  File \"/opt/homebrew/Cellar/python@3.13/3.13.2/Frameworks/Python.framework/Versions/3.13/lib/python3.13/concurrent/futures/_base.py\", line 401, in __get_result\n",
            "    raise self._exception\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/tenacity/asyncio/__init__.py\", line 114, in __call__\n",
            "    result = await fn(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/ragas/llms/base.py\", line 251, in agenerate_text\n",
            "    result = await self.langchain_llm.agenerate_prompt(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "    ...<3 lines>...\n",
            "    )\n",
            "    ^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langchain_core/language_models/chat_models.py\", line 991, in agenerate_prompt\n",
            "    return await self.agenerate(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "        prompt_messages, stop=stop, callbacks=callbacks, **kwargs\n",
            "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "    )\n",
            "    ^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langchain_core/language_models/chat_models.py\", line 949, in agenerate\n",
            "    raise exceptions[0]\n",
            "  File \"/opt/homebrew/Cellar/python@3.13/3.13.2/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/tasks.py\", line 304, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langchain_core/language_models/chat_models.py\", line 1117, in _agenerate_with_cache\n",
            "    result = await self._agenerate(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^\n",
            "        messages, stop=stop, run_manager=run_manager, **kwargs\n",
            "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "    )\n",
            "    ^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langchain_openai/chat_models/base.py\", line 1367, in _agenerate\n",
            "    response = await self.async_client.create(**payload)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py\", line 2454, in create\n",
            "    return await self._post(\n",
            "           ^^^^^^^^^^^^^^^^^\n",
            "    ...<45 lines>...\n",
            "    )\n",
            "    ^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/openai/_base_client.py\", line 1791, in post\n",
            "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/openai/_base_client.py\", line 1591, in request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4.1 in organization org-0rHLjwHvNzPhGkB8I6dgKadJ on tokens per min (TPM): Limit 30000, Used 30000, Requested 1034. Please try again in 2.068s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Task exception was never retrieved\n",
            "future: <Task finished name='Task-130' coro=<as_completed.<locals>.sema_coro() done, defined at /Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/ragas/executor.py:46> exception=RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4.1 in organization org-0rHLjwHvNzPhGkB8I6dgKadJ on tokens per min (TPM): Limit 30000, Used 29463, Requested 1089. Please try again in 1.104s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")>\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/homebrew/Cellar/python@3.13/3.13.2/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/tasks.py\", line 304, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/ragas/executor.py\", line 48, in sema_coro\n",
            "    return await coro\n",
            "           ^^^^^^^^^^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/ragas/testset/transforms/base.py\", line 178, in apply_extract\n",
            "    property_name, property_value = await self.extract(node)\n",
            "                                    ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/ragas/testset/transforms/extractors/llm_based.py\", line 180, in extract\n",
            "    result = await self.prompt.generate(self.llm, data=StringIO(text=chunks[0]))\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/ragas/prompt/pydantic_prompt.py\", line 126, in generate\n",
            "    output_single = await self.generate_multiple(\n",
            "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "    ...<7 lines>...\n",
            "    )\n",
            "    ^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/ragas/prompt/pydantic_prompt.py\", line 187, in generate_multiple\n",
            "    resp = await llm.generate(\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "    ...<5 lines>...\n",
            "    )\n",
            "    ^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/ragas/llms/base.py\", line 108, in generate\n",
            "    result = await agenerate_text_with_retry(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "    ...<5 lines>...\n",
            "    )\n",
            "    ^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/tenacity/asyncio/__init__.py\", line 189, in async_wrapped\n",
            "    return await copy(fn, *args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/tenacity/asyncio/__init__.py\", line 111, in __call__\n",
            "    do = await self.iter(retry_state=retry_state)\n",
            "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n",
            "    result = await action(retry_state)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/tenacity/_utils.py\", line 99, in inner\n",
            "    return call(*args, **kwargs)\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/tenacity/__init__.py\", line 420, in exc_check\n",
            "    raise retry_exc.reraise()\n",
            "          ~~~~~~~~~~~~~~~~~^^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/tenacity/__init__.py\", line 187, in reraise\n",
            "    raise self.last_attempt.result()\n",
            "          ~~~~~~~~~~~~~~~~~~~~~~~~^^\n",
            "  File \"/opt/homebrew/Cellar/python@3.13/3.13.2/Frameworks/Python.framework/Versions/3.13/lib/python3.13/concurrent/futures/_base.py\", line 449, in result\n",
            "    return self.__get_result()\n",
            "           ~~~~~~~~~~~~~~~~~^^\n",
            "  File \"/opt/homebrew/Cellar/python@3.13/3.13.2/Frameworks/Python.framework/Versions/3.13/lib/python3.13/concurrent/futures/_base.py\", line 401, in __get_result\n",
            "    raise self._exception\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/tenacity/asyncio/__init__.py\", line 114, in __call__\n",
            "    result = await fn(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/ragas/llms/base.py\", line 251, in agenerate_text\n",
            "    result = await self.langchain_llm.agenerate_prompt(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "    ...<3 lines>...\n",
            "    )\n",
            "    ^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langchain_core/language_models/chat_models.py\", line 991, in agenerate_prompt\n",
            "    return await self.agenerate(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "        prompt_messages, stop=stop, callbacks=callbacks, **kwargs\n",
            "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "    )\n",
            "    ^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langchain_core/language_models/chat_models.py\", line 949, in agenerate\n",
            "    raise exceptions[0]\n",
            "  File \"/opt/homebrew/Cellar/python@3.13/3.13.2/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/tasks.py\", line 304, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langchain_core/language_models/chat_models.py\", line 1117, in _agenerate_with_cache\n",
            "    result = await self._agenerate(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^\n",
            "        messages, stop=stop, run_manager=run_manager, **kwargs\n",
            "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "    )\n",
            "    ^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langchain_openai/chat_models/base.py\", line 1367, in _agenerate\n",
            "    response = await self.async_client.create(**payload)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py\", line 2454, in create\n",
            "    return await self._post(\n",
            "           ^^^^^^^^^^^^^^^^^\n",
            "    ...<45 lines>...\n",
            "    )\n",
            "    ^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/openai/_base_client.py\", line 1791, in post\n",
            "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/openai/_base_client.py\", line 1591, in request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4.1 in organization org-0rHLjwHvNzPhGkB8I6dgKadJ on tokens per min (TPM): Limit 30000, Used 29463, Requested 1089. Please try again in 1.104s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Task exception was never retrieved\n",
            "future: <Task finished name='Task-128' coro=<as_completed.<locals>.sema_coro() done, defined at /Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/ragas/executor.py:46> exception=RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4.1 in organization org-0rHLjwHvNzPhGkB8I6dgKadJ on tokens per min (TPM): Limit 30000, Used 30000, Requested 1347. Please try again in 2.694s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")>\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/homebrew/Cellar/python@3.13/3.13.2/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/tasks.py\", line 304, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/ragas/executor.py\", line 48, in sema_coro\n",
            "    return await coro\n",
            "           ^^^^^^^^^^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/ragas/testset/transforms/base.py\", line 178, in apply_extract\n",
            "    property_name, property_value = await self.extract(node)\n",
            "                                    ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/ragas/testset/transforms/extractors/llm_based.py\", line 180, in extract\n",
            "    result = await self.prompt.generate(self.llm, data=StringIO(text=chunks[0]))\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/ragas/prompt/pydantic_prompt.py\", line 126, in generate\n",
            "    output_single = await self.generate_multiple(\n",
            "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "    ...<7 lines>...\n",
            "    )\n",
            "    ^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/ragas/prompt/pydantic_prompt.py\", line 187, in generate_multiple\n",
            "    resp = await llm.generate(\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "    ...<5 lines>...\n",
            "    )\n",
            "    ^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/ragas/llms/base.py\", line 108, in generate\n",
            "    result = await agenerate_text_with_retry(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "    ...<5 lines>...\n",
            "    )\n",
            "    ^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/tenacity/asyncio/__init__.py\", line 189, in async_wrapped\n",
            "    return await copy(fn, *args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/tenacity/asyncio/__init__.py\", line 111, in __call__\n",
            "    do = await self.iter(retry_state=retry_state)\n",
            "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n",
            "    result = await action(retry_state)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/tenacity/_utils.py\", line 99, in inner\n",
            "    return call(*args, **kwargs)\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/tenacity/__init__.py\", line 420, in exc_check\n",
            "    raise retry_exc.reraise()\n",
            "          ~~~~~~~~~~~~~~~~~^^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/tenacity/__init__.py\", line 187, in reraise\n",
            "    raise self.last_attempt.result()\n",
            "          ~~~~~~~~~~~~~~~~~~~~~~~~^^\n",
            "  File \"/opt/homebrew/Cellar/python@3.13/3.13.2/Frameworks/Python.framework/Versions/3.13/lib/python3.13/concurrent/futures/_base.py\", line 449, in result\n",
            "    return self.__get_result()\n",
            "           ~~~~~~~~~~~~~~~~~^^\n",
            "  File \"/opt/homebrew/Cellar/python@3.13/3.13.2/Frameworks/Python.framework/Versions/3.13/lib/python3.13/concurrent/futures/_base.py\", line 401, in __get_result\n",
            "    raise self._exception\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/tenacity/asyncio/__init__.py\", line 114, in __call__\n",
            "    result = await fn(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/ragas/llms/base.py\", line 251, in agenerate_text\n",
            "    result = await self.langchain_llm.agenerate_prompt(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "    ...<3 lines>...\n",
            "    )\n",
            "    ^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langchain_core/language_models/chat_models.py\", line 991, in agenerate_prompt\n",
            "    return await self.agenerate(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "        prompt_messages, stop=stop, callbacks=callbacks, **kwargs\n",
            "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "    )\n",
            "    ^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langchain_core/language_models/chat_models.py\", line 949, in agenerate\n",
            "    raise exceptions[0]\n",
            "  File \"/opt/homebrew/Cellar/python@3.13/3.13.2/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/tasks.py\", line 304, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langchain_core/language_models/chat_models.py\", line 1117, in _agenerate_with_cache\n",
            "    result = await self._agenerate(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^\n",
            "        messages, stop=stop, run_manager=run_manager, **kwargs\n",
            "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "    )\n",
            "    ^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langchain_openai/chat_models/base.py\", line 1367, in _agenerate\n",
            "    response = await self.async_client.create(**payload)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py\", line 2454, in create\n",
            "    return await self._post(\n",
            "           ^^^^^^^^^^^^^^^^^\n",
            "    ...<45 lines>...\n",
            "    )\n",
            "    ^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/openai/_base_client.py\", line 1791, in post\n",
            "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/openai/_base_client.py\", line 1591, in request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4.1 in organization org-0rHLjwHvNzPhGkB8I6dgKadJ on tokens per min (TPM): Limit 30000, Used 30000, Requested 1347. Please try again in 2.694s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Task exception was never retrieved\n",
            "future: <Task finished name='Task-183' coro=<as_completed.<locals>.sema_coro() done, defined at /Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/ragas/executor.py:46> exception=RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4.1 in organization org-0rHLjwHvNzPhGkB8I6dgKadJ on tokens per min (TPM): Limit 30000, Used 29569, Requested 1322. Please try again in 1.782s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")>\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/homebrew/Cellar/python@3.13/3.13.2/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/tasks.py\", line 304, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/ragas/executor.py\", line 48, in sema_coro\n",
            "    return await coro\n",
            "           ^^^^^^^^^^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/ragas/testset/transforms/base.py\", line 178, in apply_extract\n",
            "    property_name, property_value = await self.extract(node)\n",
            "                                    ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/ragas/testset/transforms/extractors/llm_based.py\", line 180, in extract\n",
            "    result = await self.prompt.generate(self.llm, data=StringIO(text=chunks[0]))\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/ragas/prompt/pydantic_prompt.py\", line 126, in generate\n",
            "    output_single = await self.generate_multiple(\n",
            "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "    ...<7 lines>...\n",
            "    )\n",
            "    ^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/ragas/prompt/pydantic_prompt.py\", line 187, in generate_multiple\n",
            "    resp = await llm.generate(\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "    ...<5 lines>...\n",
            "    )\n",
            "    ^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/ragas/llms/base.py\", line 108, in generate\n",
            "    result = await agenerate_text_with_retry(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "    ...<5 lines>...\n",
            "    )\n",
            "    ^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/tenacity/asyncio/__init__.py\", line 189, in async_wrapped\n",
            "    return await copy(fn, *args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/tenacity/asyncio/__init__.py\", line 111, in __call__\n",
            "    do = await self.iter(retry_state=retry_state)\n",
            "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n",
            "    result = await action(retry_state)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/tenacity/_utils.py\", line 99, in inner\n",
            "    return call(*args, **kwargs)\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/tenacity/__init__.py\", line 420, in exc_check\n",
            "    raise retry_exc.reraise()\n",
            "          ~~~~~~~~~~~~~~~~~^^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/tenacity/__init__.py\", line 187, in reraise\n",
            "    raise self.last_attempt.result()\n",
            "          ~~~~~~~~~~~~~~~~~~~~~~~~^^\n",
            "  File \"/opt/homebrew/Cellar/python@3.13/3.13.2/Frameworks/Python.framework/Versions/3.13/lib/python3.13/concurrent/futures/_base.py\", line 449, in result\n",
            "    return self.__get_result()\n",
            "           ~~~~~~~~~~~~~~~~~^^\n",
            "  File \"/opt/homebrew/Cellar/python@3.13/3.13.2/Frameworks/Python.framework/Versions/3.13/lib/python3.13/concurrent/futures/_base.py\", line 401, in __get_result\n",
            "    raise self._exception\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/tenacity/asyncio/__init__.py\", line 114, in __call__\n",
            "    result = await fn(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/ragas/llms/base.py\", line 251, in agenerate_text\n",
            "    result = await self.langchain_llm.agenerate_prompt(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "    ...<3 lines>...\n",
            "    )\n",
            "    ^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langchain_core/language_models/chat_models.py\", line 991, in agenerate_prompt\n",
            "    return await self.agenerate(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "        prompt_messages, stop=stop, callbacks=callbacks, **kwargs\n",
            "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "    )\n",
            "    ^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langchain_core/language_models/chat_models.py\", line 949, in agenerate\n",
            "    raise exceptions[0]\n",
            "  File \"/opt/homebrew/Cellar/python@3.13/3.13.2/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/tasks.py\", line 304, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langchain_core/language_models/chat_models.py\", line 1117, in _agenerate_with_cache\n",
            "    result = await self._agenerate(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^\n",
            "        messages, stop=stop, run_manager=run_manager, **kwargs\n",
            "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "    )\n",
            "    ^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langchain_openai/chat_models/base.py\", line 1367, in _agenerate\n",
            "    response = await self.async_client.create(**payload)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py\", line 2454, in create\n",
            "    return await self._post(\n",
            "           ^^^^^^^^^^^^^^^^^\n",
            "    ...<45 lines>...\n",
            "    )\n",
            "    ^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/openai/_base_client.py\", line 1791, in post\n",
            "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/openai/_base_client.py\", line 1591, in request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4.1 in organization org-0rHLjwHvNzPhGkB8I6dgKadJ on tokens per min (TPM): Limit 30000, Used 29569, Requested 1322. Please try again in 1.782s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Task exception was never retrieved\n",
            "future: <Task finished name='Task-233' coro=<as_completed.<locals>.sema_coro() done, defined at /Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/ragas/executor.py:46> exception=RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4.1 in organization org-0rHLjwHvNzPhGkB8I6dgKadJ on tokens per min (TPM): Limit 30000, Used 29065, Requested 2042. Please try again in 2.214s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")>\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/homebrew/Cellar/python@3.13/3.13.2/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/tasks.py\", line 304, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/ragas/executor.py\", line 48, in sema_coro\n",
            "    return await coro\n",
            "           ^^^^^^^^^^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/ragas/testset/transforms/base.py\", line 178, in apply_extract\n",
            "    property_name, property_value = await self.extract(node)\n",
            "                                    ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/ragas/testset/transforms/extractors/llm_based.py\", line 180, in extract\n",
            "    result = await self.prompt.generate(self.llm, data=StringIO(text=chunks[0]))\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/ragas/prompt/pydantic_prompt.py\", line 126, in generate\n",
            "    output_single = await self.generate_multiple(\n",
            "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "    ...<7 lines>...\n",
            "    )\n",
            "    ^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/ragas/prompt/pydantic_prompt.py\", line 187, in generate_multiple\n",
            "    resp = await llm.generate(\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "    ...<5 lines>...\n",
            "    )\n",
            "    ^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/ragas/llms/base.py\", line 108, in generate\n",
            "    result = await agenerate_text_with_retry(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "    ...<5 lines>...\n",
            "    )\n",
            "    ^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/tenacity/asyncio/__init__.py\", line 189, in async_wrapped\n",
            "    return await copy(fn, *args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/tenacity/asyncio/__init__.py\", line 111, in __call__\n",
            "    do = await self.iter(retry_state=retry_state)\n",
            "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n",
            "    result = await action(retry_state)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/tenacity/_utils.py\", line 99, in inner\n",
            "    return call(*args, **kwargs)\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/tenacity/__init__.py\", line 420, in exc_check\n",
            "    raise retry_exc.reraise()\n",
            "          ~~~~~~~~~~~~~~~~~^^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/tenacity/__init__.py\", line 187, in reraise\n",
            "    raise self.last_attempt.result()\n",
            "          ~~~~~~~~~~~~~~~~~~~~~~~~^^\n",
            "  File \"/opt/homebrew/Cellar/python@3.13/3.13.2/Frameworks/Python.framework/Versions/3.13/lib/python3.13/concurrent/futures/_base.py\", line 449, in result\n",
            "    return self.__get_result()\n",
            "           ~~~~~~~~~~~~~~~~~^^\n",
            "  File \"/opt/homebrew/Cellar/python@3.13/3.13.2/Frameworks/Python.framework/Versions/3.13/lib/python3.13/concurrent/futures/_base.py\", line 401, in __get_result\n",
            "    raise self._exception\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/tenacity/asyncio/__init__.py\", line 114, in __call__\n",
            "    result = await fn(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/ragas/llms/base.py\", line 251, in agenerate_text\n",
            "    result = await self.langchain_llm.agenerate_prompt(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "    ...<3 lines>...\n",
            "    )\n",
            "    ^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langchain_core/language_models/chat_models.py\", line 991, in agenerate_prompt\n",
            "    return await self.agenerate(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "        prompt_messages, stop=stop, callbacks=callbacks, **kwargs\n",
            "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "    )\n",
            "    ^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langchain_core/language_models/chat_models.py\", line 949, in agenerate\n",
            "    raise exceptions[0]\n",
            "  File \"/opt/homebrew/Cellar/python@3.13/3.13.2/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/tasks.py\", line 304, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langchain_core/language_models/chat_models.py\", line 1117, in _agenerate_with_cache\n",
            "    result = await self._agenerate(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^\n",
            "        messages, stop=stop, run_manager=run_manager, **kwargs\n",
            "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "    )\n",
            "    ^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langchain_openai/chat_models/base.py\", line 1367, in _agenerate\n",
            "    response = await self.async_client.create(**payload)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py\", line 2454, in create\n",
            "    return await self._post(\n",
            "           ^^^^^^^^^^^^^^^^^\n",
            "    ...<45 lines>...\n",
            "    )\n",
            "    ^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/openai/_base_client.py\", line 1791, in post\n",
            "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/openai/_base_client.py\", line 1591, in request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4.1 in organization org-0rHLjwHvNzPhGkB8I6dgKadJ on tokens per min (TPM): Limit 30000, Used 29065, Requested 2042. Please try again in 2.214s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Task exception was never retrieved\n",
            "future: <Task finished name='Task-178' coro=<as_completed.<locals>.sema_coro() done, defined at /Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/ragas/executor.py:46> exception=RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4.1 in organization org-0rHLjwHvNzPhGkB8I6dgKadJ on tokens per min (TPM): Limit 30000, Used 30000, Requested 1230. Please try again in 2.46s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")>\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/homebrew/Cellar/python@3.13/3.13.2/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/tasks.py\", line 304, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/ragas/executor.py\", line 48, in sema_coro\n",
            "    return await coro\n",
            "           ^^^^^^^^^^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/ragas/testset/transforms/base.py\", line 178, in apply_extract\n",
            "    property_name, property_value = await self.extract(node)\n",
            "                                    ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/ragas/testset/transforms/extractors/llm_based.py\", line 180, in extract\n",
            "    result = await self.prompt.generate(self.llm, data=StringIO(text=chunks[0]))\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/ragas/prompt/pydantic_prompt.py\", line 126, in generate\n",
            "    output_single = await self.generate_multiple(\n",
            "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "    ...<7 lines>...\n",
            "    )\n",
            "    ^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/ragas/prompt/pydantic_prompt.py\", line 187, in generate_multiple\n",
            "    resp = await llm.generate(\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "    ...<5 lines>...\n",
            "    )\n",
            "    ^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/ragas/llms/base.py\", line 108, in generate\n",
            "    result = await agenerate_text_with_retry(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "    ...<5 lines>...\n",
            "    )\n",
            "    ^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/tenacity/asyncio/__init__.py\", line 189, in async_wrapped\n",
            "    return await copy(fn, *args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/tenacity/asyncio/__init__.py\", line 111, in __call__\n",
            "    do = await self.iter(retry_state=retry_state)\n",
            "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n",
            "    result = await action(retry_state)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/tenacity/_utils.py\", line 99, in inner\n",
            "    return call(*args, **kwargs)\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/tenacity/__init__.py\", line 420, in exc_check\n",
            "    raise retry_exc.reraise()\n",
            "          ~~~~~~~~~~~~~~~~~^^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/tenacity/__init__.py\", line 187, in reraise\n",
            "    raise self.last_attempt.result()\n",
            "          ~~~~~~~~~~~~~~~~~~~~~~~~^^\n",
            "  File \"/opt/homebrew/Cellar/python@3.13/3.13.2/Frameworks/Python.framework/Versions/3.13/lib/python3.13/concurrent/futures/_base.py\", line 449, in result\n",
            "    return self.__get_result()\n",
            "           ~~~~~~~~~~~~~~~~~^^\n",
            "  File \"/opt/homebrew/Cellar/python@3.13/3.13.2/Frameworks/Python.framework/Versions/3.13/lib/python3.13/concurrent/futures/_base.py\", line 401, in __get_result\n",
            "    raise self._exception\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/tenacity/asyncio/__init__.py\", line 114, in __call__\n",
            "    result = await fn(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/ragas/llms/base.py\", line 251, in agenerate_text\n",
            "    result = await self.langchain_llm.agenerate_prompt(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "    ...<3 lines>...\n",
            "    )\n",
            "    ^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langchain_core/language_models/chat_models.py\", line 991, in agenerate_prompt\n",
            "    return await self.agenerate(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "        prompt_messages, stop=stop, callbacks=callbacks, **kwargs\n",
            "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "    )\n",
            "    ^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langchain_core/language_models/chat_models.py\", line 949, in agenerate\n",
            "    raise exceptions[0]\n",
            "  File \"/opt/homebrew/Cellar/python@3.13/3.13.2/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/tasks.py\", line 304, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langchain_core/language_models/chat_models.py\", line 1117, in _agenerate_with_cache\n",
            "    result = await self._agenerate(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^\n",
            "        messages, stop=stop, run_manager=run_manager, **kwargs\n",
            "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "    )\n",
            "    ^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langchain_openai/chat_models/base.py\", line 1367, in _agenerate\n",
            "    response = await self.async_client.create(**payload)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py\", line 2454, in create\n",
            "    return await self._post(\n",
            "           ^^^^^^^^^^^^^^^^^\n",
            "    ...<45 lines>...\n",
            "    )\n",
            "    ^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/openai/_base_client.py\", line 1791, in post\n",
            "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/openai/_base_client.py\", line 1591, in request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4.1 in organization org-0rHLjwHvNzPhGkB8I6dgKadJ on tokens per min (TPM): Limit 30000, Used 30000, Requested 1230. Please try again in 2.46s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Task exception was never retrieved\n",
            "future: <Task finished name='Task-136' coro=<as_completed.<locals>.sema_coro() done, defined at /Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/ragas/executor.py:46> exception=RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4.1 in organization org-0rHLjwHvNzPhGkB8I6dgKadJ on tokens per min (TPM): Limit 30000, Used 29972, Requested 2976. Please try again in 5.896s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")>\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/homebrew/Cellar/python@3.13/3.13.2/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/tasks.py\", line 304, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/ragas/executor.py\", line 48, in sema_coro\n",
            "    return await coro\n",
            "           ^^^^^^^^^^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/ragas/testset/transforms/base.py\", line 178, in apply_extract\n",
            "    property_name, property_value = await self.extract(node)\n",
            "                                    ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/ragas/testset/transforms/extractors/llm_based.py\", line 180, in extract\n",
            "    result = await self.prompt.generate(self.llm, data=StringIO(text=chunks[0]))\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/ragas/prompt/pydantic_prompt.py\", line 126, in generate\n",
            "    output_single = await self.generate_multiple(\n",
            "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "    ...<7 lines>...\n",
            "    )\n",
            "    ^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/ragas/prompt/pydantic_prompt.py\", line 187, in generate_multiple\n",
            "    resp = await llm.generate(\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "    ...<5 lines>...\n",
            "    )\n",
            "    ^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/ragas/llms/base.py\", line 108, in generate\n",
            "    result = await agenerate_text_with_retry(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "    ...<5 lines>...\n",
            "    )\n",
            "    ^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/tenacity/asyncio/__init__.py\", line 189, in async_wrapped\n",
            "    return await copy(fn, *args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/tenacity/asyncio/__init__.py\", line 111, in __call__\n",
            "    do = await self.iter(retry_state=retry_state)\n",
            "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n",
            "    result = await action(retry_state)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/tenacity/_utils.py\", line 99, in inner\n",
            "    return call(*args, **kwargs)\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/tenacity/__init__.py\", line 420, in exc_check\n",
            "    raise retry_exc.reraise()\n",
            "          ~~~~~~~~~~~~~~~~~^^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/tenacity/__init__.py\", line 187, in reraise\n",
            "    raise self.last_attempt.result()\n",
            "          ~~~~~~~~~~~~~~~~~~~~~~~~^^\n",
            "  File \"/opt/homebrew/Cellar/python@3.13/3.13.2/Frameworks/Python.framework/Versions/3.13/lib/python3.13/concurrent/futures/_base.py\", line 449, in result\n",
            "    return self.__get_result()\n",
            "           ~~~~~~~~~~~~~~~~~^^\n",
            "  File \"/opt/homebrew/Cellar/python@3.13/3.13.2/Frameworks/Python.framework/Versions/3.13/lib/python3.13/concurrent/futures/_base.py\", line 401, in __get_result\n",
            "    raise self._exception\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/tenacity/asyncio/__init__.py\", line 114, in __call__\n",
            "    result = await fn(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/ragas/llms/base.py\", line 251, in agenerate_text\n",
            "    result = await self.langchain_llm.agenerate_prompt(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "    ...<3 lines>...\n",
            "    )\n",
            "    ^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langchain_core/language_models/chat_models.py\", line 991, in agenerate_prompt\n",
            "    return await self.agenerate(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "        prompt_messages, stop=stop, callbacks=callbacks, **kwargs\n",
            "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "    )\n",
            "    ^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langchain_core/language_models/chat_models.py\", line 949, in agenerate\n",
            "    raise exceptions[0]\n",
            "  File \"/opt/homebrew/Cellar/python@3.13/3.13.2/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/tasks.py\", line 304, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langchain_core/language_models/chat_models.py\", line 1117, in _agenerate_with_cache\n",
            "    result = await self._agenerate(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^\n",
            "        messages, stop=stop, run_manager=run_manager, **kwargs\n",
            "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "    )\n",
            "    ^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langchain_openai/chat_models/base.py\", line 1367, in _agenerate\n",
            "    response = await self.async_client.create(**payload)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py\", line 2454, in create\n",
            "    return await self._post(\n",
            "           ^^^^^^^^^^^^^^^^^\n",
            "    ...<45 lines>...\n",
            "    )\n",
            "    ^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/openai/_base_client.py\", line 1791, in post\n",
            "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/openai/_base_client.py\", line 1591, in request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4.1 in organization org-0rHLjwHvNzPhGkB8I6dgKadJ on tokens per min (TPM): Limit 30000, Used 29972, Requested 2976. Please try again in 5.896s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Task exception was never retrieved\n",
            "future: <Task finished name='Task-242' coro=<as_completed.<locals>.sema_coro() done, defined at /Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/ragas/executor.py:46> exception=RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4.1 in organization org-0rHLjwHvNzPhGkB8I6dgKadJ on tokens per min (TPM): Limit 30000, Used 29836, Requested 1102. Please try again in 1.876s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")>\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/homebrew/Cellar/python@3.13/3.13.2/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/tasks.py\", line 304, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/ragas/executor.py\", line 48, in sema_coro\n",
            "    return await coro\n",
            "           ^^^^^^^^^^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/ragas/testset/transforms/base.py\", line 178, in apply_extract\n",
            "    property_name, property_value = await self.extract(node)\n",
            "                                    ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/ragas/testset/transforms/extractors/llm_based.py\", line 180, in extract\n",
            "    result = await self.prompt.generate(self.llm, data=StringIO(text=chunks[0]))\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/ragas/prompt/pydantic_prompt.py\", line 126, in generate\n",
            "    output_single = await self.generate_multiple(\n",
            "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "    ...<7 lines>...\n",
            "    )\n",
            "    ^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/ragas/prompt/pydantic_prompt.py\", line 187, in generate_multiple\n",
            "    resp = await llm.generate(\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "    ...<5 lines>...\n",
            "    )\n",
            "    ^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/ragas/llms/base.py\", line 108, in generate\n",
            "    result = await agenerate_text_with_retry(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "    ...<5 lines>...\n",
            "    )\n",
            "    ^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/tenacity/asyncio/__init__.py\", line 189, in async_wrapped\n",
            "    return await copy(fn, *args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/tenacity/asyncio/__init__.py\", line 111, in __call__\n",
            "    do = await self.iter(retry_state=retry_state)\n",
            "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n",
            "    result = await action(retry_state)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/tenacity/_utils.py\", line 99, in inner\n",
            "    return call(*args, **kwargs)\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/tenacity/__init__.py\", line 420, in exc_check\n",
            "    raise retry_exc.reraise()\n",
            "          ~~~~~~~~~~~~~~~~~^^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/tenacity/__init__.py\", line 187, in reraise\n",
            "    raise self.last_attempt.result()\n",
            "          ~~~~~~~~~~~~~~~~~~~~~~~~^^\n",
            "  File \"/opt/homebrew/Cellar/python@3.13/3.13.2/Frameworks/Python.framework/Versions/3.13/lib/python3.13/concurrent/futures/_base.py\", line 449, in result\n",
            "    return self.__get_result()\n",
            "           ~~~~~~~~~~~~~~~~~^^\n",
            "  File \"/opt/homebrew/Cellar/python@3.13/3.13.2/Frameworks/Python.framework/Versions/3.13/lib/python3.13/concurrent/futures/_base.py\", line 401, in __get_result\n",
            "    raise self._exception\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/tenacity/asyncio/__init__.py\", line 114, in __call__\n",
            "    result = await fn(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/ragas/llms/base.py\", line 251, in agenerate_text\n",
            "    result = await self.langchain_llm.agenerate_prompt(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "    ...<3 lines>...\n",
            "    )\n",
            "    ^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langchain_core/language_models/chat_models.py\", line 991, in agenerate_prompt\n",
            "    return await self.agenerate(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "        prompt_messages, stop=stop, callbacks=callbacks, **kwargs\n",
            "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "    )\n",
            "    ^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langchain_core/language_models/chat_models.py\", line 949, in agenerate\n",
            "    raise exceptions[0]\n",
            "  File \"/opt/homebrew/Cellar/python@3.13/3.13.2/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/tasks.py\", line 304, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langchain_core/language_models/chat_models.py\", line 1117, in _agenerate_with_cache\n",
            "    result = await self._agenerate(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^\n",
            "        messages, stop=stop, run_manager=run_manager, **kwargs\n",
            "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "    )\n",
            "    ^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langchain_openai/chat_models/base.py\", line 1367, in _agenerate\n",
            "    response = await self.async_client.create(**payload)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py\", line 2454, in create\n",
            "    return await self._post(\n",
            "           ^^^^^^^^^^^^^^^^^\n",
            "    ...<45 lines>...\n",
            "    )\n",
            "    ^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/openai/_base_client.py\", line 1791, in post\n",
            "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/openai/_base_client.py\", line 1591, in request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4.1 in organization org-0rHLjwHvNzPhGkB8I6dgKadJ on tokens per min (TPM): Limit 30000, Used 29836, Requested 1102. Please try again in 1.876s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Task exception was never retrieved\n",
            "future: <Task finished name='Task-220' coro=<as_completed.<locals>.sema_coro() done, defined at /Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/ragas/executor.py:46> exception=RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4.1 in organization org-0rHLjwHvNzPhGkB8I6dgKadJ on tokens per min (TPM): Limit 30000, Used 30000, Requested 2083. Please try again in 4.166s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")>\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/homebrew/Cellar/python@3.13/3.13.2/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/tasks.py\", line 304, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/ragas/executor.py\", line 48, in sema_coro\n",
            "    return await coro\n",
            "           ^^^^^^^^^^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/ragas/testset/transforms/base.py\", line 178, in apply_extract\n",
            "    property_name, property_value = await self.extract(node)\n",
            "                                    ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/ragas/testset/transforms/extractors/llm_based.py\", line 180, in extract\n",
            "    result = await self.prompt.generate(self.llm, data=StringIO(text=chunks[0]))\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/ragas/prompt/pydantic_prompt.py\", line 126, in generate\n",
            "    output_single = await self.generate_multiple(\n",
            "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "    ...<7 lines>...\n",
            "    )\n",
            "    ^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/ragas/prompt/pydantic_prompt.py\", line 187, in generate_multiple\n",
            "    resp = await llm.generate(\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "    ...<5 lines>...\n",
            "    )\n",
            "    ^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/ragas/llms/base.py\", line 108, in generate\n",
            "    result = await agenerate_text_with_retry(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "    ...<5 lines>...\n",
            "    )\n",
            "    ^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/tenacity/asyncio/__init__.py\", line 189, in async_wrapped\n",
            "    return await copy(fn, *args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/tenacity/asyncio/__init__.py\", line 111, in __call__\n",
            "    do = await self.iter(retry_state=retry_state)\n",
            "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n",
            "    result = await action(retry_state)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/tenacity/_utils.py\", line 99, in inner\n",
            "    return call(*args, **kwargs)\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/tenacity/__init__.py\", line 420, in exc_check\n",
            "    raise retry_exc.reraise()\n",
            "          ~~~~~~~~~~~~~~~~~^^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/tenacity/__init__.py\", line 187, in reraise\n",
            "    raise self.last_attempt.result()\n",
            "          ~~~~~~~~~~~~~~~~~~~~~~~~^^\n",
            "  File \"/opt/homebrew/Cellar/python@3.13/3.13.2/Frameworks/Python.framework/Versions/3.13/lib/python3.13/concurrent/futures/_base.py\", line 449, in result\n",
            "    return self.__get_result()\n",
            "           ~~~~~~~~~~~~~~~~~^^\n",
            "  File \"/opt/homebrew/Cellar/python@3.13/3.13.2/Frameworks/Python.framework/Versions/3.13/lib/python3.13/concurrent/futures/_base.py\", line 401, in __get_result\n",
            "    raise self._exception\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/tenacity/asyncio/__init__.py\", line 114, in __call__\n",
            "    result = await fn(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/ragas/llms/base.py\", line 251, in agenerate_text\n",
            "    result = await self.langchain_llm.agenerate_prompt(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "    ...<3 lines>...\n",
            "    )\n",
            "    ^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langchain_core/language_models/chat_models.py\", line 991, in agenerate_prompt\n",
            "    return await self.agenerate(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "        prompt_messages, stop=stop, callbacks=callbacks, **kwargs\n",
            "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "    )\n",
            "    ^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langchain_core/language_models/chat_models.py\", line 949, in agenerate\n",
            "    raise exceptions[0]\n",
            "  File \"/opt/homebrew/Cellar/python@3.13/3.13.2/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/tasks.py\", line 304, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langchain_core/language_models/chat_models.py\", line 1117, in _agenerate_with_cache\n",
            "    result = await self._agenerate(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^\n",
            "        messages, stop=stop, run_manager=run_manager, **kwargs\n",
            "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "    )\n",
            "    ^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langchain_openai/chat_models/base.py\", line 1367, in _agenerate\n",
            "    response = await self.async_client.create(**payload)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py\", line 2454, in create\n",
            "    return await self._post(\n",
            "           ^^^^^^^^^^^^^^^^^\n",
            "    ...<45 lines>...\n",
            "    )\n",
            "    ^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/openai/_base_client.py\", line 1791, in post\n",
            "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/openai/_base_client.py\", line 1591, in request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4.1 in organization org-0rHLjwHvNzPhGkB8I6dgKadJ on tokens per min (TPM): Limit 30000, Used 30000, Requested 2083. Please try again in 4.166s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Task exception was never retrieved\n",
            "future: <Task finished name='Task-247' coro=<as_completed.<locals>.sema_coro() done, defined at /Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/ragas/executor.py:46> exception=RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4.1 in organization org-0rHLjwHvNzPhGkB8I6dgKadJ on tokens per min (TPM): Limit 30000, Used 29996, Requested 1162. Please try again in 2.316s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")>\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/homebrew/Cellar/python@3.13/3.13.2/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/tasks.py\", line 304, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/ragas/executor.py\", line 48, in sema_coro\n",
            "    return await coro\n",
            "           ^^^^^^^^^^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/ragas/testset/transforms/base.py\", line 178, in apply_extract\n",
            "    property_name, property_value = await self.extract(node)\n",
            "                                    ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/ragas/testset/transforms/extractors/llm_based.py\", line 180, in extract\n",
            "    result = await self.prompt.generate(self.llm, data=StringIO(text=chunks[0]))\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/ragas/prompt/pydantic_prompt.py\", line 126, in generate\n",
            "    output_single = await self.generate_multiple(\n",
            "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "    ...<7 lines>...\n",
            "    )\n",
            "    ^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/ragas/prompt/pydantic_prompt.py\", line 187, in generate_multiple\n",
            "    resp = await llm.generate(\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "    ...<5 lines>...\n",
            "    )\n",
            "    ^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/ragas/llms/base.py\", line 108, in generate\n",
            "    result = await agenerate_text_with_retry(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "    ...<5 lines>...\n",
            "    )\n",
            "    ^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/tenacity/asyncio/__init__.py\", line 189, in async_wrapped\n",
            "    return await copy(fn, *args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/tenacity/asyncio/__init__.py\", line 111, in __call__\n",
            "    do = await self.iter(retry_state=retry_state)\n",
            "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n",
            "    result = await action(retry_state)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/tenacity/_utils.py\", line 99, in inner\n",
            "    return call(*args, **kwargs)\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/tenacity/__init__.py\", line 420, in exc_check\n",
            "    raise retry_exc.reraise()\n",
            "          ~~~~~~~~~~~~~~~~~^^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/tenacity/__init__.py\", line 187, in reraise\n",
            "    raise self.last_attempt.result()\n",
            "          ~~~~~~~~~~~~~~~~~~~~~~~~^^\n",
            "  File \"/opt/homebrew/Cellar/python@3.13/3.13.2/Frameworks/Python.framework/Versions/3.13/lib/python3.13/concurrent/futures/_base.py\", line 449, in result\n",
            "    return self.__get_result()\n",
            "           ~~~~~~~~~~~~~~~~~^^\n",
            "  File \"/opt/homebrew/Cellar/python@3.13/3.13.2/Frameworks/Python.framework/Versions/3.13/lib/python3.13/concurrent/futures/_base.py\", line 401, in __get_result\n",
            "    raise self._exception\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/tenacity/asyncio/__init__.py\", line 114, in __call__\n",
            "    result = await fn(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/ragas/llms/base.py\", line 251, in agenerate_text\n",
            "    result = await self.langchain_llm.agenerate_prompt(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "    ...<3 lines>...\n",
            "    )\n",
            "    ^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langchain_core/language_models/chat_models.py\", line 991, in agenerate_prompt\n",
            "    return await self.agenerate(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "        prompt_messages, stop=stop, callbacks=callbacks, **kwargs\n",
            "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "    )\n",
            "    ^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langchain_core/language_models/chat_models.py\", line 949, in agenerate\n",
            "    raise exceptions[0]\n",
            "  File \"/opt/homebrew/Cellar/python@3.13/3.13.2/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/tasks.py\", line 304, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langchain_core/language_models/chat_models.py\", line 1117, in _agenerate_with_cache\n",
            "    result = await self._agenerate(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^\n",
            "        messages, stop=stop, run_manager=run_manager, **kwargs\n",
            "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "    )\n",
            "    ^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langchain_openai/chat_models/base.py\", line 1367, in _agenerate\n",
            "    response = await self.async_client.create(**payload)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py\", line 2454, in create\n",
            "    return await self._post(\n",
            "           ^^^^^^^^^^^^^^^^^\n",
            "    ...<45 lines>...\n",
            "    )\n",
            "    ^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/openai/_base_client.py\", line 1791, in post\n",
            "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/openai/_base_client.py\", line 1591, in request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4.1 in organization org-0rHLjwHvNzPhGkB8I6dgKadJ on tokens per min (TPM): Limit 30000, Used 29996, Requested 1162. Please try again in 2.316s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Task exception was never retrieved\n",
            "future: <Task finished name='Task-301' coro=<as_completed.<locals>.sema_coro() done, defined at /Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/ragas/executor.py:46> exception=RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4.1 in organization org-0rHLjwHvNzPhGkB8I6dgKadJ on tokens per min (TPM): Limit 30000, Used 29652, Requested 1215. Please try again in 1.734s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")>\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/homebrew/Cellar/python@3.13/3.13.2/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/tasks.py\", line 304, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/ragas/executor.py\", line 48, in sema_coro\n",
            "    return await coro\n",
            "           ^^^^^^^^^^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/ragas/testset/transforms/base.py\", line 178, in apply_extract\n",
            "    property_name, property_value = await self.extract(node)\n",
            "                                    ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/ragas/testset/transforms/extractors/llm_based.py\", line 180, in extract\n",
            "    result = await self.prompt.generate(self.llm, data=StringIO(text=chunks[0]))\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/ragas/prompt/pydantic_prompt.py\", line 126, in generate\n",
            "    output_single = await self.generate_multiple(\n",
            "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "    ...<7 lines>...\n",
            "    )\n",
            "    ^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/ragas/prompt/pydantic_prompt.py\", line 187, in generate_multiple\n",
            "    resp = await llm.generate(\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "    ...<5 lines>...\n",
            "    )\n",
            "    ^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/ragas/llms/base.py\", line 108, in generate\n",
            "    result = await agenerate_text_with_retry(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "    ...<5 lines>...\n",
            "    )\n",
            "    ^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/tenacity/asyncio/__init__.py\", line 189, in async_wrapped\n",
            "    return await copy(fn, *args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/tenacity/asyncio/__init__.py\", line 111, in __call__\n",
            "    do = await self.iter(retry_state=retry_state)\n",
            "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n",
            "    result = await action(retry_state)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/tenacity/_utils.py\", line 99, in inner\n",
            "    return call(*args, **kwargs)\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/tenacity/__init__.py\", line 420, in exc_check\n",
            "    raise retry_exc.reraise()\n",
            "          ~~~~~~~~~~~~~~~~~^^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/tenacity/__init__.py\", line 187, in reraise\n",
            "    raise self.last_attempt.result()\n",
            "          ~~~~~~~~~~~~~~~~~~~~~~~~^^\n",
            "  File \"/opt/homebrew/Cellar/python@3.13/3.13.2/Frameworks/Python.framework/Versions/3.13/lib/python3.13/concurrent/futures/_base.py\", line 449, in result\n",
            "    return self.__get_result()\n",
            "           ~~~~~~~~~~~~~~~~~^^\n",
            "  File \"/opt/homebrew/Cellar/python@3.13/3.13.2/Frameworks/Python.framework/Versions/3.13/lib/python3.13/concurrent/futures/_base.py\", line 401, in __get_result\n",
            "    raise self._exception\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/tenacity/asyncio/__init__.py\", line 114, in __call__\n",
            "    result = await fn(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/ragas/llms/base.py\", line 251, in agenerate_text\n",
            "    result = await self.langchain_llm.agenerate_prompt(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "    ...<3 lines>...\n",
            "    )\n",
            "    ^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langchain_core/language_models/chat_models.py\", line 991, in agenerate_prompt\n",
            "    return await self.agenerate(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "        prompt_messages, stop=stop, callbacks=callbacks, **kwargs\n",
            "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "    )\n",
            "    ^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langchain_core/language_models/chat_models.py\", line 949, in agenerate\n",
            "    raise exceptions[0]\n",
            "  File \"/opt/homebrew/Cellar/python@3.13/3.13.2/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/tasks.py\", line 304, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langchain_core/language_models/chat_models.py\", line 1117, in _agenerate_with_cache\n",
            "    result = await self._agenerate(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^\n",
            "        messages, stop=stop, run_manager=run_manager, **kwargs\n",
            "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "    )\n",
            "    ^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langchain_openai/chat_models/base.py\", line 1367, in _agenerate\n",
            "    response = await self.async_client.create(**payload)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py\", line 2454, in create\n",
            "    return await self._post(\n",
            "           ^^^^^^^^^^^^^^^^^\n",
            "    ...<45 lines>...\n",
            "    )\n",
            "    ^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/openai/_base_client.py\", line 1791, in post\n",
            "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/openai/_base_client.py\", line 1591, in request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4.1 in organization org-0rHLjwHvNzPhGkB8I6dgKadJ on tokens per min (TPM): Limit 30000, Used 29652, Requested 1215. Please try again in 1.734s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Task exception was never retrieved\n",
            "future: <Task finished name='Task-347' coro=<as_completed.<locals>.sema_coro() done, defined at /Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/ragas/executor.py:46> exception=RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4.1 in organization org-0rHLjwHvNzPhGkB8I6dgKadJ on tokens per min (TPM): Limit 30000, Used 29709, Requested 950. Please try again in 1.318s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")>\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/homebrew/Cellar/python@3.13/3.13.2/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/tasks.py\", line 304, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/ragas/executor.py\", line 48, in sema_coro\n",
            "    return await coro\n",
            "           ^^^^^^^^^^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/ragas/testset/transforms/base.py\", line 178, in apply_extract\n",
            "    property_name, property_value = await self.extract(node)\n",
            "                                    ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/ragas/testset/transforms/extractors/llm_based.py\", line 180, in extract\n",
            "    result = await self.prompt.generate(self.llm, data=StringIO(text=chunks[0]))\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/ragas/prompt/pydantic_prompt.py\", line 126, in generate\n",
            "    output_single = await self.generate_multiple(\n",
            "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "    ...<7 lines>...\n",
            "    )\n",
            "    ^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/ragas/prompt/pydantic_prompt.py\", line 187, in generate_multiple\n",
            "    resp = await llm.generate(\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "    ...<5 lines>...\n",
            "    )\n",
            "    ^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/ragas/llms/base.py\", line 108, in generate\n",
            "    result = await agenerate_text_with_retry(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "    ...<5 lines>...\n",
            "    )\n",
            "    ^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/tenacity/asyncio/__init__.py\", line 189, in async_wrapped\n",
            "    return await copy(fn, *args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/tenacity/asyncio/__init__.py\", line 111, in __call__\n",
            "    do = await self.iter(retry_state=retry_state)\n",
            "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n",
            "    result = await action(retry_state)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/tenacity/_utils.py\", line 99, in inner\n",
            "    return call(*args, **kwargs)\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/tenacity/__init__.py\", line 420, in exc_check\n",
            "    raise retry_exc.reraise()\n",
            "          ~~~~~~~~~~~~~~~~~^^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/tenacity/__init__.py\", line 187, in reraise\n",
            "    raise self.last_attempt.result()\n",
            "          ~~~~~~~~~~~~~~~~~~~~~~~~^^\n",
            "  File \"/opt/homebrew/Cellar/python@3.13/3.13.2/Frameworks/Python.framework/Versions/3.13/lib/python3.13/concurrent/futures/_base.py\", line 449, in result\n",
            "    return self.__get_result()\n",
            "           ~~~~~~~~~~~~~~~~~^^\n",
            "  File \"/opt/homebrew/Cellar/python@3.13/3.13.2/Frameworks/Python.framework/Versions/3.13/lib/python3.13/concurrent/futures/_base.py\", line 401, in __get_result\n",
            "    raise self._exception\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/tenacity/asyncio/__init__.py\", line 114, in __call__\n",
            "    result = await fn(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/ragas/llms/base.py\", line 251, in agenerate_text\n",
            "    result = await self.langchain_llm.agenerate_prompt(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "    ...<3 lines>...\n",
            "    )\n",
            "    ^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langchain_core/language_models/chat_models.py\", line 991, in agenerate_prompt\n",
            "    return await self.agenerate(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "        prompt_messages, stop=stop, callbacks=callbacks, **kwargs\n",
            "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "    )\n",
            "    ^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langchain_core/language_models/chat_models.py\", line 949, in agenerate\n",
            "    raise exceptions[0]\n",
            "  File \"/opt/homebrew/Cellar/python@3.13/3.13.2/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/tasks.py\", line 304, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langchain_core/language_models/chat_models.py\", line 1117, in _agenerate_with_cache\n",
            "    result = await self._agenerate(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^\n",
            "        messages, stop=stop, run_manager=run_manager, **kwargs\n",
            "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "    )\n",
            "    ^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langchain_openai/chat_models/base.py\", line 1367, in _agenerate\n",
            "    response = await self.async_client.create(**payload)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py\", line 2454, in create\n",
            "    return await self._post(\n",
            "           ^^^^^^^^^^^^^^^^^\n",
            "    ...<45 lines>...\n",
            "    )\n",
            "    ^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/openai/_base_client.py\", line 1791, in post\n",
            "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/openai/_base_client.py\", line 1591, in request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4.1 in organization org-0rHLjwHvNzPhGkB8I6dgKadJ on tokens per min (TPM): Limit 30000, Used 29709, Requested 950. Please try again in 1.318s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Task exception was never retrieved\n",
            "future: <Task finished name='Task-312' coro=<as_completed.<locals>.sema_coro() done, defined at /Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/ragas/executor.py:46> exception=RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4.1 in organization org-0rHLjwHvNzPhGkB8I6dgKadJ on tokens per min (TPM): Limit 30000, Used 29806, Requested 1342. Please try again in 2.296s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")>\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/homebrew/Cellar/python@3.13/3.13.2/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/tasks.py\", line 304, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/ragas/executor.py\", line 48, in sema_coro\n",
            "    return await coro\n",
            "           ^^^^^^^^^^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/ragas/testset/transforms/base.py\", line 178, in apply_extract\n",
            "    property_name, property_value = await self.extract(node)\n",
            "                                    ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/ragas/testset/transforms/extractors/llm_based.py\", line 180, in extract\n",
            "    result = await self.prompt.generate(self.llm, data=StringIO(text=chunks[0]))\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/ragas/prompt/pydantic_prompt.py\", line 126, in generate\n",
            "    output_single = await self.generate_multiple(\n",
            "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "    ...<7 lines>...\n",
            "    )\n",
            "    ^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/ragas/prompt/pydantic_prompt.py\", line 187, in generate_multiple\n",
            "    resp = await llm.generate(\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "    ...<5 lines>...\n",
            "    )\n",
            "    ^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/ragas/llms/base.py\", line 108, in generate\n",
            "    result = await agenerate_text_with_retry(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "    ...<5 lines>...\n",
            "    )\n",
            "    ^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/tenacity/asyncio/__init__.py\", line 189, in async_wrapped\n",
            "    return await copy(fn, *args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/tenacity/asyncio/__init__.py\", line 111, in __call__\n",
            "    do = await self.iter(retry_state=retry_state)\n",
            "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n",
            "    result = await action(retry_state)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/tenacity/_utils.py\", line 99, in inner\n",
            "    return call(*args, **kwargs)\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/tenacity/__init__.py\", line 420, in exc_check\n",
            "    raise retry_exc.reraise()\n",
            "          ~~~~~~~~~~~~~~~~~^^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/tenacity/__init__.py\", line 187, in reraise\n",
            "    raise self.last_attempt.result()\n",
            "          ~~~~~~~~~~~~~~~~~~~~~~~~^^\n",
            "  File \"/opt/homebrew/Cellar/python@3.13/3.13.2/Frameworks/Python.framework/Versions/3.13/lib/python3.13/concurrent/futures/_base.py\", line 449, in result\n",
            "    return self.__get_result()\n",
            "           ~~~~~~~~~~~~~~~~~^^\n",
            "  File \"/opt/homebrew/Cellar/python@3.13/3.13.2/Frameworks/Python.framework/Versions/3.13/lib/python3.13/concurrent/futures/_base.py\", line 401, in __get_result\n",
            "    raise self._exception\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/tenacity/asyncio/__init__.py\", line 114, in __call__\n",
            "    result = await fn(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/ragas/llms/base.py\", line 251, in agenerate_text\n",
            "    result = await self.langchain_llm.agenerate_prompt(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "    ...<3 lines>...\n",
            "    )\n",
            "    ^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langchain_core/language_models/chat_models.py\", line 991, in agenerate_prompt\n",
            "    return await self.agenerate(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "        prompt_messages, stop=stop, callbacks=callbacks, **kwargs\n",
            "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "    )\n",
            "    ^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langchain_core/language_models/chat_models.py\", line 949, in agenerate\n",
            "    raise exceptions[0]\n",
            "  File \"/opt/homebrew/Cellar/python@3.13/3.13.2/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/tasks.py\", line 304, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langchain_core/language_models/chat_models.py\", line 1117, in _agenerate_with_cache\n",
            "    result = await self._agenerate(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^\n",
            "        messages, stop=stop, run_manager=run_manager, **kwargs\n",
            "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "    )\n",
            "    ^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langchain_openai/chat_models/base.py\", line 1367, in _agenerate\n",
            "    response = await self.async_client.create(**payload)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py\", line 2454, in create\n",
            "    return await self._post(\n",
            "           ^^^^^^^^^^^^^^^^^\n",
            "    ...<45 lines>...\n",
            "    )\n",
            "    ^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/openai/_base_client.py\", line 1791, in post\n",
            "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/openai/_base_client.py\", line 1591, in request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4.1 in organization org-0rHLjwHvNzPhGkB8I6dgKadJ on tokens per min (TPM): Limit 30000, Used 29806, Requested 1342. Please try again in 2.296s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Task exception was never retrieved\n",
            "future: <Task finished name='Task-363' coro=<as_completed.<locals>.sema_coro() done, defined at /Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/ragas/executor.py:46> exception=RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4.1 in organization org-0rHLjwHvNzPhGkB8I6dgKadJ on tokens per min (TPM): Limit 30000, Used 29991, Requested 1006. Please try again in 1.993s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")>\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/homebrew/Cellar/python@3.13/3.13.2/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/tasks.py\", line 304, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/ragas/executor.py\", line 48, in sema_coro\n",
            "    return await coro\n",
            "           ^^^^^^^^^^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/ragas/testset/transforms/base.py\", line 178, in apply_extract\n",
            "    property_name, property_value = await self.extract(node)\n",
            "                                    ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/ragas/testset/transforms/extractors/llm_based.py\", line 180, in extract\n",
            "    result = await self.prompt.generate(self.llm, data=StringIO(text=chunks[0]))\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/ragas/prompt/pydantic_prompt.py\", line 126, in generate\n",
            "    output_single = await self.generate_multiple(\n",
            "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "    ...<7 lines>...\n",
            "    )\n",
            "    ^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/ragas/prompt/pydantic_prompt.py\", line 187, in generate_multiple\n",
            "    resp = await llm.generate(\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "    ...<5 lines>...\n",
            "    )\n",
            "    ^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/ragas/llms/base.py\", line 108, in generate\n",
            "    result = await agenerate_text_with_retry(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "    ...<5 lines>...\n",
            "    )\n",
            "    ^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/tenacity/asyncio/__init__.py\", line 189, in async_wrapped\n",
            "    return await copy(fn, *args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/tenacity/asyncio/__init__.py\", line 111, in __call__\n",
            "    do = await self.iter(retry_state=retry_state)\n",
            "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n",
            "    result = await action(retry_state)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/tenacity/_utils.py\", line 99, in inner\n",
            "    return call(*args, **kwargs)\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/tenacity/__init__.py\", line 420, in exc_check\n",
            "    raise retry_exc.reraise()\n",
            "          ~~~~~~~~~~~~~~~~~^^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/tenacity/__init__.py\", line 187, in reraise\n",
            "    raise self.last_attempt.result()\n",
            "          ~~~~~~~~~~~~~~~~~~~~~~~~^^\n",
            "  File \"/opt/homebrew/Cellar/python@3.13/3.13.2/Frameworks/Python.framework/Versions/3.13/lib/python3.13/concurrent/futures/_base.py\", line 449, in result\n",
            "    return self.__get_result()\n",
            "           ~~~~~~~~~~~~~~~~~^^\n",
            "  File \"/opt/homebrew/Cellar/python@3.13/3.13.2/Frameworks/Python.framework/Versions/3.13/lib/python3.13/concurrent/futures/_base.py\", line 401, in __get_result\n",
            "    raise self._exception\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/tenacity/asyncio/__init__.py\", line 114, in __call__\n",
            "    result = await fn(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/ragas/llms/base.py\", line 251, in agenerate_text\n",
            "    result = await self.langchain_llm.agenerate_prompt(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "    ...<3 lines>...\n",
            "    )\n",
            "    ^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langchain_core/language_models/chat_models.py\", line 991, in agenerate_prompt\n",
            "    return await self.agenerate(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "        prompt_messages, stop=stop, callbacks=callbacks, **kwargs\n",
            "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "    )\n",
            "    ^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langchain_core/language_models/chat_models.py\", line 949, in agenerate\n",
            "    raise exceptions[0]\n",
            "  File \"/opt/homebrew/Cellar/python@3.13/3.13.2/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/tasks.py\", line 304, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langchain_core/language_models/chat_models.py\", line 1117, in _agenerate_with_cache\n",
            "    result = await self._agenerate(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^\n",
            "        messages, stop=stop, run_manager=run_manager, **kwargs\n",
            "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "    )\n",
            "    ^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langchain_openai/chat_models/base.py\", line 1367, in _agenerate\n",
            "    response = await self.async_client.create(**payload)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py\", line 2454, in create\n",
            "    return await self._post(\n",
            "           ^^^^^^^^^^^^^^^^^\n",
            "    ...<45 lines>...\n",
            "    )\n",
            "    ^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/openai/_base_client.py\", line 1791, in post\n",
            "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/openai/_base_client.py\", line 1591, in request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4.1 in organization org-0rHLjwHvNzPhGkB8I6dgKadJ on tokens per min (TPM): Limit 30000, Used 29991, Requested 1006. Please try again in 1.993s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Task exception was never retrieved\n",
            "future: <Task finished name='Task-360' coro=<as_completed.<locals>.sema_coro() done, defined at /Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/ragas/executor.py:46> exception=RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4.1 in organization org-0rHLjwHvNzPhGkB8I6dgKadJ on tokens per min (TPM): Limit 30000, Used 29936, Requested 1176. Please try again in 2.224s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")>\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/homebrew/Cellar/python@3.13/3.13.2/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/tasks.py\", line 304, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/ragas/executor.py\", line 48, in sema_coro\n",
            "    return await coro\n",
            "           ^^^^^^^^^^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/ragas/testset/transforms/base.py\", line 178, in apply_extract\n",
            "    property_name, property_value = await self.extract(node)\n",
            "                                    ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/ragas/testset/transforms/extractors/llm_based.py\", line 180, in extract\n",
            "    result = await self.prompt.generate(self.llm, data=StringIO(text=chunks[0]))\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/ragas/prompt/pydantic_prompt.py\", line 126, in generate\n",
            "    output_single = await self.generate_multiple(\n",
            "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "    ...<7 lines>...\n",
            "    )\n",
            "    ^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/ragas/prompt/pydantic_prompt.py\", line 187, in generate_multiple\n",
            "    resp = await llm.generate(\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "    ...<5 lines>...\n",
            "    )\n",
            "    ^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/ragas/llms/base.py\", line 108, in generate\n",
            "    result = await agenerate_text_with_retry(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "    ...<5 lines>...\n",
            "    )\n",
            "    ^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/tenacity/asyncio/__init__.py\", line 189, in async_wrapped\n",
            "    return await copy(fn, *args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/tenacity/asyncio/__init__.py\", line 111, in __call__\n",
            "    do = await self.iter(retry_state=retry_state)\n",
            "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n",
            "    result = await action(retry_state)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/tenacity/_utils.py\", line 99, in inner\n",
            "    return call(*args, **kwargs)\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/tenacity/__init__.py\", line 420, in exc_check\n",
            "    raise retry_exc.reraise()\n",
            "          ~~~~~~~~~~~~~~~~~^^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/tenacity/__init__.py\", line 187, in reraise\n",
            "    raise self.last_attempt.result()\n",
            "          ~~~~~~~~~~~~~~~~~~~~~~~~^^\n",
            "  File \"/opt/homebrew/Cellar/python@3.13/3.13.2/Frameworks/Python.framework/Versions/3.13/lib/python3.13/concurrent/futures/_base.py\", line 449, in result\n",
            "    return self.__get_result()\n",
            "           ~~~~~~~~~~~~~~~~~^^\n",
            "  File \"/opt/homebrew/Cellar/python@3.13/3.13.2/Frameworks/Python.framework/Versions/3.13/lib/python3.13/concurrent/futures/_base.py\", line 401, in __get_result\n",
            "    raise self._exception\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/tenacity/asyncio/__init__.py\", line 114, in __call__\n",
            "    result = await fn(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/ragas/llms/base.py\", line 251, in agenerate_text\n",
            "    result = await self.langchain_llm.agenerate_prompt(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "    ...<3 lines>...\n",
            "    )\n",
            "    ^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langchain_core/language_models/chat_models.py\", line 991, in agenerate_prompt\n",
            "    return await self.agenerate(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "        prompt_messages, stop=stop, callbacks=callbacks, **kwargs\n",
            "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "    )\n",
            "    ^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langchain_core/language_models/chat_models.py\", line 949, in agenerate\n",
            "    raise exceptions[0]\n",
            "  File \"/opt/homebrew/Cellar/python@3.13/3.13.2/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/tasks.py\", line 304, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langchain_core/language_models/chat_models.py\", line 1117, in _agenerate_with_cache\n",
            "    result = await self._agenerate(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^\n",
            "        messages, stop=stop, run_manager=run_manager, **kwargs\n",
            "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "    )\n",
            "    ^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langchain_openai/chat_models/base.py\", line 1367, in _agenerate\n",
            "    response = await self.async_client.create(**payload)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py\", line 2454, in create\n",
            "    return await self._post(\n",
            "           ^^^^^^^^^^^^^^^^^\n",
            "    ...<45 lines>...\n",
            "    )\n",
            "    ^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/openai/_base_client.py\", line 1791, in post\n",
            "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/openai/_base_client.py\", line 1591, in request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4.1 in organization org-0rHLjwHvNzPhGkB8I6dgKadJ on tokens per min (TPM): Limit 30000, Used 29936, Requested 1176. Please try again in 2.224s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Task exception was never retrieved\n",
            "future: <Task finished name='Task-372' coro=<as_completed.<locals>.sema_coro() done, defined at /Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/ragas/executor.py:46> exception=RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4.1 in organization org-0rHLjwHvNzPhGkB8I6dgKadJ on tokens per min (TPM): Limit 30000, Used 29808, Requested 1901. Please try again in 3.418s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")>\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/homebrew/Cellar/python@3.13/3.13.2/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/tasks.py\", line 304, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/ragas/executor.py\", line 48, in sema_coro\n",
            "    return await coro\n",
            "           ^^^^^^^^^^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/ragas/testset/transforms/base.py\", line 178, in apply_extract\n",
            "    property_name, property_value = await self.extract(node)\n",
            "                                    ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/ragas/testset/transforms/extractors/llm_based.py\", line 180, in extract\n",
            "    result = await self.prompt.generate(self.llm, data=StringIO(text=chunks[0]))\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/ragas/prompt/pydantic_prompt.py\", line 126, in generate\n",
            "    output_single = await self.generate_multiple(\n",
            "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "    ...<7 lines>...\n",
            "    )\n",
            "    ^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/ragas/prompt/pydantic_prompt.py\", line 187, in generate_multiple\n",
            "    resp = await llm.generate(\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "    ...<5 lines>...\n",
            "    )\n",
            "    ^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/ragas/llms/base.py\", line 108, in generate\n",
            "    result = await agenerate_text_with_retry(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "    ...<5 lines>...\n",
            "    )\n",
            "    ^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/tenacity/asyncio/__init__.py\", line 189, in async_wrapped\n",
            "    return await copy(fn, *args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/tenacity/asyncio/__init__.py\", line 111, in __call__\n",
            "    do = await self.iter(retry_state=retry_state)\n",
            "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n",
            "    result = await action(retry_state)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/tenacity/_utils.py\", line 99, in inner\n",
            "    return call(*args, **kwargs)\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/tenacity/__init__.py\", line 420, in exc_check\n",
            "    raise retry_exc.reraise()\n",
            "          ~~~~~~~~~~~~~~~~~^^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/tenacity/__init__.py\", line 187, in reraise\n",
            "    raise self.last_attempt.result()\n",
            "          ~~~~~~~~~~~~~~~~~~~~~~~~^^\n",
            "  File \"/opt/homebrew/Cellar/python@3.13/3.13.2/Frameworks/Python.framework/Versions/3.13/lib/python3.13/concurrent/futures/_base.py\", line 449, in result\n",
            "    return self.__get_result()\n",
            "           ~~~~~~~~~~~~~~~~~^^\n",
            "  File \"/opt/homebrew/Cellar/python@3.13/3.13.2/Frameworks/Python.framework/Versions/3.13/lib/python3.13/concurrent/futures/_base.py\", line 401, in __get_result\n",
            "    raise self._exception\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/tenacity/asyncio/__init__.py\", line 114, in __call__\n",
            "    result = await fn(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/ragas/llms/base.py\", line 251, in agenerate_text\n",
            "    result = await self.langchain_llm.agenerate_prompt(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "    ...<3 lines>...\n",
            "    )\n",
            "    ^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langchain_core/language_models/chat_models.py\", line 991, in agenerate_prompt\n",
            "    return await self.agenerate(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "        prompt_messages, stop=stop, callbacks=callbacks, **kwargs\n",
            "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "    )\n",
            "    ^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langchain_core/language_models/chat_models.py\", line 949, in agenerate\n",
            "    raise exceptions[0]\n",
            "  File \"/opt/homebrew/Cellar/python@3.13/3.13.2/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/tasks.py\", line 304, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langchain_core/language_models/chat_models.py\", line 1117, in _agenerate_with_cache\n",
            "    result = await self._agenerate(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^\n",
            "        messages, stop=stop, run_manager=run_manager, **kwargs\n",
            "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "    )\n",
            "    ^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langchain_openai/chat_models/base.py\", line 1367, in _agenerate\n",
            "    response = await self.async_client.create(**payload)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py\", line 2454, in create\n",
            "    return await self._post(\n",
            "           ^^^^^^^^^^^^^^^^^\n",
            "    ...<45 lines>...\n",
            "    )\n",
            "    ^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/openai/_base_client.py\", line 1791, in post\n",
            "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/openai/_base_client.py\", line 1591, in request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4.1 in organization org-0rHLjwHvNzPhGkB8I6dgKadJ on tokens per min (TPM): Limit 30000, Used 29808, Requested 1901. Please try again in 3.418s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Task exception was never retrieved\n",
            "future: <Task finished name='Task-380' coro=<as_completed.<locals>.sema_coro() done, defined at /Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/ragas/executor.py:46> exception=RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4.1 in organization org-0rHLjwHvNzPhGkB8I6dgKadJ on tokens per min (TPM): Limit 30000, Used 29534, Requested 1437. Please try again in 1.942s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")>\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/homebrew/Cellar/python@3.13/3.13.2/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/tasks.py\", line 304, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/ragas/executor.py\", line 48, in sema_coro\n",
            "    return await coro\n",
            "           ^^^^^^^^^^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/ragas/testset/transforms/base.py\", line 178, in apply_extract\n",
            "    property_name, property_value = await self.extract(node)\n",
            "                                    ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/ragas/testset/transforms/extractors/llm_based.py\", line 180, in extract\n",
            "    result = await self.prompt.generate(self.llm, data=StringIO(text=chunks[0]))\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/ragas/prompt/pydantic_prompt.py\", line 126, in generate\n",
            "    output_single = await self.generate_multiple(\n",
            "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "    ...<7 lines>...\n",
            "    )\n",
            "    ^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/ragas/prompt/pydantic_prompt.py\", line 187, in generate_multiple\n",
            "    resp = await llm.generate(\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "    ...<5 lines>...\n",
            "    )\n",
            "    ^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/ragas/llms/base.py\", line 108, in generate\n",
            "    result = await agenerate_text_with_retry(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "    ...<5 lines>...\n",
            "    )\n",
            "    ^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/tenacity/asyncio/__init__.py\", line 189, in async_wrapped\n",
            "    return await copy(fn, *args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/tenacity/asyncio/__init__.py\", line 111, in __call__\n",
            "    do = await self.iter(retry_state=retry_state)\n",
            "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n",
            "    result = await action(retry_state)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/tenacity/_utils.py\", line 99, in inner\n",
            "    return call(*args, **kwargs)\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/tenacity/__init__.py\", line 420, in exc_check\n",
            "    raise retry_exc.reraise()\n",
            "          ~~~~~~~~~~~~~~~~~^^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/tenacity/__init__.py\", line 187, in reraise\n",
            "    raise self.last_attempt.result()\n",
            "          ~~~~~~~~~~~~~~~~~~~~~~~~^^\n",
            "  File \"/opt/homebrew/Cellar/python@3.13/3.13.2/Frameworks/Python.framework/Versions/3.13/lib/python3.13/concurrent/futures/_base.py\", line 449, in result\n",
            "    return self.__get_result()\n",
            "           ~~~~~~~~~~~~~~~~~^^\n",
            "  File \"/opt/homebrew/Cellar/python@3.13/3.13.2/Frameworks/Python.framework/Versions/3.13/lib/python3.13/concurrent/futures/_base.py\", line 401, in __get_result\n",
            "    raise self._exception\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/tenacity/asyncio/__init__.py\", line 114, in __call__\n",
            "    result = await fn(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/ragas/llms/base.py\", line 251, in agenerate_text\n",
            "    result = await self.langchain_llm.agenerate_prompt(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "    ...<3 lines>...\n",
            "    )\n",
            "    ^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langchain_core/language_models/chat_models.py\", line 991, in agenerate_prompt\n",
            "    return await self.agenerate(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "        prompt_messages, stop=stop, callbacks=callbacks, **kwargs\n",
            "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "    )\n",
            "    ^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langchain_core/language_models/chat_models.py\", line 949, in agenerate\n",
            "    raise exceptions[0]\n",
            "  File \"/opt/homebrew/Cellar/python@3.13/3.13.2/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/tasks.py\", line 304, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langchain_core/language_models/chat_models.py\", line 1117, in _agenerate_with_cache\n",
            "    result = await self._agenerate(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^\n",
            "        messages, stop=stop, run_manager=run_manager, **kwargs\n",
            "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "    )\n",
            "    ^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langchain_openai/chat_models/base.py\", line 1367, in _agenerate\n",
            "    response = await self.async_client.create(**payload)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py\", line 2454, in create\n",
            "    return await self._post(\n",
            "           ^^^^^^^^^^^^^^^^^\n",
            "    ...<45 lines>...\n",
            "    )\n",
            "    ^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/openai/_base_client.py\", line 1791, in post\n",
            "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/openai/_base_client.py\", line 1591, in request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4.1 in organization org-0rHLjwHvNzPhGkB8I6dgKadJ on tokens per min (TPM): Limit 30000, Used 29534, Requested 1437. Please try again in 1.942s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Task exception was never retrieved\n",
            "future: <Task finished name='Task-436' coro=<as_completed.<locals>.sema_coro() done, defined at /Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/ragas/executor.py:46> exception=RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4.1 in organization org-0rHLjwHvNzPhGkB8I6dgKadJ on tokens per min (TPM): Limit 30000, Used 30000, Requested 1537. Please try again in 3.074s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")>\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/homebrew/Cellar/python@3.13/3.13.2/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/tasks.py\", line 304, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/ragas/executor.py\", line 48, in sema_coro\n",
            "    return await coro\n",
            "           ^^^^^^^^^^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/ragas/testset/transforms/base.py\", line 178, in apply_extract\n",
            "    property_name, property_value = await self.extract(node)\n",
            "                                    ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/ragas/testset/transforms/extractors/llm_based.py\", line 180, in extract\n",
            "    result = await self.prompt.generate(self.llm, data=StringIO(text=chunks[0]))\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/ragas/prompt/pydantic_prompt.py\", line 126, in generate\n",
            "    output_single = await self.generate_multiple(\n",
            "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "    ...<7 lines>...\n",
            "    )\n",
            "    ^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/ragas/prompt/pydantic_prompt.py\", line 187, in generate_multiple\n",
            "    resp = await llm.generate(\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "    ...<5 lines>...\n",
            "    )\n",
            "    ^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/ragas/llms/base.py\", line 108, in generate\n",
            "    result = await agenerate_text_with_retry(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "    ...<5 lines>...\n",
            "    )\n",
            "    ^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/tenacity/asyncio/__init__.py\", line 189, in async_wrapped\n",
            "    return await copy(fn, *args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/tenacity/asyncio/__init__.py\", line 111, in __call__\n",
            "    do = await self.iter(retry_state=retry_state)\n",
            "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n",
            "    result = await action(retry_state)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/tenacity/_utils.py\", line 99, in inner\n",
            "    return call(*args, **kwargs)\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/tenacity/__init__.py\", line 420, in exc_check\n",
            "    raise retry_exc.reraise()\n",
            "          ~~~~~~~~~~~~~~~~~^^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/tenacity/__init__.py\", line 187, in reraise\n",
            "    raise self.last_attempt.result()\n",
            "          ~~~~~~~~~~~~~~~~~~~~~~~~^^\n",
            "  File \"/opt/homebrew/Cellar/python@3.13/3.13.2/Frameworks/Python.framework/Versions/3.13/lib/python3.13/concurrent/futures/_base.py\", line 449, in result\n",
            "    return self.__get_result()\n",
            "           ~~~~~~~~~~~~~~~~~^^\n",
            "  File \"/opt/homebrew/Cellar/python@3.13/3.13.2/Frameworks/Python.framework/Versions/3.13/lib/python3.13/concurrent/futures/_base.py\", line 401, in __get_result\n",
            "    raise self._exception\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/tenacity/asyncio/__init__.py\", line 114, in __call__\n",
            "    result = await fn(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/ragas/llms/base.py\", line 251, in agenerate_text\n",
            "    result = await self.langchain_llm.agenerate_prompt(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "    ...<3 lines>...\n",
            "    )\n",
            "    ^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langchain_core/language_models/chat_models.py\", line 991, in agenerate_prompt\n",
            "    return await self.agenerate(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "        prompt_messages, stop=stop, callbacks=callbacks, **kwargs\n",
            "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "    )\n",
            "    ^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langchain_core/language_models/chat_models.py\", line 949, in agenerate\n",
            "    raise exceptions[0]\n",
            "  File \"/opt/homebrew/Cellar/python@3.13/3.13.2/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/tasks.py\", line 304, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langchain_core/language_models/chat_models.py\", line 1117, in _agenerate_with_cache\n",
            "    result = await self._agenerate(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^\n",
            "        messages, stop=stop, run_manager=run_manager, **kwargs\n",
            "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "    )\n",
            "    ^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langchain_openai/chat_models/base.py\", line 1367, in _agenerate\n",
            "    response = await self.async_client.create(**payload)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py\", line 2454, in create\n",
            "    return await self._post(\n",
            "           ^^^^^^^^^^^^^^^^^\n",
            "    ...<45 lines>...\n",
            "    )\n",
            "    ^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/openai/_base_client.py\", line 1791, in post\n",
            "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/openai/_base_client.py\", line 1591, in request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4.1 in organization org-0rHLjwHvNzPhGkB8I6dgKadJ on tokens per min (TPM): Limit 30000, Used 30000, Requested 1537. Please try again in 3.074s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Task exception was never retrieved\n",
            "future: <Task finished name='Task-383' coro=<as_completed.<locals>.sema_coro() done, defined at /Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/ragas/executor.py:46> exception=RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4.1 in organization org-0rHLjwHvNzPhGkB8I6dgKadJ on tokens per min (TPM): Limit 30000, Used 30000, Requested 1687. Please try again in 3.374s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")>\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/homebrew/Cellar/python@3.13/3.13.2/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/tasks.py\", line 304, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/ragas/executor.py\", line 48, in sema_coro\n",
            "    return await coro\n",
            "           ^^^^^^^^^^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/ragas/testset/transforms/base.py\", line 178, in apply_extract\n",
            "    property_name, property_value = await self.extract(node)\n",
            "                                    ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/ragas/testset/transforms/extractors/llm_based.py\", line 180, in extract\n",
            "    result = await self.prompt.generate(self.llm, data=StringIO(text=chunks[0]))\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/ragas/prompt/pydantic_prompt.py\", line 126, in generate\n",
            "    output_single = await self.generate_multiple(\n",
            "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "    ...<7 lines>...\n",
            "    )\n",
            "    ^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/ragas/prompt/pydantic_prompt.py\", line 187, in generate_multiple\n",
            "    resp = await llm.generate(\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "    ...<5 lines>...\n",
            "    )\n",
            "    ^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/ragas/llms/base.py\", line 108, in generate\n",
            "    result = await agenerate_text_with_retry(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "    ...<5 lines>...\n",
            "    )\n",
            "    ^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/tenacity/asyncio/__init__.py\", line 189, in async_wrapped\n",
            "    return await copy(fn, *args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/tenacity/asyncio/__init__.py\", line 111, in __call__\n",
            "    do = await self.iter(retry_state=retry_state)\n",
            "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n",
            "    result = await action(retry_state)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/tenacity/_utils.py\", line 99, in inner\n",
            "    return call(*args, **kwargs)\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/tenacity/__init__.py\", line 420, in exc_check\n",
            "    raise retry_exc.reraise()\n",
            "          ~~~~~~~~~~~~~~~~~^^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/tenacity/__init__.py\", line 187, in reraise\n",
            "    raise self.last_attempt.result()\n",
            "          ~~~~~~~~~~~~~~~~~~~~~~~~^^\n",
            "  File \"/opt/homebrew/Cellar/python@3.13/3.13.2/Frameworks/Python.framework/Versions/3.13/lib/python3.13/concurrent/futures/_base.py\", line 449, in result\n",
            "    return self.__get_result()\n",
            "           ~~~~~~~~~~~~~~~~~^^\n",
            "  File \"/opt/homebrew/Cellar/python@3.13/3.13.2/Frameworks/Python.framework/Versions/3.13/lib/python3.13/concurrent/futures/_base.py\", line 401, in __get_result\n",
            "    raise self._exception\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/tenacity/asyncio/__init__.py\", line 114, in __call__\n",
            "    result = await fn(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/ragas/llms/base.py\", line 251, in agenerate_text\n",
            "    result = await self.langchain_llm.agenerate_prompt(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "    ...<3 lines>...\n",
            "    )\n",
            "    ^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langchain_core/language_models/chat_models.py\", line 991, in agenerate_prompt\n",
            "    return await self.agenerate(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "        prompt_messages, stop=stop, callbacks=callbacks, **kwargs\n",
            "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "    )\n",
            "    ^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langchain_core/language_models/chat_models.py\", line 949, in agenerate\n",
            "    raise exceptions[0]\n",
            "  File \"/opt/homebrew/Cellar/python@3.13/3.13.2/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/tasks.py\", line 304, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langchain_core/language_models/chat_models.py\", line 1117, in _agenerate_with_cache\n",
            "    result = await self._agenerate(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^\n",
            "        messages, stop=stop, run_manager=run_manager, **kwargs\n",
            "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "    )\n",
            "    ^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langchain_openai/chat_models/base.py\", line 1367, in _agenerate\n",
            "    response = await self.async_client.create(**payload)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py\", line 2454, in create\n",
            "    return await self._post(\n",
            "           ^^^^^^^^^^^^^^^^^\n",
            "    ...<45 lines>...\n",
            "    )\n",
            "    ^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/openai/_base_client.py\", line 1791, in post\n",
            "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/openai/_base_client.py\", line 1591, in request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4.1 in organization org-0rHLjwHvNzPhGkB8I6dgKadJ on tokens per min (TPM): Limit 30000, Used 30000, Requested 1687. Please try again in 3.374s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Task exception was never retrieved\n",
            "future: <Task finished name='Task-438' coro=<as_completed.<locals>.sema_coro() done, defined at /Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/ragas/executor.py:46> exception=RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4.1 in organization org-0rHLjwHvNzPhGkB8I6dgKadJ on tokens per min (TPM): Limit 30000, Used 29844, Requested 1161. Please try again in 2.01s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")>\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/homebrew/Cellar/python@3.13/3.13.2/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/tasks.py\", line 304, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/ragas/executor.py\", line 48, in sema_coro\n",
            "    return await coro\n",
            "           ^^^^^^^^^^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/ragas/testset/transforms/base.py\", line 178, in apply_extract\n",
            "    property_name, property_value = await self.extract(node)\n",
            "                                    ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/ragas/testset/transforms/extractors/llm_based.py\", line 180, in extract\n",
            "    result = await self.prompt.generate(self.llm, data=StringIO(text=chunks[0]))\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/ragas/prompt/pydantic_prompt.py\", line 126, in generate\n",
            "    output_single = await self.generate_multiple(\n",
            "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "    ...<7 lines>...\n",
            "    )\n",
            "    ^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/ragas/prompt/pydantic_prompt.py\", line 187, in generate_multiple\n",
            "    resp = await llm.generate(\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "    ...<5 lines>...\n",
            "    )\n",
            "    ^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/ragas/llms/base.py\", line 108, in generate\n",
            "    result = await agenerate_text_with_retry(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "    ...<5 lines>...\n",
            "    )\n",
            "    ^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/tenacity/asyncio/__init__.py\", line 189, in async_wrapped\n",
            "    return await copy(fn, *args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/tenacity/asyncio/__init__.py\", line 111, in __call__\n",
            "    do = await self.iter(retry_state=retry_state)\n",
            "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n",
            "    result = await action(retry_state)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/tenacity/_utils.py\", line 99, in inner\n",
            "    return call(*args, **kwargs)\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/tenacity/__init__.py\", line 420, in exc_check\n",
            "    raise retry_exc.reraise()\n",
            "          ~~~~~~~~~~~~~~~~~^^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/tenacity/__init__.py\", line 187, in reraise\n",
            "    raise self.last_attempt.result()\n",
            "          ~~~~~~~~~~~~~~~~~~~~~~~~^^\n",
            "  File \"/opt/homebrew/Cellar/python@3.13/3.13.2/Frameworks/Python.framework/Versions/3.13/lib/python3.13/concurrent/futures/_base.py\", line 449, in result\n",
            "    return self.__get_result()\n",
            "           ~~~~~~~~~~~~~~~~~^^\n",
            "  File \"/opt/homebrew/Cellar/python@3.13/3.13.2/Frameworks/Python.framework/Versions/3.13/lib/python3.13/concurrent/futures/_base.py\", line 401, in __get_result\n",
            "    raise self._exception\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/tenacity/asyncio/__init__.py\", line 114, in __call__\n",
            "    result = await fn(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/ragas/llms/base.py\", line 251, in agenerate_text\n",
            "    result = await self.langchain_llm.agenerate_prompt(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "    ...<3 lines>...\n",
            "    )\n",
            "    ^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langchain_core/language_models/chat_models.py\", line 991, in agenerate_prompt\n",
            "    return await self.agenerate(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "        prompt_messages, stop=stop, callbacks=callbacks, **kwargs\n",
            "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "    )\n",
            "    ^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langchain_core/language_models/chat_models.py\", line 949, in agenerate\n",
            "    raise exceptions[0]\n",
            "  File \"/opt/homebrew/Cellar/python@3.13/3.13.2/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/tasks.py\", line 304, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langchain_core/language_models/chat_models.py\", line 1117, in _agenerate_with_cache\n",
            "    result = await self._agenerate(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^\n",
            "        messages, stop=stop, run_manager=run_manager, **kwargs\n",
            "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "    )\n",
            "    ^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langchain_openai/chat_models/base.py\", line 1367, in _agenerate\n",
            "    response = await self.async_client.create(**payload)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py\", line 2454, in create\n",
            "    return await self._post(\n",
            "           ^^^^^^^^^^^^^^^^^\n",
            "    ...<45 lines>...\n",
            "    )\n",
            "    ^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/openai/_base_client.py\", line 1791, in post\n",
            "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/psuresh/Documents/2025AIEngineer/AIE7/09_Advanced_Retrieval/.venv/lib/python3.13/site-packages/openai/_base_client.py\", line 1591, in request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4.1 in organization org-0rHLjwHvNzPhGkB8I6dgKadJ on tokens per min (TPM): Limit 30000, Used 29844, Requested 1161. Please try again in 2.01s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n"
          ]
        }
      ],
      "source": [
        "### YOUR CODE HERE\n",
        "from ragas.testset import TestsetGenerator\n",
        "\n",
        "generator = TestsetGenerator(llm=generator_llm, embedding_model=generator_embeddings)\n",
        "dataset = generator.generate_with_langchain_docs(loan_complaint_data[:20], testset_size=10)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>user_input</th>\n",
              "      <th>reference_contexts</th>\n",
              "      <th>reference</th>\n",
              "      <th>synthesizer_name</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>What are the potential impacts on borrowers wh...</td>\n",
              "      <td>[The federal student loan COVID-19 forbearance...</td>\n",
              "      <td>When the federal student loan COVID-19 forbear...</td>\n",
              "      <td>single_hop_specifc_query_synthesizer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Why is Aidvantage billing me for an IDR paymen...</td>\n",
              "      <td>[I submitted my annual Income-Driven Repayment...</td>\n",
              "      <td>Aidvantage billed you for an IDR payment amoun...</td>\n",
              "      <td>single_hop_specifc_query_synthesizer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Why my student loan info got shared when FERPA...</td>\n",
              "      <td>[My personal and financial data was compromise...</td>\n",
              "      <td>My personal and financial data was compromised...</td>\n",
              "      <td>single_hop_specifc_query_synthesizer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>What information does Studentaid.gov provide r...</td>\n",
              "      <td>[According to Studentaid.gov, Im to get an ema...</td>\n",
              "      <td>According to Studentaid.gov, you are supposed ...</td>\n",
              "      <td>single_hop_specifc_query_synthesizer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>How has the resumption of federal loan payment...</td>\n",
              "      <td>[Since the resumption of federal loan payments...</td>\n",
              "      <td>Since the resumption of federal loan payments,...</td>\n",
              "      <td>single_hop_specifc_query_synthesizer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>why my department of education loan with aidva...</td>\n",
              "      <td>[&lt;1-hop&gt;\\n\\nI set up autopay with AidVantage, ...</td>\n",
              "      <td>my department of education loan with aidvantag...</td>\n",
              "      <td>multi_hop_specific_query_synthesizer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>so like i set up autopay with aidvantage for m...</td>\n",
              "      <td>[&lt;1-hop&gt;\\n\\nI set up autopay with AidVantage, ...</td>\n",
              "      <td>so first i set up autopay with aidvantage for ...</td>\n",
              "      <td>multi_hop_specific_query_synthesizer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>how come department of education let DOGE team...</td>\n",
              "      <td>[&lt;1-hop&gt;\\n\\nBreach of Contract - All four bran...</td>\n",
              "      <td>Department of Education let DOGE team get into...</td>\n",
              "      <td>multi_hop_specific_query_synthesizer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>How does the violation of the Family Education...</td>\n",
              "      <td>[&lt;1-hop&gt;\\n\\nThis is a formal legal demand for ...</td>\n",
              "      <td>The violation of the Family Educational Rights...</td>\n",
              "      <td>multi_hop_specific_query_synthesizer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>How do the actions of the loan servicer and Ne...</td>\n",
              "      <td>[&lt;1-hop&gt;\\n\\nIllegal Student Loan Reporting &amp; C...</td>\n",
              "      <td>The actions of the loan servicer and Nelnet de...</td>\n",
              "      <td>multi_hop_specific_query_synthesizer</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                          user_input  \\\n",
              "0  What are the potential impacts on borrowers wh...   \n",
              "1  Why is Aidvantage billing me for an IDR paymen...   \n",
              "2  Why my student loan info got shared when FERPA...   \n",
              "3  What information does Studentaid.gov provide r...   \n",
              "4  How has the resumption of federal loan payment...   \n",
              "5  why my department of education loan with aidva...   \n",
              "6  so like i set up autopay with aidvantage for m...   \n",
              "7  how come department of education let DOGE team...   \n",
              "8  How does the violation of the Family Education...   \n",
              "9  How do the actions of the loan servicer and Ne...   \n",
              "\n",
              "                                  reference_contexts  \\\n",
              "0  [The federal student loan COVID-19 forbearance...   \n",
              "1  [I submitted my annual Income-Driven Repayment...   \n",
              "2  [My personal and financial data was compromise...   \n",
              "3  [According to Studentaid.gov, Im to get an ema...   \n",
              "4  [Since the resumption of federal loan payments...   \n",
              "5  [<1-hop>\\n\\nI set up autopay with AidVantage, ...   \n",
              "6  [<1-hop>\\n\\nI set up autopay with AidVantage, ...   \n",
              "7  [<1-hop>\\n\\nBreach of Contract - All four bran...   \n",
              "8  [<1-hop>\\n\\nThis is a formal legal demand for ...   \n",
              "9  [<1-hop>\\n\\nIllegal Student Loan Reporting & C...   \n",
              "\n",
              "                                           reference  \\\n",
              "0  When the federal student loan COVID-19 forbear...   \n",
              "1  Aidvantage billed you for an IDR payment amoun...   \n",
              "2  My personal and financial data was compromised...   \n",
              "3  According to Studentaid.gov, you are supposed ...   \n",
              "4  Since the resumption of federal loan payments,...   \n",
              "5  my department of education loan with aidvantag...   \n",
              "6  so first i set up autopay with aidvantage for ...   \n",
              "7  Department of Education let DOGE team get into...   \n",
              "8  The violation of the Family Educational Rights...   \n",
              "9  The actions of the loan servicer and Nelnet de...   \n",
              "\n",
              "                       synthesizer_name  \n",
              "0  single_hop_specifc_query_synthesizer  \n",
              "1  single_hop_specifc_query_synthesizer  \n",
              "2  single_hop_specifc_query_synthesizer  \n",
              "3  single_hop_specifc_query_synthesizer  \n",
              "4  single_hop_specifc_query_synthesizer  \n",
              "5  multi_hop_specific_query_synthesizer  \n",
              "6  multi_hop_specific_query_synthesizer  \n",
              "7  multi_hop_specific_query_synthesizer  \n",
              "8  multi_hop_specific_query_synthesizer  \n",
              "9  multi_hop_specific_query_synthesizer  "
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dataset.to_pandas()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [],
      "source": [
        "for test_row in dataset:\n",
        "  response = naive_retrieval_chain.invoke({\"question\" : test_row.eval_sample.user_input})\n",
        "  test_row.eval_sample.response = response[\"response\"].content\n",
        "  test_row.eval_sample.retrieved_contexts = [context.page_content for context in response[\"context\"]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>user_input</th>\n",
              "      <th>retrieved_contexts</th>\n",
              "      <th>reference_contexts</th>\n",
              "      <th>response</th>\n",
              "      <th>reference</th>\n",
              "      <th>synthesizer_name</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>What are the potential impacts on borrowers wh...</td>\n",
              "      <td>[The federal student loan COVID-19 forbearance...</td>\n",
              "      <td>[The federal student loan COVID-19 forbearance...</td>\n",
              "      <td>The end of the federal student loan COVID-19 f...</td>\n",
              "      <td>When the federal student loan COVID-19 forbear...</td>\n",
              "      <td>single_hop_specifc_query_synthesizer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Why is Aidvantage billing me for an IDR paymen...</td>\n",
              "      <td>[I submitted my annual Income-Driven Repayment...</td>\n",
              "      <td>[I submitted my annual Income-Driven Repayment...</td>\n",
              "      <td>Based on the information provided, Aidvantage ...</td>\n",
              "      <td>Aidvantage billed you for an IDR payment amoun...</td>\n",
              "      <td>single_hop_specifc_query_synthesizer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Why my student loan info got shared when FERPA...</td>\n",
              "      <td>[My student loan information was accessed and ...</td>\n",
              "      <td>[My personal and financial data was compromise...</td>\n",
              "      <td>I'm sorry to hear about your situation. Based ...</td>\n",
              "      <td>My personal and financial data was compromised...</td>\n",
              "      <td>single_hop_specifc_query_synthesizer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>What information does Studentaid.gov provide r...</td>\n",
              "      <td>[According to Studentaid.gov, Im to get an ema...</td>\n",
              "      <td>[According to Studentaid.gov, Im to get an ema...</td>\n",
              "      <td>According to Studentaid.gov, when a student lo...</td>\n",
              "      <td>According to Studentaid.gov, you are supposed ...</td>\n",
              "      <td>single_hop_specifc_query_synthesizer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>How has the resumption of federal loan payment...</td>\n",
              "      <td>[Since the resumption of federal loan payments...</td>\n",
              "      <td>[Since the resumption of federal loan payments...</td>\n",
              "      <td>The resumption of federal loan payments has ne...</td>\n",
              "      <td>Since the resumption of federal loan payments,...</td>\n",
              "      <td>single_hop_specifc_query_synthesizer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>why my department of education loan with aidva...</td>\n",
              "      <td>[I paid off my student loans in XXXX The Depar...</td>\n",
              "      <td>[&lt;1-hop&gt;\\n\\nI set up autopay with AidVantage, ...</td>\n",
              "      <td>Based on the information provided, here are so...</td>\n",
              "      <td>my department of education loan with aidvantag...</td>\n",
              "      <td>multi_hop_specific_query_synthesizer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>so like i set up autopay with aidvantage for m...</td>\n",
              "      <td>[Today I called dept. of Ed Nelnet to report t...</td>\n",
              "      <td>[&lt;1-hop&gt;\\n\\nI set up autopay with AidVantage, ...</td>\n",
              "      <td>Based on the information provided, if the Depa...</td>\n",
              "      <td>so first i set up autopay with aidvantage for ...</td>\n",
              "      <td>multi_hop_specific_query_synthesizer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>how come department of education let DOGE team...</td>\n",
              "      <td>[Recent changes in Dept of Education and misha...</td>\n",
              "      <td>[&lt;1-hop&gt;\\n\\nBreach of Contract - All four bran...</td>\n",
              "      <td>I understand your concerns about the security ...</td>\n",
              "      <td>Department of Education let DOGE team get into...</td>\n",
              "      <td>multi_hop_specific_query_synthesizer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>How does the violation of the Family Education...</td>\n",
              "      <td>[I am writing to formally dispute my federal s...</td>\n",
              "      <td>[&lt;1-hop&gt;\\n\\nThis is a formal legal demand for ...</td>\n",
              "      <td>The violation of the Family Educational Rights...</td>\n",
              "      <td>The violation of the Family Educational Rights...</td>\n",
              "      <td>multi_hop_specific_query_synthesizer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>How do the actions of the loan servicer and Ne...</td>\n",
              "      <td>[This account was transferred to Nelnet ( NNI ...</td>\n",
              "      <td>[&lt;1-hop&gt;\\n\\nIllegal Student Loan Reporting &amp; C...</td>\n",
              "      <td>The actions of Nelnet as described in the comp...</td>\n",
              "      <td>The actions of the loan servicer and Nelnet de...</td>\n",
              "      <td>multi_hop_specific_query_synthesizer</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                          user_input  \\\n",
              "0  What are the potential impacts on borrowers wh...   \n",
              "1  Why is Aidvantage billing me for an IDR paymen...   \n",
              "2  Why my student loan info got shared when FERPA...   \n",
              "3  What information does Studentaid.gov provide r...   \n",
              "4  How has the resumption of federal loan payment...   \n",
              "5  why my department of education loan with aidva...   \n",
              "6  so like i set up autopay with aidvantage for m...   \n",
              "7  how come department of education let DOGE team...   \n",
              "8  How does the violation of the Family Education...   \n",
              "9  How do the actions of the loan servicer and Ne...   \n",
              "\n",
              "                                  retrieved_contexts  \\\n",
              "0  [The federal student loan COVID-19 forbearance...   \n",
              "1  [I submitted my annual Income-Driven Repayment...   \n",
              "2  [My student loan information was accessed and ...   \n",
              "3  [According to Studentaid.gov, Im to get an ema...   \n",
              "4  [Since the resumption of federal loan payments...   \n",
              "5  [I paid off my student loans in XXXX The Depar...   \n",
              "6  [Today I called dept. of Ed Nelnet to report t...   \n",
              "7  [Recent changes in Dept of Education and misha...   \n",
              "8  [I am writing to formally dispute my federal s...   \n",
              "9  [This account was transferred to Nelnet ( NNI ...   \n",
              "\n",
              "                                  reference_contexts  \\\n",
              "0  [The federal student loan COVID-19 forbearance...   \n",
              "1  [I submitted my annual Income-Driven Repayment...   \n",
              "2  [My personal and financial data was compromise...   \n",
              "3  [According to Studentaid.gov, Im to get an ema...   \n",
              "4  [Since the resumption of federal loan payments...   \n",
              "5  [<1-hop>\\n\\nI set up autopay with AidVantage, ...   \n",
              "6  [<1-hop>\\n\\nI set up autopay with AidVantage, ...   \n",
              "7  [<1-hop>\\n\\nBreach of Contract - All four bran...   \n",
              "8  [<1-hop>\\n\\nThis is a formal legal demand for ...   \n",
              "9  [<1-hop>\\n\\nIllegal Student Loan Reporting & C...   \n",
              "\n",
              "                                            response  \\\n",
              "0  The end of the federal student loan COVID-19 f...   \n",
              "1  Based on the information provided, Aidvantage ...   \n",
              "2  I'm sorry to hear about your situation. Based ...   \n",
              "3  According to Studentaid.gov, when a student lo...   \n",
              "4  The resumption of federal loan payments has ne...   \n",
              "5  Based on the information provided, here are so...   \n",
              "6  Based on the information provided, if the Depa...   \n",
              "7  I understand your concerns about the security ...   \n",
              "8  The violation of the Family Educational Rights...   \n",
              "9  The actions of Nelnet as described in the comp...   \n",
              "\n",
              "                                           reference  \\\n",
              "0  When the federal student loan COVID-19 forbear...   \n",
              "1  Aidvantage billed you for an IDR payment amoun...   \n",
              "2  My personal and financial data was compromised...   \n",
              "3  According to Studentaid.gov, you are supposed ...   \n",
              "4  Since the resumption of federal loan payments,...   \n",
              "5  my department of education loan with aidvantag...   \n",
              "6  so first i set up autopay with aidvantage for ...   \n",
              "7  Department of Education let DOGE team get into...   \n",
              "8  The violation of the Family Educational Rights...   \n",
              "9  The actions of the loan servicer and Nelnet de...   \n",
              "\n",
              "                       synthesizer_name  \n",
              "0  single_hop_specifc_query_synthesizer  \n",
              "1  single_hop_specifc_query_synthesizer  \n",
              "2  single_hop_specifc_query_synthesizer  \n",
              "3  single_hop_specifc_query_synthesizer  \n",
              "4  single_hop_specifc_query_synthesizer  \n",
              "5  multi_hop_specific_query_synthesizer  \n",
              "6  multi_hop_specific_query_synthesizer  \n",
              "7  multi_hop_specific_query_synthesizer  \n",
              "8  multi_hop_specific_query_synthesizer  \n",
              "9  multi_hop_specific_query_synthesizer  "
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dataset.to_pandas()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [],
      "source": [
        "for test_row in dataset:\n",
        "  response = naive_retrieval_chain.invoke({\"question\" : test_row.eval_sample.user_input})\n",
        "  test_row.eval_sample.response = response[\"response\"].content\n",
        "  test_row.eval_sample.retrieved_contexts = [context.page_content for context in response[\"context\"]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>user_input</th>\n",
              "      <th>retrieved_contexts</th>\n",
              "      <th>reference_contexts</th>\n",
              "      <th>response</th>\n",
              "      <th>reference</th>\n",
              "      <th>synthesizer_name</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>What are the potential impacts on borrowers wh...</td>\n",
              "      <td>[The federal student loan COVID-19 forbearance...</td>\n",
              "      <td>[The federal student loan COVID-19 forbearance...</td>\n",
              "      <td>When the federal student loan COVID-19 forbear...</td>\n",
              "      <td>When the federal student loan COVID-19 forbear...</td>\n",
              "      <td>single_hop_specifc_query_synthesizer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Why is Aidvantage billing me for an IDR paymen...</td>\n",
              "      <td>[I submitted my annual Income-Driven Repayment...</td>\n",
              "      <td>[I submitted my annual Income-Driven Repayment...</td>\n",
              "      <td>Based on the information provided, Aidvantage ...</td>\n",
              "      <td>Aidvantage billed you for an IDR payment amoun...</td>\n",
              "      <td>single_hop_specifc_query_synthesizer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Why my student loan info got shared when FERPA...</td>\n",
              "      <td>[My student loan information was accessed and ...</td>\n",
              "      <td>[My personal and financial data was compromise...</td>\n",
              "      <td>I'm sorry to hear about your situation. Accord...</td>\n",
              "      <td>My personal and financial data was compromised...</td>\n",
              "      <td>single_hop_specifc_query_synthesizer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>What information does Studentaid.gov provide r...</td>\n",
              "      <td>[According to Studentaid.gov, Im to get an ema...</td>\n",
              "      <td>[According to Studentaid.gov, Im to get an ema...</td>\n",
              "      <td>According to Studentaid.gov, you are to receiv...</td>\n",
              "      <td>According to Studentaid.gov, you are supposed ...</td>\n",
              "      <td>single_hop_specifc_query_synthesizer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>How has the resumption of federal loan payment...</td>\n",
              "      <td>[Since the resumption of federal loan payments...</td>\n",
              "      <td>[Since the resumption of federal loan payments...</td>\n",
              "      <td>The resumption of federal loan payments has ne...</td>\n",
              "      <td>Since the resumption of federal loan payments,...</td>\n",
              "      <td>single_hop_specifc_query_synthesizer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>why my department of education loan with aidva...</td>\n",
              "      <td>[I paid off my student loans in XXXX The Depar...</td>\n",
              "      <td>[&lt;1-hop&gt;\\n\\nI set up autopay with AidVantage, ...</td>\n",
              "      <td>It sounds like you're experiencing issues with...</td>\n",
              "      <td>my department of education loan with aidvantag...</td>\n",
              "      <td>multi_hop_specific_query_synthesizer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>so like i set up autopay with aidvantage for m...</td>\n",
              "      <td>[Today I called dept. of Ed Nelnet to report t...</td>\n",
              "      <td>[&lt;1-hop&gt;\\n\\nI set up autopay with AidVantage, ...</td>\n",
              "      <td>Based on the information provided, if your stu...</td>\n",
              "      <td>so first i set up autopay with aidvantage for ...</td>\n",
              "      <td>multi_hop_specific_query_synthesizer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>how come department of education let DOGE team...</td>\n",
              "      <td>[Recent changes in Dept of Education and misha...</td>\n",
              "      <td>[&lt;1-hop&gt;\\n\\nBreach of Contract - All four bran...</td>\n",
              "      <td>I'm sorry to hear about your concerns. Based o...</td>\n",
              "      <td>Department of Education let DOGE team get into...</td>\n",
              "      <td>multi_hop_specific_query_synthesizer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>How does the violation of the Family Education...</td>\n",
              "      <td>[I am writing to formally dispute my federal s...</td>\n",
              "      <td>[&lt;1-hop&gt;\\n\\nThis is a formal legal demand for ...</td>\n",
              "      <td>The violation of the Family Educational Rights...</td>\n",
              "      <td>The violation of the Family Educational Rights...</td>\n",
              "      <td>multi_hop_specific_query_synthesizer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>How do the actions of the loan servicer and Ne...</td>\n",
              "      <td>[This account was transferred to Nelnet ( NNI ...</td>\n",
              "      <td>[&lt;1-hop&gt;\\n\\nIllegal Student Loan Reporting &amp; C...</td>\n",
              "      <td>The actions of Nelnet as described in the comp...</td>\n",
              "      <td>The actions of the loan servicer and Nelnet de...</td>\n",
              "      <td>multi_hop_specific_query_synthesizer</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                          user_input  \\\n",
              "0  What are the potential impacts on borrowers wh...   \n",
              "1  Why is Aidvantage billing me for an IDR paymen...   \n",
              "2  Why my student loan info got shared when FERPA...   \n",
              "3  What information does Studentaid.gov provide r...   \n",
              "4  How has the resumption of federal loan payment...   \n",
              "5  why my department of education loan with aidva...   \n",
              "6  so like i set up autopay with aidvantage for m...   \n",
              "7  how come department of education let DOGE team...   \n",
              "8  How does the violation of the Family Education...   \n",
              "9  How do the actions of the loan servicer and Ne...   \n",
              "\n",
              "                                  retrieved_contexts  \\\n",
              "0  [The federal student loan COVID-19 forbearance...   \n",
              "1  [I submitted my annual Income-Driven Repayment...   \n",
              "2  [My student loan information was accessed and ...   \n",
              "3  [According to Studentaid.gov, Im to get an ema...   \n",
              "4  [Since the resumption of federal loan payments...   \n",
              "5  [I paid off my student loans in XXXX The Depar...   \n",
              "6  [Today I called dept. of Ed Nelnet to report t...   \n",
              "7  [Recent changes in Dept of Education and misha...   \n",
              "8  [I am writing to formally dispute my federal s...   \n",
              "9  [This account was transferred to Nelnet ( NNI ...   \n",
              "\n",
              "                                  reference_contexts  \\\n",
              "0  [The federal student loan COVID-19 forbearance...   \n",
              "1  [I submitted my annual Income-Driven Repayment...   \n",
              "2  [My personal and financial data was compromise...   \n",
              "3  [According to Studentaid.gov, Im to get an ema...   \n",
              "4  [Since the resumption of federal loan payments...   \n",
              "5  [<1-hop>\\n\\nI set up autopay with AidVantage, ...   \n",
              "6  [<1-hop>\\n\\nI set up autopay with AidVantage, ...   \n",
              "7  [<1-hop>\\n\\nBreach of Contract - All four bran...   \n",
              "8  [<1-hop>\\n\\nThis is a formal legal demand for ...   \n",
              "9  [<1-hop>\\n\\nIllegal Student Loan Reporting & C...   \n",
              "\n",
              "                                            response  \\\n",
              "0  When the federal student loan COVID-19 forbear...   \n",
              "1  Based on the information provided, Aidvantage ...   \n",
              "2  I'm sorry to hear about your situation. Accord...   \n",
              "3  According to Studentaid.gov, you are to receiv...   \n",
              "4  The resumption of federal loan payments has ne...   \n",
              "5  It sounds like you're experiencing issues with...   \n",
              "6  Based on the information provided, if your stu...   \n",
              "7  I'm sorry to hear about your concerns. Based o...   \n",
              "8  The violation of the Family Educational Rights...   \n",
              "9  The actions of Nelnet as described in the comp...   \n",
              "\n",
              "                                           reference  \\\n",
              "0  When the federal student loan COVID-19 forbear...   \n",
              "1  Aidvantage billed you for an IDR payment amoun...   \n",
              "2  My personal and financial data was compromised...   \n",
              "3  According to Studentaid.gov, you are supposed ...   \n",
              "4  Since the resumption of federal loan payments,...   \n",
              "5  my department of education loan with aidvantag...   \n",
              "6  so first i set up autopay with aidvantage for ...   \n",
              "7  Department of Education let DOGE team get into...   \n",
              "8  The violation of the Family Educational Rights...   \n",
              "9  The actions of the loan servicer and Nelnet de...   \n",
              "\n",
              "                       synthesizer_name  \n",
              "0  single_hop_specifc_query_synthesizer  \n",
              "1  single_hop_specifc_query_synthesizer  \n",
              "2  single_hop_specifc_query_synthesizer  \n",
              "3  single_hop_specifc_query_synthesizer  \n",
              "4  single_hop_specifc_query_synthesizer  \n",
              "5  multi_hop_specific_query_synthesizer  \n",
              "6  multi_hop_specific_query_synthesizer  \n",
              "7  multi_hop_specific_query_synthesizer  \n",
              "8  multi_hop_specific_query_synthesizer  \n",
              "9  multi_hop_specific_query_synthesizer  "
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dataset.to_pandas()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [],
      "source": [
        "from ragas import EvaluationDataset\n",
        "\n",
        "evaluation_dataset = EvaluationDataset.from_pandas(dataset.to_pandas())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [],
      "source": [
        "from ragas import evaluate\n",
        "from ragas.llms import LangchainLLMWrapper\n",
        "\n",
        "evaluator_llm = LangchainLLMWrapper(ChatOpenAI(model=\"gpt-4.1-mini\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d6f6d4283185442da68f7c94feffc84d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Evaluating:   0%|          | 0/60 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Exception raised in Job[5]: TimeoutError()\n",
            "Exception raised in Job[35]: TimeoutError()\n",
            "Exception raised in Job[41]: TimeoutError()\n",
            "Exception raised in Job[47]: TimeoutError()\n",
            "Exception raised in Job[53]: TimeoutError()\n",
            "Exception raised in Job[59]: TimeoutError()\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'context_recall': 0.9417, 'faithfulness': 0.8012, 'factual_correctness': 0.4790, 'answer_relevancy': 0.4792, 'context_entity_recall': 0.5086, 'noise_sensitivity_relevant': 0.3140}"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from ragas.metrics import LLMContextRecall, Faithfulness, FactualCorrectness, ResponseRelevancy, ContextEntityRecall, NoiseSensitivity\n",
        "from ragas import evaluate, RunConfig\n",
        "\n",
        "custom_run_config = RunConfig(timeout=360)\n",
        "\n",
        "result = evaluate(\n",
        "    dataset=evaluation_dataset,\n",
        "    metrics=[LLMContextRecall(), Faithfulness(), FactualCorrectness(), ResponseRelevancy(), ContextEntityRecall(), NoiseSensitivity()],\n",
        "    llm=evaluator_llm,\n",
        "    run_config=custom_run_config\n",
        ")\n",
        "result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'naive_retriever': [], 'bm25_retriever': [], 'compression_retriever': [], 'multi_query_retriever': [], 'parent_document_retriever': [], 'ensemble_retriever': [], 'semantic_retriever': []}\n"
          ]
        }
      ],
      "source": [
        "retrievers = ['naive_retriever', 'bm25_retriever', 'compression_retriever', 'multi_query_retriever', 'parent_document_retriever', 'ensemble_retriever', 'semantic_retriever']\n",
        "\n",
        "metrics_dict = dict.fromkeys(retrievers, [])\n",
        "\n",
        "print(metrics_dict)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'naive_retriever': {'context_recall': 0.9417, 'faithfulness': 0.8012, 'factual_correctness': 0.4790, 'answer_relevancy': 0.4792, 'context_entity_recall': 0.5086, 'noise_sensitivity_relevant': 0.3140}, 'bm25_retriever': [], 'compression_retriever': [], 'multi_query_retriever': [], 'parent_document_retriever': [], 'ensemble_retriever': [], 'semantic_retriever': []}\n"
          ]
        }
      ],
      "source": [
        "metrics_dict['naive_retriever'] = result\n",
        "print(metrics_dict)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langsmith import Client, traceable\n",
        "from datasets import Dataset\n",
        "from ragas import evaluate\n",
        "import time\n",
        "\n",
        "client = Client()\n",
        "\n",
        "@traceable(name=\"ragas_evaluation\")\n",
        "def evaluate_per_sample_with_tracking(samples, retriever, llm, embeddings, name=\"tracked\"):\n",
        "    scores = {\n",
        "        \"answer_correctness\": [],\n",
        "        \"answer_relevancy\": [],\n",
        "        \"faithfulness\": [],\n",
        "        \"context_recall\": [],\n",
        "        \"context_precision\": [],\n",
        "        \"latency\": [],\n",
        "        \"token_count\": []\n",
        "    }\n",
        "    \n",
        "    total_cost = 0\n",
        "    \n",
        "    for i, sample in enumerate(samples, 1):\n",
        "        start_time = time.time()\n",
        "        \n",
        "        question = sample.eval_sample.user_input\n",
        "        ground_truth = sample.eval_sample.reference\n",
        "        \n",
        "        # Track retrieval\n",
        "        with client.trace(name=\"document_retrieval\"):\n",
        "            docs = retriever.get_relevant_documents(question)\n",
        "            contexts = [doc.page_content for doc in docs]\n",
        "        \n",
        "        # Track LLM generation\n",
        "        with client.trace(name=\"answer_generation\"):\n",
        "            answer = safe_generate(llm, question)\n",
        "        \n",
        "        # Calculate latency\n",
        "        latency = time.time() - start_time\n",
        "        \n",
        "        eval_ds = Dataset.from_dict({\n",
        "            \"question\": [question],\n",
        "            \"contexts\": [contexts],\n",
        "            \"answer\": [answer],\n",
        "            \"ground_truth\": [ground_truth],\n",
        "        })\n",
        "        \n",
        "        try:\n",
        "            # Track RAGAS evaluation\n",
        "            with client.trace(name=\"ragas_metrics\"):\n",
        "                result = evaluate(\n",
        "                    dataset=eval_ds,\n",
        "                    metrics=[answer_relevancy, faithfulness, context_recall, \n",
        "                            context_precision, answer_correctness],\n",
        "                    llm=llm,\n",
        "                    embeddings=embeddings\n",
        "                )\n",
        "            \n",
        "            # Store results\n",
        "            for k in [\"answer_correctness\", \"answer_relevancy\", \"faithfulness\", \n",
        "                     \"context_recall\", \"context_precision\"]:\n",
        "                scores[k].append(result[k])\n",
        "            \n",
        "            scores[\"latency\"].append(latency)\n",
        "            \n",
        "            # Log comprehensive metrics\n",
        "            client.create_run(\n",
        "                name=f\"evaluation_sample_{i}\",\n",
        "                inputs={\"question\": question},\n",
        "                outputs={\n",
        "                    \"answer\": answer,\n",
        "                    \"metrics\": {k: result[k] for k in result.keys()}\n",
        "                },\n",
        "                extra={\n",
        "                    \"latency_seconds\": latency,\n",
        "                    \"sample_index\": i,\n",
        "                    \"retrieval_doc_count\": len(docs),\n",
        "                    \"evaluation_type\": name\n",
        "                }\n",
        "            )\n",
        "            \n",
        "            print(f\"[{name}] ✅ {i}/{len(samples)} complete - Latency: {latency:.2f}s\")\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"[{name}] ❌ {i}/{len(samples)} failed: {e}\")\n",
        "            for k in scores:\n",
        "                scores[k].append(None)\n",
        "        \n",
        "        time.sleep(3)\n",
        "    \n",
        "    return scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_community.retrievers import BM25Retriever\n",
        "\n",
        "bm25_retriever = BM25Retriever.from_documents(loan_complaint_data)\n",
        "\n",
        "bm25_retrieval_chain = (\n",
        "    {\"context\": itemgetter(\"question\") | bm25_retriever, \"question\": itemgetter(\"question\")}\n",
        "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
        "    | {\"response\": rag_prompt | chat_model, \"context\": itemgetter(\"context\")}\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'The context provided does not specify a specific loan amount. Therefore, I do not know what the exact loan amount is.'"
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "bm25_retrieval_chain.invoke({\"question\": \"What is the loan amount?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [],
      "source": [
        "for test_row in dataset:\n",
        "  response = bm25_retrieval_chain.invoke({\"question\" : test_row.eval_sample.user_input})\n",
        "  test_row.eval_sample.response = response[\"response\"].content\n",
        "  test_row.eval_sample.retrieved_contexts = [context.page_content for context in response[\"context\"]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>user_input</th>\n",
              "      <th>retrieved_contexts</th>\n",
              "      <th>reference_contexts</th>\n",
              "      <th>response</th>\n",
              "      <th>reference</th>\n",
              "      <th>synthesizer_name</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>What are the potential impacts on borrowers wh...</td>\n",
              "      <td>[The federal student loan COVID-19 forbearance...</td>\n",
              "      <td>[The federal student loan COVID-19 forbearance...</td>\n",
              "      <td>When the federal student loan COVID-19 forbear...</td>\n",
              "      <td>When the federal student loan COVID-19 forbear...</td>\n",
              "      <td>single_hop_specifc_query_synthesizer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Why is Aidvantage billing me for an IDR paymen...</td>\n",
              "      <td>[I submitted my annual Income-Driven Repayment...</td>\n",
              "      <td>[I submitted my annual Income-Driven Repayment...</td>\n",
              "      <td>Based on the information provided, Aidvantage ...</td>\n",
              "      <td>Aidvantage billed you for an IDR payment amoun...</td>\n",
              "      <td>single_hop_specifc_query_synthesizer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Why my student loan info got shared when FERPA...</td>\n",
              "      <td>[I went to XXXX collage in XXXX Colorado in XX...</td>\n",
              "      <td>[My personal and financial data was compromise...</td>\n",
              "      <td>I'm sorry to hear about your concerns. Based o...</td>\n",
              "      <td>My personal and financial data was compromised...</td>\n",
              "      <td>single_hop_specifc_query_synthesizer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>What information does Studentaid.gov provide r...</td>\n",
              "      <td>[In a letter dated XX/XX/XXXX, I received a le...</td>\n",
              "      <td>[According to Studentaid.gov, Im to get an ema...</td>\n",
              "      <td>Based on the provided context, Studentaid.gov ...</td>\n",
              "      <td>According to Studentaid.gov, you are supposed ...</td>\n",
              "      <td>single_hop_specifc_query_synthesizer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>How has the resumption of federal loan payment...</td>\n",
              "      <td>[Since the resumption of federal loan payments...</td>\n",
              "      <td>[Since the resumption of federal loan payments...</td>\n",
              "      <td>The resumption of federal loan payments has ne...</td>\n",
              "      <td>Since the resumption of federal loan payments,...</td>\n",
              "      <td>single_hop_specifc_query_synthesizer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>why my department of education loan with aidva...</td>\n",
              "      <td>[XXXX XXXX XXXX XXXX XXXX : XXXX. \\n\\nI have b...</td>\n",
              "      <td>[&lt;1-hop&gt;\\n\\nI set up autopay with AidVantage, ...</td>\n",
              "      <td>Based on the information provided, here are so...</td>\n",
              "      <td>my department of education loan with aidvantag...</td>\n",
              "      <td>multi_hop_specific_query_synthesizer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>so like i set up autopay with aidvantage for m...</td>\n",
              "      <td>[I set up autopay with AidVantage, where my de...</td>\n",
              "      <td>[&lt;1-hop&gt;\\n\\nI set up autopay with AidVantage, ...</td>\n",
              "      <td>Based on the information provided, it's unders...</td>\n",
              "      <td>so first i set up autopay with aidvantage for ...</td>\n",
              "      <td>multi_hop_specific_query_synthesizer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>how come department of education let DOGE team...</td>\n",
              "      <td>[Nelnet sent me a letter on XX/XX/XXXX stating...</td>\n",
              "      <td>[&lt;1-hop&gt;\\n\\nBreach of Contract - All four bran...</td>\n",
              "      <td>I'm sorry you're experiencing these concerns. ...</td>\n",
              "      <td>Department of Education let DOGE team get into...</td>\n",
              "      <td>multi_hop_specific_query_synthesizer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>How does the violation of the Family Education...</td>\n",
              "      <td>[I am writing to formally dispute my federal s...</td>\n",
              "      <td>[&lt;1-hop&gt;\\n\\nThis is a formal legal demand for ...</td>\n",
              "      <td>The violation of the Family Educational Rights...</td>\n",
              "      <td>The violation of the Family Educational Rights...</td>\n",
              "      <td>multi_hop_specific_query_synthesizer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>How do the actions of the loan servicer and Ne...</td>\n",
              "      <td>[Urgent Legal Action, Regulatory Complaint, an...</td>\n",
              "      <td>[&lt;1-hop&gt;\\n\\nIllegal Student Loan Reporting &amp; C...</td>\n",
              "      <td>The actions of the loan servicer Nelnet, as de...</td>\n",
              "      <td>The actions of the loan servicer and Nelnet de...</td>\n",
              "      <td>multi_hop_specific_query_synthesizer</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                          user_input  \\\n",
              "0  What are the potential impacts on borrowers wh...   \n",
              "1  Why is Aidvantage billing me for an IDR paymen...   \n",
              "2  Why my student loan info got shared when FERPA...   \n",
              "3  What information does Studentaid.gov provide r...   \n",
              "4  How has the resumption of federal loan payment...   \n",
              "5  why my department of education loan with aidva...   \n",
              "6  so like i set up autopay with aidvantage for m...   \n",
              "7  how come department of education let DOGE team...   \n",
              "8  How does the violation of the Family Education...   \n",
              "9  How do the actions of the loan servicer and Ne...   \n",
              "\n",
              "                                  retrieved_contexts  \\\n",
              "0  [The federal student loan COVID-19 forbearance...   \n",
              "1  [I submitted my annual Income-Driven Repayment...   \n",
              "2  [I went to XXXX collage in XXXX Colorado in XX...   \n",
              "3  [In a letter dated XX/XX/XXXX, I received a le...   \n",
              "4  [Since the resumption of federal loan payments...   \n",
              "5  [XXXX XXXX XXXX XXXX XXXX : XXXX. \\n\\nI have b...   \n",
              "6  [I set up autopay with AidVantage, where my de...   \n",
              "7  [Nelnet sent me a letter on XX/XX/XXXX stating...   \n",
              "8  [I am writing to formally dispute my federal s...   \n",
              "9  [Urgent Legal Action, Regulatory Complaint, an...   \n",
              "\n",
              "                                  reference_contexts  \\\n",
              "0  [The federal student loan COVID-19 forbearance...   \n",
              "1  [I submitted my annual Income-Driven Repayment...   \n",
              "2  [My personal and financial data was compromise...   \n",
              "3  [According to Studentaid.gov, Im to get an ema...   \n",
              "4  [Since the resumption of federal loan payments...   \n",
              "5  [<1-hop>\\n\\nI set up autopay with AidVantage, ...   \n",
              "6  [<1-hop>\\n\\nI set up autopay with AidVantage, ...   \n",
              "7  [<1-hop>\\n\\nBreach of Contract - All four bran...   \n",
              "8  [<1-hop>\\n\\nThis is a formal legal demand for ...   \n",
              "9  [<1-hop>\\n\\nIllegal Student Loan Reporting & C...   \n",
              "\n",
              "                                            response  \\\n",
              "0  When the federal student loan COVID-19 forbear...   \n",
              "1  Based on the information provided, Aidvantage ...   \n",
              "2  I'm sorry to hear about your concerns. Based o...   \n",
              "3  Based on the provided context, Studentaid.gov ...   \n",
              "4  The resumption of federal loan payments has ne...   \n",
              "5  Based on the information provided, here are so...   \n",
              "6  Based on the information provided, it's unders...   \n",
              "7  I'm sorry you're experiencing these concerns. ...   \n",
              "8  The violation of the Family Educational Rights...   \n",
              "9  The actions of the loan servicer Nelnet, as de...   \n",
              "\n",
              "                                           reference  \\\n",
              "0  When the federal student loan COVID-19 forbear...   \n",
              "1  Aidvantage billed you for an IDR payment amoun...   \n",
              "2  My personal and financial data was compromised...   \n",
              "3  According to Studentaid.gov, you are supposed ...   \n",
              "4  Since the resumption of federal loan payments,...   \n",
              "5  my department of education loan with aidvantag...   \n",
              "6  so first i set up autopay with aidvantage for ...   \n",
              "7  Department of Education let DOGE team get into...   \n",
              "8  The violation of the Family Educational Rights...   \n",
              "9  The actions of the loan servicer and Nelnet de...   \n",
              "\n",
              "                       synthesizer_name  \n",
              "0  single_hop_specifc_query_synthesizer  \n",
              "1  single_hop_specifc_query_synthesizer  \n",
              "2  single_hop_specifc_query_synthesizer  \n",
              "3  single_hop_specifc_query_synthesizer  \n",
              "4  single_hop_specifc_query_synthesizer  \n",
              "5  multi_hop_specific_query_synthesizer  \n",
              "6  multi_hop_specific_query_synthesizer  \n",
              "7  multi_hop_specific_query_synthesizer  \n",
              "8  multi_hop_specific_query_synthesizer  \n",
              "9  multi_hop_specific_query_synthesizer  "
            ]
          },
          "execution_count": 34,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dataset.to_pandas()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [],
      "source": [
        "evaluation_dataset = EvaluationDataset.from_pandas(dataset.to_pandas())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c9eac1cfabdb4c829807b155824eb26e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Evaluating:   0%|          | 0/60 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Exception raised in Job[11]: AttributeError('StringIO' object has no attribute 'statements')\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'context_recall': 0.4792, 'faithfulness': 0.7072, 'factual_correctness': 0.4240, 'answer_relevancy': 0.5719, 'context_entity_recall': 0.2229, 'noise_sensitivity_relevant': 0.1909}"
            ]
          },
          "execution_count": 36,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "custom_run_config = RunConfig(timeout=360)\n",
        "\n",
        "result = evaluate(\n",
        "    dataset=evaluation_dataset,\n",
        "    metrics=[LLMContextRecall(), Faithfulness(), FactualCorrectness(), ResponseRelevancy(), ContextEntityRecall(), NoiseSensitivity()],\n",
        "    llm=evaluator_llm,\n",
        "    run_config=custom_run_config\n",
        ")\n",
        "result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'naive_retriever': {'context_recall': 0.9417, 'faithfulness': 0.8012, 'factual_correctness': 0.4790, 'answer_relevancy': 0.4792, 'context_entity_recall': 0.5086, 'noise_sensitivity_relevant': 0.3140}, 'bm25_retriever': {'context_recall': 0.4792, 'faithfulness': 0.7072, 'factual_correctness': 0.4240, 'answer_relevancy': 0.5719, 'context_entity_recall': 0.2229, 'noise_sensitivity_relevant': 0.1909}, 'compression_retriever': [], 'multi_query_retriever': [], 'parent_document_retriever': [], 'ensemble_retriever': [], 'semantic_retriever': []}\n"
          ]
        }
      ],
      "source": [
        "metrics_dict['bm25_retriever'] = result\n",
        "print(metrics_dict)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "scores = evaluate_per_sample_with_tracking(evaluation_dataset, bm25_retriever, evaluator_llm, embeddings, \"bm25_retriever\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'dict'>\n",
            "{'naive_retriever': {'context_recall': 0.9417, 'faithfulness': 0.8012, 'factual_correctness': 0.4790, 'answer_relevancy': 0.4792, 'context_entity_recall': 0.5086, 'noise_sensitivity_relevant': 0.3140}, 'bm25_retriever': {'context_recall': 0.4792, 'faithfulness': 0.7072, 'factual_correctness': 0.4240, 'answer_relevancy': 0.5719, 'context_entity_recall': 0.2229, 'noise_sensitivity_relevant': 0.1909}, 'compression_retriever': [], 'multi_query_retriever': [], 'parent_document_retriever': [], 'ensemble_retriever': [], 'semantic_retriever': []}\n"
          ]
        }
      ],
      "source": [
        "print(type(metrics_dict))\n",
        "print(metrics_dict)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Naive scores: {'context_recall': [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.75, 1.0, 0.6666666666666666, 1.0], 'faithfulness': [1.0, 0.6666666666666666, 0.2, 1.0, 1.0, 0.7027027027027027, 1.0, 0.7368421052631579, 0.7297297297297297, 0.975609756097561], 'factual_correctness': [np.float64(0.35), np.float64(0.2), np.float64(0.11), np.float64(1.0), np.float64(0.53), np.float64(0.58), np.float64(0.42), np.float64(0.39), np.float64(0.69), np.float64(0.52)], 'answer_relevancy': [np.float64(0.9721049507950116), np.float64(0.0), np.float64(0.0), np.float64(0.9433610908823237), np.float64(0.9575411606193777), np.float64(0.0), np.float64(0.0), np.float64(0.0), np.float64(0.9761301949354427), np.float64(0.9428606807839047)], 'context_entity_recall': [0.49999999875, 0.3333333322222222, 0.999999995, 0.9999999900000002, 0.49999999875, 0.571428570612245, 0.21428571413265304, 0.0, 0.29999999969999996, 0.6666666655555555], 'noise_sensitivity_relevant': [nan, np.float64(0.6666666666666666), np.float64(0.1111111111111111), np.float64(0.0), np.float64(0.4782608695652174), nan, nan, nan, nan, nan]}\n",
            "BM25 scores: {'context_recall': [1.0, 1.0, 0.0, 0.0, 1.0, 0.3333333333333333, 0.125, 0.3333333333333333, 0.5, 0.5], 'faithfulness': [0.9375, 0.7727272727272727, 0.10526315789473684, 1.0, 1.0, 0.8611111111111112, 0.2894736842105263, 0.6296296296296297, 0.47619047619047616, 1.0], 'factual_correctness': [np.float64(0.47), np.float64(0.58), np.float64(0.09), np.float64(0.1), np.float64(0.69), np.float64(0.44), np.float64(0.53), np.float64(0.17), np.float64(0.64), np.float64(0.53)], 'answer_relevancy': [np.float64(0.9721049507950116), np.float64(0.9944299455458395), np.float64(0.0), np.float64(0.0), np.float64(0.9916406621550945), np.float64(0.0), np.float64(0.8384958671850669), np.float64(0.0), np.float64(0.9722090519269436), np.float64(0.950025414353514)], 'context_entity_recall': [0.49999999875, 0.3333333322222222, 0.0, 0.0, 0.49999999875, 0.0, 0.07142857137755101, 0.14285714265306124, 0.18181818165289254, 0.49999999916666665], 'noise_sensitivity_relevant': [np.float64(0.2727272727272727), nan, np.float64(0.0), np.float64(0.0), np.float64(0.3333333333333333), np.float64(0.10344827586206896), np.float64(0.1794871794871795), np.float64(0.034482758620689655), np.float64(0.14285714285714285), np.float64(0.6521739130434783)]}\n"
          ]
        }
      ],
      "source": [
        "naive_scores = metrics_dict['naive_retriever']._scores_dict\n",
        "bm25_scores = metrics_dict['bm25_retriever']._scores_dict\n",
        "\n",
        "print(\"Naive scores:\", naive_scores)\n",
        "print(\"BM25 scores:\", bm25_scores)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {},
      "outputs": [],
      "source": [
        "naive_scores = eval(str(metrics_dict['naive_retriever']))  # Convert string repr back to dict\n",
        "bm25_scores = eval(str(metrics_dict['bm25_retriever']))  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {},
      "outputs": [],
      "source": [
        "metrics = list(naive_scores.keys())\n",
        "naive_values = list(naive_scores.values())\n",
        "bm25_values = list(bm25_scores.values())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Matplotlib is building the font cache; this may take a moment.\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1200x800 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Set up the plot\n",
        "fig, ax = plt.subplots(figsize=(12, 8))\n",
        "\n",
        "# Set positions for bars\n",
        "x = np.arange(len(metrics))\n",
        "width = 0.35\n",
        "\n",
        "# Create grouped bars\n",
        "bars1 = ax.bar(x - width/2, naive_values, width, label='Naive Retriever', alpha=0.8, color='skyblue')\n",
        "bars2 = ax.bar(x + width/2, bm25_values, width, label='BM25 Retriever', alpha=0.8, color='lightcoral')\n",
        "\n",
        "# Customize the plot\n",
        "ax.set_xlabel('Metrics', fontsize=12)\n",
        "ax.set_ylabel('Score', fontsize=12)\n",
        "ax.set_title('Retriever Performance Comparison', fontsize=14, fontweight='bold')\n",
        "ax.set_xticks(x)\n",
        "ax.set_xticklabels(metrics, rotation=45, ha='right')\n",
        "ax.legend()\n",
        "ax.grid(axis='y', alpha=0.3)\n",
        "\n",
        "# Add value labels on bars\n",
        "def add_value_labels(bars):\n",
        "    for bar in bars:\n",
        "        height = bar.get_height()\n",
        "        ax.annotate(f'{height:.3f}',\n",
        "                   xy=(bar.get_x() + bar.get_width() / 2, height),\n",
        "                   xytext=(0, 3),  # 3 points vertical offset\n",
        "                   textcoords=\"offset points\",\n",
        "                   ha='center', va='bottom',\n",
        "                   fontsize=9)\n",
        "\n",
        "add_value_labels(bars1)\n",
        "add_value_labels(bars2)\n",
        "\n",
        "# Adjust layout to prevent label cutoff\n",
        "plt.tight_layout()\n",
        "\n",
        "# Display the plot\n",
        "plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
